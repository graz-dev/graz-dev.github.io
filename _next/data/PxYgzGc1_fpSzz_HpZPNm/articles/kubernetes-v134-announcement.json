{"pageProps":{"data":{"article":{"title":"Kubernetes v1.34: Of Wind & Will (O' WaW)","date":"2025-08-27"},"children":"<p><a href=\"https://kubernetes.io/blog/2025/08/27/kubernetes-v1-34-release/\">Published by Kubernetes co-authored with Agustina Barbetta, Alejandro Josue Leon Bellido, Melony Qin, Dipesh Rawat</a></p>\n<p>Similar to previous releases, the release of Kubernetes v1.34 introduces new stable, beta, and alpha features. The consistent delivery of high-quality releases underscores the strength of our development cycle and the vibrant support from our community.</p>\n<p>This release consists of 58 enhancements. Of those enhancements, 23 have graduated to Stable, 22 have entered Beta, and 13 have entered Alpha.</p>\n<p>There are also some <a href=\"https://kubernetes.io/blog/2025/08/27/kubernetes-v1-34-release/#deprecations-and-removals\">deprecations and removals</a> in this release; make sure to read about those.</p>\n<h2>Release theme and logo</h2>\n<p><img src=\"https://kubernetes.io/blog/2025/08/27/kubernetes-v1-34-release/k8s-v1.34.png\" alt=\"Kubernetes v1.34 logo: Three bears sail a wooden ship with a flag featuring a paw and a helm symbol on the sail, as wind blows across the ocean\"></p>\n<p>Kubernetes v1.34 logo: Three bears sail a wooden ship with a flag featuring a paw and a helm symbol on the sail, as wind blows across the ocean</p>\n<p>A release powered by the wind around us — and the will within us.</p>\n<p>Every release cycle, we inherit winds that we don't really control — the state of our tooling, documentation, and the historical quirks of our project. Sometimes these winds fill our sails, sometimes they push us sideways or die down.</p>\n<p>What keeps Kubernetes moving isn't the perfect winds, but the will of our sailors who adjust the sails, man the helm, chart the courses and keep the ship steady. The release happens not because conditions are always ideal, but because of the people who build it, the people who release it, and the bears ^, cats, dogs, wizards, and curious minds who keep Kubernetes sailing strong — no matter which way the wind blows.</p>\n<p>This release, <strong>Of Wind &#x26; Will (O' WaW)</strong>, honors the winds that have shaped us, and the will that propels us forward.</p>\n<p>^ Oh, and you wonder why bears? Keep wondering!</p>\n<h2>Spotlight on key updates</h2>\n<p>Kubernetes v1.34 is packed with new features and improvements. Here are a few select updates the Release Team would like to highlight!</p>\n<h3>Stable: The core of DRA is GA</h3>\n<p><a href=\"https://kubernetes.io/docs/concepts/scheduling-eviction/dynamic-resource-allocation/\">Dynamic Resource Allocation</a> (DRA) enables more powerful ways to select, allocate, share, and configure GPUs, TPUs, NICs and other devices.</p>\n<p>Since the v1.30 release, DRA has been based around claiming devices using <em>structured parameters</em> that are opaque to the core of Kubernetes. This enhancement took inspiration from dynamic provisioning for storage volumes. DRA with structured parameters relies on a set of supporting API kinds: ResourceClaim, DeviceClass, ResourceClaimTemplate, and ResourceSlice API types under <code>resource.k8s.io</code>, while extending the <code>.spec</code> for Pods with a new <code>resourceClaims</code> field.<br>\nThe <code>resource.k8s.io/v1</code> APIs have graduated to stable and are now available by default.</p>\n<p>This work was done as part of <a href=\"https://kep.k8s.io/4381\">KEP #4381</a> led by WG Device Management.</p>\n<h3>Beta: Projected ServiceAccount tokens for kubelet image credential providers</h3>\n<p>The <code>kubelet</code> credential providers, used for pulling private container images, traditionally relied on long-lived Secrets stored on the node or in the cluster. This approach increased security risks and management overhead, as these credentials were not tied to the specific workload and did not rotate automatically.<br>\nTo solve this, the <code>kubelet</code> can now request short-lived, audience-bound ServiceAccount tokens for authenticating to container registries. This allows image pulls to be authorized based on the Pod's own identity rather than a node-level credential.<br>\nThe primary benefit is a significant security improvement. It eliminates the need for long-lived Secrets for image pulls, reducing the attack surface and simplifying credential management for both administrators and developers.</p>\n<p>This work was done as part of <a href=\"https://kep.k8s.io/4412\">KEP #4412</a> led by SIG Auth and SIG Node.</p>\n<h3>Alpha: Support for KYAML, a Kubernetes dialect of YAML</h3>\n<p>KYAML aims to be a safer and less ambiguous YAML subset, and was designed specifically for Kubernetes. Whatever version of Kubernetes you use, starting from Kubernetes v1.34 you are able to use KYAML as a new output format for kubectl.</p>\n<p>KYAML addresses specific challenges with both YAML and JSON. YAML's significant whitespace requires careful attention to indentation and nesting, while its optional string-quoting can lead to unexpected type coercion (for example: <a href=\"https://hitchdev.com/strictyaml/why/implicit-typing-removed/\">\"The Norway Bug\"</a>). Meanwhile, JSON lacks comment support and has strict requirements for trailing commas and quoted keys.</p>\n<p>You can write KYAML and pass it as an input to any version of <code>kubectl</code>, because all KYAML files are also valid as YAML. With <code>kubectl</code> v1.34, you are also able to <a href=\"https://kubernetes.io/docs/reference/kubectl/#syntax-1\">request KYAML output</a> (as in kubectl get -o kyaml …) by setting environment variable <code>KUBECTL_KYAML=true</code>. If you prefer, you can still request the output in JSON or YAML format.</p>\n<p>This work was done as part of <a href=\"https://kep.k8s.io/5295\">KEP #5295</a> led by SIG CLI.</p>\n<h2>Features graduating to Stable</h2>\n<p><em>This is a selection of some of the improvements that are now stable following the v1.34 release.</em></p>\n<h3>Delayed creation of Job’s replacement Pods</h3>\n<p>By default, Job controllers create replacement Pods immediately when a Pod starts terminating, causing both Pods to run simultaneously. This can cause resource contention in constrained clusters, where the replacement Pod may struggle to find available nodes until the original Pod fully terminates. The situation can also trigger unwanted cluster autoscaler scale-ups. Additionally, some machine learning frameworks like TensorFlow and <a href=\"https://jax.readthedocs.io/en/latest/\">JAX</a> require only one Pod per index to run at a time, making simultaneous Pod execution problematic. This feature introduces <code>.spec.podReplacementPolicy</code> in Jobs. You may choose to create replacement Pods only when the Pod is fully terminated (has <code>.status.phase: Failed</code>). To do this, set <code>.spec.podReplacementPolicy: Failed</code>.<br>\nIntroduced as alpha in v1.28, this feature has graduated to stable in v1.34.</p>\n<p>This work was done as part of <a href=\"https://kep.k8s.io/3939\">KEP #3939</a> led by SIG Apps.</p>\n<h3>Recovery from volume expansion failure</h3>\n<p>This feature allows users to cancel volume expansions that are unsupported by the underlying storage provider, and retry volume expansion with smaller values that may succeed.<br>\nIntroduced as alpha in v1.23, this feature has graduated to stable in v1.34.</p>\n<p>This work was done as part of <a href=\"https://kep.k8s.io/1790\">KEP #1790</a> led by SIG Storage.</p>\n<h3>VolumeAttributesClass for volume modification</h3>\n<p><a href=\"https://kubernetes.io/docs/concepts/storage/volume-attributes-classes/\">VolumeAttributesClass</a> has graduated to stable in v1.34. VolumeAttributesClass is a generic, Kubernetes-native API for modifying volume parameters like provisioned IO. It allows workloads to vertically scale their volumes on-line to balance cost and performance, if supported by their provider.<br>\nLike all new volume features in Kubernetes, this API is implemented via the <a href=\"https://kubernetes-csi.github.io/docs/\">container storage interface (CSI)</a>. Your provisioner-specific CSI driver must support the new ModifyVolume API which is the CSI side of this feature.</p>\n<p>This work was done as part of <a href=\"https://kep.k8s.io/3751\">KEP #3751</a> led by SIG Storage.</p>\n<h3>Structured authentication configuration</h3>\n<p>Kubernetes v1.29 introduced a configuration file format to manage API server client authentication, moving away from the previous reliance on a large set of command-line options. The <a href=\"https://kubernetes.io/docs/reference/access-authn-authz/authentication/#using-authentication-configuration\">AuthenticationConfiguration</a> kind allows administrators to support multiple JWT authenticators, CEL expression validation, and dynamic reloading. This change significantly improves the manageability and auditability of the cluster's authentication settings - and has graduated to stable in v1.34.</p>\n<p>This work was done as part of <a href=\"https://kep.k8s.io/3331\">KEP #3331</a> led by SIG Auth.</p>\n<h3>Finer-grained authorization based on selectors</h3>\n<p>Kubernetes authorizers, including webhook authorizers and the built-in node authorizer, can now make authorization decisions based on field and label selectors in incoming requests. When you send <strong>list</strong>, <strong>watch</strong> or <strong>deletecollection</strong> requests with selectors, the authorization layer can now evaluate access with that additional context.</p>\n<p>For example, you can write an authorization policy that only allows listing Pods bound to a specific <code>.spec.nodeName</code>. The client (perhaps the kubelet on a particular node) must specify the field selector that the policy requires, otherwise the request is forbidden. This change makes it feasible to set up least privilege rules, provided that the client knows how to conform to the restrictions you set. Kubernetes v1.34 now supports more granular control in environments like per-node isolation or custom multi-tenant setups.</p>\n<p>This work was done as part of <a href=\"https://kep.k8s.io/4601\">KEP #4601</a> led by SIG Auth.</p>\n<h3>Restrict anonymous requests with fine-grained controls</h3>\n<p>Instead of fully enabling or disabling anonymous access, you can now configure a strict list of endpoints where unauthenticated requests are allowed. This provides a safer alternative for clusters that rely on anonymous access to health or bootstrap endpoints like <code>/healthz</code>, <code>/readyz</code>, or <code>/livez</code>.</p>\n<p>With this feature, accidental RBAC misconfigurations that grant broad access to anonymous users can be avoided without requiring changes to external probes or bootstrapping tools.</p>\n<p>This work was done as part of <a href=\"https://kep.k8s.io/4633\">KEP #4633</a> led by SIG Auth.</p>\n<p>The <code>kube-scheduler</code> can now make more accurate decisions about when to retry scheduling Pods that were previously unschedulable. Each scheduling plugin can now register callback functions that tell the scheduler whether an incoming cluster event is likely to make a rejected Pod schedulable again.</p>\n<p>This reduces unnecessary retries and improves overall scheduling throughput - especially in clusters using dynamic resource allocation. The feature also lets certain plugins skip the usual backoff delay when it is safe to do so, making scheduling faster in specific cases.</p>\n<p>This work was done as part of <a href=\"https://kep.k8s.io/4247\">KEP #4247</a> led by SIG Scheduling.</p>\n<h3>Ordered Namespace deletion</h3>\n<p>Semi-random resource deletion order can create security gaps or unintended behavior, such as Pods persisting after their associated NetworkPolicies are deleted.<br>\nThis improvement introduces a more structured deletion process for Kubernetes <a href=\"https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/\">namespaces</a> to ensure secure and deterministic resource removal. By enforcing a structured deletion sequence that respects logical and security dependencies, this approach ensures Pods are removed before other resources.<br>\nThis feature was introduced in Kubernetes v1.33 and graduated to stable in v1.34. The graduation improves security and reliability by mitigating risks from non-deterministic deletions, including the vulnerability described in <a href=\"https://github.com/advisories/GHSA-r56h-j38w-hrqq\">CVE-2024-7598</a>.</p>\n<p>This work was done as part of <a href=\"https://kep.k8s.io/5080\">KEP #5080</a> led by SIG API Machinery.</p>\n<h3>Streaming list responses</h3>\n<p>Handling large <strong>list</strong> responses in Kubernetes previously posed a significant scalability challenge. When clients requested extensive resource lists, such as thousands of Pods or Custom Resources, the API server was required to serialize the entire collection of objects into a single, large memory buffer before sending it. This process created substantial memory pressure and could lead to performance degradation, impacting the overall stability of the cluster.<br>\nTo address this limitation, a streaming encoding mechanism for collections (list responses) has been introduced. For the JSON and Kubernetes Protobuf response formats, that streaming mechanism is automatically active and the associated feature gate is stable. The primary benefit of this approach is the avoidance of large memory allocations on the API server, resulting in a much smaller and more predictable memory footprint. Consequently, the cluster becomes more resilient and performant, especially in large-scale environments where frequent requests for extensive resource lists are common.</p>\n<p>This work was done as part of <a href=\"https://kep.k8s.io/5116\">KEP #5116</a> led by SIG API Machinery.</p>\n<h3>Resilient watch cache initialization</h3>\n<p>Watch cache is a caching layer inside <code>kube-apiserver</code> that maintains an eventually consistent cache of cluster state stored in etcd. In the past, issues could occur when the watch cache was not yet initialized during <code>kube-apiserver</code> startup or when it required re-initialization.</p>\n<p>To address these issues, the watch cache initialization process has been made more resilient to failures, improving control plane robustness and ensuring controllers and clients can reliably establish watches. This improvement was introduced as beta in v1.31 and is now stable.</p>\n<p>This work was done as part of <a href=\"https://kep.k8s.io/4568\">KEP #4568</a> led by SIG API Machinery and SIG Scalability.</p>\n<p>Previously, the strict validation of a Pod's DNS <code>search</code> path in Kubernetes often created integration challenges in complex or legacy network environments. This restrictiveness could block configurations that were necessary for an organization's infrastructure, forcing administrators to implement difficult workarounds.<br>\nTo address this, relaxed DNS validation was introduced as alpha in v1.32 and has now graduated to stable in v1.34. A common use case involves Pods that need to communicate with both internal Kubernetes services and external domains. By setting a single dot (<code>.</code>) as the first entry in the <code>searches</code> list of the Pod's <code>.spec.dnsConfig</code>, administrators can prevent the system's resolver from appending the cluster's internal search domains to external queries. This avoids generating unnecessary DNS requests to the internal DNS server for external hostnames, improving efficiency and preventing potential resolution errors.</p>\n<p>This work was done as part of <a href=\"https://kep.k8s.io/4427\">KEP #4427</a> led by SIG Network.</p>\n<h3>Support for Direct Service Return (DSR) in Windows kube-proxy</h3>\n<p>DSR provides performance optimizations by allowing return traffic routed through load balancers to bypass the load balancer and respond directly to the client, reducing load on the load balancer and improving overall latency. For information on DSR on Windows, read <a href=\"https://techcommunity.microsoft.com/blog/networkingblog/direct-server-return-dsr-in-a-nutshell/693710\">Direct Server Return (DSR) in a nutshell</a>.<br>\nInitially introduced in v1.14, this feature has graduated to stable in v1.34.</p>\n<p>This work was done as part of <a href=\"https://kep.k8s.io/5100\">KEP #5100</a> led by SIG Windows.</p>\n<h3>Sleep action for Container lifecycle hooks</h3>\n<p>A Sleep action for containers’ PreStop and PostStart lifecycle hooks was introduced to provide a straightforward way to manage graceful shutdowns and improve overall container lifecycle management.<br>\nThe Sleep action allows containers to pause for a specified duration after starting or before termination. Using a negative or zero sleep duration returns immediately, resulting in a no-op.<br>\nThe Sleep action was introduced in Kubernetes v1.29, with zero value support added in v1.32. Both features graduated to stable in v1.34.</p>\n<p>This work was done as part of <a href=\"https://kep.k8s.io/3960\">KEP #3960</a> and <a href=\"https://kep.k8s.io/4818\">KEP #4818</a> led by SIG Node.</p>\n<h3>Linux node swap support</h3>\n<p>Historically, the lack of swap support in Kubernetes could lead to workload instability, as nodes under memory pressure often had to terminate processes abruptly. This particularly affected applications with large but infrequently accessed memory footprints and prevented more graceful resource management.</p>\n<p>To address this, configurable per-node swap support was introduced in v1.22. It has progressed through alpha and beta stages and has graduated to stable in v1.34. The primary mode, <code>LimitedSwap</code>, allows Pods to use swap within their existing memory limits, providing a direct solution to the problem. By default, the <code>kubelet</code> is configured with <code>NoSwap</code> mode, which means Kubernetes workloads cannot use swap.</p>\n<p>This feature improves workload stability and allows for more efficient resource utilization. It enables clusters to support a wider variety of applications, especially in resource-constrained environments, though administrators must consider the potential performance impact of swapping.</p>\n<p>This work was done as part of <a href=\"https://kep.k8s.io/2400\">KEP #2400</a> led by SIG Node.</p>\n<h3>Allow special characters in environment variables</h3>\n<p>The environment variable validation rules in Kubernetes have been relaxed to allow nearly all printable ASCII characters in variable names, excluding <code>=</code>. This change supports scenarios where workloads require nonstandard characters in variable names - for example, frameworks like.NET Core that use <code>:</code> to represent nested configuration keys.</p>\n<p>The relaxed validation applies to environment variables defined directly in Pod spec, as well as those injected using <code>envFrom</code> references to ConfigMaps and Secrets.</p>\n<p>This work was done as part of <a href=\"https://kep.k8s.io/4369\">KEP #4369</a> led by SIG Node.</p>\n<h3>Taint management is separated from Node lifecycle</h3>\n<p>Historically, the <code>TaintManager</code> 's logic for applying NoSchedule and NoExecute taints to nodes based on their condition (NotReady, Unreachable, etc.) was tightly coupled with the node lifecycle controller. This tight coupling made the code harder to maintain and test, and it also limited the flexibility of the taint-based eviction mechanism. This KEP refactors the <code>TaintManager</code> into its own separate controller within the Kubernetes controller manager. It is an internal architectural improvement designed to increase code modularity and maintainability. This change allows the logic for taint-based evictions to be tested and evolved independently, but it has no direct user-facing impact on how taints are used.</p>\n<p>This work was done as part of <a href=\"https://kep.k8s.io/3902\">KEP #3902</a> led by SIG Scheduling and SIG Node.</p>\n<h2>New features in Beta</h2>\n<p><em>This is a selection of some of the improvements that are now beta following the v1.34 release.</em></p>\n<h3>Pod-level resource requests and limits</h3>\n<p>Defining resource needs for Pods with multiple containers has been challenging, as requests and limits could only be set on a per-container basis. This forced developers to either over-provision resources for each container or meticulously divide the total desired resources, making configuration complex and often leading to inefficient resource allocation. To simplify this, the ability to specify resource requests and limits at the Pod level was introduced. This allows developers to define an overall resource budget for a Pod, which is then shared among its constituent containers. This feature was introduced as alpha in v1.32 and has graduated to beta in v1.34, with HPA now supporting pod-level resource specifications.</p>\n<p>The primary benefit is a more intuitive and straightforward way to manage resources for multi-container Pods. It ensures that the total resources used by all containers do not exceed the Pod's defined limits, leading to better resource planning, more accurate scheduling, and more efficient utilization of cluster resources.</p>\n<p>This work was done as part of <a href=\"https://kep.k8s.io/2837\">KEP #2837</a> led by SIG Scheduling and SIG Autoscaling.</p>\n<h3>.kuberc file for kubectl user preferences</h3>\n<p>A <code>.kuberc</code> configuration file allows you to define preferences for <code>kubectl</code>, such as default options and command aliases. Unlike the kubeconfig file, the <code>.kuberc</code> configuration file does not contain cluster details, usernames or passwords.<br>\nThis feature was introduced as alpha in v1.33, gated behind the environment variable <code>KUBECTL_KUBERC</code>. It has graduated to beta in v1.34 and is enabled by default.</p>\n<p>This work was done as part of <a href=\"https://kep.k8s.io/3104\">KEP #3104</a> led by SIG CLI.</p>\n<h3>External ServiceAccount token signing</h3>\n<p>Traditionally, Kubernetes manages ServiceAccount tokens using static signing keys that are loaded from disk at <code>kube-apiserver</code> startup. This feature introduces an <code>ExternalJWTSigner</code> gRPC service for out-of-process signing, enabling Kubernetes distributions to integrate with external key management solutions (for example, HSMs, cloud KMSes) for ServiceAccount token signing instead of static disk-based keys.</p>\n<p>Introduced as alpha in v1.32, this external JWT signing capability advances to beta and is enabled by default in v1.34.</p>\n<p>This work was done as part of <a href=\"https://kep.k8s.io/740\">KEP #740</a> led by SIG Auth.</p>\n<h3>DRA features in beta</h3>\n<h4>Admin access for secure resource monitoring</h4>\n<p>DRA supports controlled administrative access via the <code>adminAccess</code> field in ResourceClaims or ResourceClaimTemplates, allowing cluster operators to access devices already in use by others for monitoring or diagnostics. This privileged mode is limited to users authorized to create such objects in namespaces labeled <code>resource.k8s.io/admin-access: \"true\"</code>, ensuring regular workloads remain unaffected. Graduating to beta in v1.34, this feature provides secure introspection capabilities while preserving workload isolation through namespace-based authorization checks.</p>\n<p>This work was done as part of <a href=\"https://kep.k8s.io/5018\">KEP #5018</a> led by WG Device Management and SIG Auth.</p>\n<h4>Prioritized alternatives in ResourceClaims and ResourceClaimTemplates</h4>\n<p>While a workload might run best on a single high-performance GPU, it might also be able to run on two mid-level GPUs.<br>\nWith the feature gate <code>DRAPrioritizedList</code> (now enabled by default), ResourceClaims and ResourceClaimTemplates get a new field named <code>firstAvailable</code>. This field is an ordered list that allows users to specify that a request may be satisfied in different ways, including allocating nothing at all if specific hardware is not available. The scheduler will attempt to satisfy the alternatives in the list in order, so the workload will be allocated the best set of devices available in the cluster.</p>\n<p>This work was done as part of <a href=\"https://kep.k8s.io/4816\">KEP #4816</a> led by WG Device Management.</p>\n<h4>The kubelet reports allocated DRA resources</h4>\n<p>The <code>kubelet</code> 's API has been updated to report on Pod resources allocated through DRA. This allows node monitoring agents to discover the allocated DRA resources for Pods on a node. Additionally, it enables node components to use the PodResourcesAPI and leverage this DRA information when developing new features and integrations.<br>\nStarting from Kubernetes v1.34, this feature is enabled by default.</p>\n<p>This work was done as part of <a href=\"https://kep.k8s.io/3695\">KEP #3695</a> led by WG Device Management.</p>\n<h3>kube-scheduler non-blocking API calls</h3>\n<p>The <code>kube-scheduler</code> makes blocking API calls during scheduling cycles, creating performance bottlenecks. This feature introduces asynchronous API handling through a prioritized queue system with request deduplication, allowing the scheduler to continue processing Pods while API operations complete in the background. Key benefits include reduced scheduling latency, prevention of scheduler thread starvation during API delays, and immediate retry capability for unschedulable Pods. The implementation maintains backward compatibility and adds metrics for monitoring pending API operations.</p>\n<p>This work was done as part of <a href=\"https://kep.k8s.io/5229\">KEP #5229</a> led by SIG Scheduling.</p>\n<h3>Mutating admission policies</h3>\n<p><a href=\"https://kubernetes.io/docs/reference/access-authn-authz/mutating-admission-policy/\">MutatingAdmissionPolicies</a> offer a declarative, in-process alternative to mutating admission webhooks. This feature leverages CEL's object instantiation and JSON Patch strategies, combined with Server Side Apply’s merge algorithms.<br>\nThis significantly simplifies admission control by allowing administrators to define mutation rules directly in the API server.<br>\nIntroduced as alpha in v1.32, mutating admission policies has graduated to beta in v1.34.</p>\n<p>This work was done as part of <a href=\"https://kep.k8s.io/3962\">KEP #3962</a> led by SIG API Machinery.</p>\n<h3>Snapshottable API server cache</h3>\n<p>The <code>kube-apiserver</code> 's caching mechanism (watch cache) efficiently serves requests for the latest observed state. However, <strong>list</strong> requests for previous states (for example, via pagination or by specifying a <code>resourceVersion</code>) often bypass this cache and are served directly from etcd. This direct etcd access significantly increases performance costs and can lead to stability issues, particularly with large resources, due to memory pressure from transferring large data blobs.<br>\nWith the <code>ListFromCacheSnapshot</code> feature gate enabled by default, <code>kube-apiserver</code> will attempt to serve the response from snapshots if one is available with <code>resourceVersion</code> older than requested. The <code>kube-apiserver</code> starts with no snapshots, creates a new snapshot on every watch event, and keeps them until it detects etcd is compacted or if cache is full with events older than 75 seconds. If the provided <code>resourceVersion</code> is unavailable, the server will fallback to etcd.</p>\n<p>This work was done as part of <a href=\"https://kep.k8s.io/4988\">KEP #4988</a> led by SIG API Machinery.</p>\n<h3>Tooling for declarative validation of Kubernetes-native types</h3>\n<p>Prior to this release, validation rules for the APIs built into Kubernetes were written entirely by hand, which makes them difficult for maintainers to discover, understand, improve or test. There was no single way to find all the validation rules that might apply to an API.<em>Declarative validation</em> benefits Kubernetes maintainers by making API development, maintenance, and review easier while enabling programmatic inspection for better tooling and documentation. For people using Kubernetes libraries to write their own code (for example: a controller), the new approach streamlines adding new fields through IDL tags, rather than complex validation functions. This change helps speed up API creation by automating validation boilerplate, and provides more relevant error messages by performing validation on versioned types.<br>\nThis enhancement (which graduated to beta in v1.33 and continues as beta in v1.34) brings CEL-based validation rules to native Kubernetes types. It allows for more granular and declarative validation to be defined directly in the type definitions, improving API consistency and developer experience.</p>\n<p>This work was done as part of <a href=\"https://kep.k8s.io/5073\">KEP #5073</a> led by SIG API Machinery.</p>\n<h3>Streaming informers for list requests</h3>\n<p>The streaming informers feature, which has been in beta since v1.32, gains further beta refinements in v1.34. This capability allows <strong>list</strong> requests to return data as a continuous stream of objects from the API server’s watch cache, rather than assembling paged results directly from etcd. By reusing the same mechanics used for <strong>watch</strong> operations, the API server can serve large datasets while keeping memory usage steady and avoiding allocation spikes that can affect stability.</p>\n<p>In this release, the <code>kube-apiserver</code> and <code>kube-controller-manager</code> both take advantage of the new <code>WatchList</code> mechanism by default. For the <code>kube-apiserver</code>, this means list requests are streamed more efficiently, while the <code>kube-controller-manager</code> benefits from a more memory-efficient and predictable way to work with informers. Together, these improvements reduce memory pressure during large list operations, and improve reliability under sustained load, making list streaming more predictable and efficient.</p>\n<p>This work was done as part of <a href=\"https://kep.k8s.io/3157\">KEP #3157</a> led by SIG API Machinery and SIG Scalability.</p>\n<h3>Graceful node shutdown handling for Windows nodes</h3>\n<p>The <code>kubelet</code> on Windows nodes can now detect system shutdown events and begin graceful termination of running Pods. This mirrors existing behavior on Linux and helps ensure workloads exit cleanly during planned shutdowns or restarts.<br>\nWhen the system begins shutting down, the <code>kubelet</code> reacts by using standard termination logic. It respects the configured lifecycle hooks and grace periods, giving Pods time to stop before the node powers off. The feature relies on Windows pre-shutdown notifications to coordinate this process. This enhancement improves workload reliability during maintenance, restarts, or system updates. It is now in beta and enabled by default.</p>\n<p>This work was done as part of <a href=\"https://kep.k8s.io/4802\">KEP #4802</a> led by SIG Windows.</p>\n<h3>In-place Pod resize improvements</h3>\n<p>Graduated to beta and enabled by default in v1.33, in-place Pod resizing receives further improvements in v1.34. These include support for decreasing memory usage and integration with Pod-level resources.</p>\n<p>This feature remains in beta in v1.34. For detailed usage instructions and examples, refer to the documentation: <a href=\"https://kubernetes.io/docs/tasks/configure-pod-container/resize-container-resources/\">Resize CPU and Memory Resources assigned to Containers</a>.</p>\n<p>This work was done as part of <a href=\"https://kep.k8s.io/1287\">KEP #1287</a> led by SIG Node and SIG Autoscaling.</p>\n<h2>New features in Alpha</h2>\n<p><em>This is a selection of some of the improvements that are now alpha following the v1.34 release.</em></p>\n<h3>Pod certificates for mTLS authentication</h3>\n<p>Authenticating workloads within a cluster, especially for communication with the API server, has primarily relied on ServiceAccount tokens. While effective, these tokens aren't always ideal for establishing a strong, verifiable identity for mutual TLS (mTLS) and can present challenges when integrating with external systems that expect certificate-based authentication.<br>\nKubernetes v1.34 introduces a built-in mechanism for Pods to obtain X.509 certificates via <a href=\"https://kubernetes.io/docs/reference/access-authn-authz/certificate-signing-requests/#pod-certificate-requests\">PodCertificateRequests</a>. The <code>kubelet</code> can request and manage certificates for Pods, which can then be used to authenticate to the Kubernetes API server and other services using mTLS. The primary benefit is a more robust and flexible identity mechanism for Pods. It provides a native way to implement strong mTLS authentication without relying solely on bearer tokens, aligning Kubernetes with standard security practices and simplifying integrations with certificate-aware observability and security tooling.</p>\n<p>This work was done as part of <a href=\"https://kep.k8s.io/4317\">KEP #4317</a> led by SIG Auth.</p>\n<h3>\"Restricted\" Pod security standard now forbids remote probes</h3>\n<p>The <code>host</code> field within probes and lifecycle handlers allows users to specify an entity other than the <code>podIP</code> for the <code>kubelet</code> to probe. However, this opens up a route for misuse and for attacks that bypass security controls, since the <code>host</code> field could be set to <strong>any</strong> value, including security sensitive external hosts, or localhost on the node. In Kubernetes v1.34, Pods only meet the <a href=\"https://kubernetes.io/docs/concepts/security/pod-security-standards/#restricted\">Restricted</a> Pod security standard if they either leave the <code>host</code> field unset, or if they don't even use this kind of probe. You can use <em>Pod security admission</em>, or a third party solution, to enforce that Pods meet this standard. Because these are security controls, check the documentation to understand the limitations and behavior of the enforcement mechanism you choose.</p>\n<p>This work was done as part of <a href=\"https://kep.k8s.io/4940\">KEP #4940</a> led by SIG Auth.</p>\n<h3>Use.status.nominatedNodeName to express Pod placement</h3>\n<p>When the <code>kube-scheduler</code> takes time to bind Pods to Nodes, cluster autoscalers may not understand that a Pod will be bound to a specific Node. Consequently, they may mistakenly consider the Node as underutilized and delete it.<br>\nTo address this issue, the <code>kube-scheduler</code> can use <code>.status.nominatedNodeName</code> not only to indicate ongoing preemption but also to express Pod placement intentions. By enabling the <code>NominatedNodeNameForExpectation</code> feature gate, the scheduler uses this field to indicate where a Pod will be bound. This exposes internal reservations to help external components make informed decisions.</p>\n<p>This work was done as part of <a href=\"https://kep.k8s.io/5278\">KEP #5278</a> led by SIG Scheduling.</p>\n<h3>DRA features in alpha</h3>\n<h4>Resource health status for DRA</h4>\n<p>It can be difficult to know when a Pod is using a device that has failed or is temporarily unhealthy, which makes troubleshooting Pod crashes challenging or impossible.<br>\nResource Health Status for DRA improves observability by exposing the health status of devices allocated to a Pod in the Pod’s status. This makes it easier to identify the cause of Pod issues related to unhealthy devices and respond appropriately.<br>\nTo enable this functionality, the <code>ResourceHealthStatus</code> feature gate must be enabled, and the DRA driver must implement the <code>DRAResourceHealth</code> gRPC service.</p>\n<p>This work was done as part of <a href=\"https://kep.k8s.io/4680\">KEP #4680</a> led by WG Device Management.</p>\n<h4>Extended resource mapping</h4>\n<p>Extended resource mapping provides a simpler alternative to DRA's expressive and flexible approach by offering a straightforward way to describe resource capacity and consumption. This feature enables cluster administrators to advertise DRA-managed resources as <em>extended resources</em>, allowing application developers and operators to continue using the familiar container’s <code>.spec.resources</code> syntax to consume them.<br>\nThis enables existing workloads to adopt DRA without modifications, simplifying the transition to DRA for both application developers and cluster administrators.</p>\n<p>This work was done as part of <a href=\"https://kep.k8s.io/5004\">KEP #5004</a> led by WG Device Management.</p>\n<h4>DRA consumable capacity</h4>\n<p>Kubernetes v1.33 added support for resource drivers to advertise slices of a device that are available, rather than exposing the entire device as an all-or-nothing resource. However, this approach couldn't handle scenarios where device drivers manage fine-grained, dynamic portions of a device resource based on user demand, or share those resources independently of ResourceClaims, which are restricted by their spec and namespace.<br>\nEnabling the <code>DRAConsumableCapacity</code> feature gate (introduced as alpha in v1.34) allows resource drivers to share the same device, or even a slice of a device, across multiple ResourceClaims or across multiple DeviceRequests. The feature also extends the scheduler to support allocating portions of device resources, as defined in the <code>capacity</code> field. This DRA feature improves device sharing across namespaces and claims, tailoring it to Pod needs. It enables drivers to enforce capacity limits, enhances scheduling, and supports new use cases like bandwidth-aware networking and multi-tenant sharing.</p>\n<p>This work was done as part of <a href=\"https://kep.k8s.io/5075\">KEP #5075</a> led by WG Device Management.</p>\n<h4>Device binding conditions</h4>\n<p>The Kubernetes scheduler gets more reliable by delaying binding a Pod to a Node until its required external resources, such as attachable devices or FPGAs, are confirmed to be ready.<br>\nThis delay mechanism is implemented in the <a href=\"https://kubernetes.io/docs/concepts/scheduling-eviction/scheduling-framework/#pre-bind\">PreBind phase</a> of the scheduling framework. During this phase, the scheduler checks whether all required device conditions are satisfied before proceeding with binding. This enables coordination with external device controllers, ensuring more robust, predictable scheduling.</p>\n<p>This work was done as part of <a href=\"https://kep.k8s.io/5007\">KEP #5007</a> led by WG Device Management.</p>\n<h3>Container restart rules</h3>\n<p>Currently, all containers within a Pod will follow the same <code>.spec.restartPolicy</code> when exited or crashed. However, Pods that run multiple containers might have different restart requirements for each container. For example, for init containers used to perform initialization, you may not want to retry initialization if they fail. Similarly, in ML research environments with long-running training workloads, containers that fail with retriable exit codes should restart quickly in place, rather than triggering Pod recreation and losing progress.<br>\nKubernetes v1.34 introduces the <code>ContainerRestartRules</code> feature gate. When enabled, a <code>restartPolicy</code> can be specified for each container within a Pod. A <code>restartPolicyRules</code> list can also be defined to override <code>restartPolicy</code> based on the last exit code. This provides the fine-grained control needed to handle complex scenarios and better utilization of compute resources.</p>\n<p>This work was done as part of <a href=\"https://kep.k8s.io/5307\">KEP #5307</a> led by SIG Node.</p>\n<h3>Load environment variables from files created in runtime</h3>\n<p>Application developers have long requested greater flexibility in declaring environment variables. Traditionally, environment variables are declared on the API server side via static values, ConfigMaps, or Secrets.</p>\n<p>Behind the <code>EnvFiles</code> feature gate, Kubernetes v1.34 introduces the ability to declare environment variables at runtime. One container (typically an init container) can generate the variable and store it in a file, and a subsequent container can start with the environment variable loaded from that file. This approach eliminates the need to \"wrap\" the target container's entry point, enabling more flexible in-Pod container orchestration.</p>\n<p>This feature particularly benefits AI/ML training workloads, where each Pod in a training Job requires initialization with runtime-defined values.</p>\n<p>This work was done as part of <a href=\"https://kep.k8s.io/3721\">KEP #5307</a> led by SIG Node.</p>\n<h2>Graduations, deprecations, and removals in v1.34</h2>\n<h3>Graduations to stable</h3>\n<p>This lists all the features that graduated to stable (also known as <em>general availability</em>). For a full list of updates including new features and graduations from alpha to beta, see the release notes.</p>\n<p>This release includes a total of 23 enhancements promoted to stable:</p>\n<ul>\n<li><a href=\"https://kep.k8s.io/4369\">Allow almost all printable ASCII characters in environment variables</a></li>\n<li><a href=\"https://kep.k8s.io/3939\">Allow for recreation of pods once fully terminated in the job controller</a></li>\n<li><a href=\"https://kep.k8s.io/4818\">Allow zero value for Sleep Action of PreStop Hook</a></li>\n<li><a href=\"https://kep.k8s.io/647\">API Server tracing</a></li>\n<li><a href=\"https://kep.k8s.io/24\">AppArmor support</a></li>\n<li><a href=\"https://kep.k8s.io/4601\">Authorize with Field and Label Selectors</a></li>\n<li><a href=\"https://kep.k8s.io/2340\">Consistent Reads from Cache</a></li>\n<li><a href=\"https://kep.k8s.io/3902\">Decouple TaintManager from NodeLifecycleController</a></li>\n<li><a href=\"https://kep.k8s.io/4033\">Discover cgroup driver from CRI</a></li>\n<li><a href=\"https://kep.k8s.io/4381\">DRA: structured parameters</a></li>\n<li><a href=\"https://kep.k8s.io/3960\">Introducing Sleep Action for PreStop Hook</a></li>\n<li><a href=\"https://kep.k8s.io/2831\">Kubelet OpenTelemetry Tracing</a></li>\n<li><a href=\"https://kep.k8s.io/3751\">Kubernetes VolumeAttributesClass ModifyVolume</a></li>\n<li><a href=\"https://kep.k8s.io/2400\">Node memory swap support</a></li>\n<li><a href=\"https://kep.k8s.io/4633\">Only allow anonymous auth for configured endpoints</a></li>\n<li><a href=\"https://kep.k8s.io/5080\">Ordered namespace deletion</a></li>\n<li><a href=\"https://kep.k8s.io/4247\">Per-plugin callback functions for accurate requeueing in kube-scheduler</a></li>\n<li><a href=\"https://kep.k8s.io/4427\">Relaxed DNS search string validation</a></li>\n<li><a href=\"https://kep.k8s.io/4568\">Resilient Watchcache Initialization</a></li>\n<li><a href=\"https://kep.k8s.io/5116\">Streaming Encoding for LIST Responses</a></li>\n<li><a href=\"https://kep.k8s.io/3331\">Structured Authentication Config</a></li>\n<li><a href=\"https://kep.k8s.io/5100\">Support for Direct Service Return (DSR) and overlay networking in Windows kube-proxy</a></li>\n<li><a href=\"https://kep.k8s.io/1790\">Support recovery from volume expansion failure</a></li>\n</ul>\n<h3>Deprecations and removals</h3>\n<p>As Kubernetes develops and matures, features may be deprecated, removed, or replaced with better ones to improve the project's overall health. See the Kubernetes <a href=\"https://kubernetes.io/docs/reference/using-api/deprecation-policy/\">deprecation and removal policy</a> for more details on this process. Kubernetes v1.34 includes a couple of deprecations.</p>\n<h4>Manual cgroup driver configuration is deprecated</h4>\n<p>Historically, configuring the correct cgroup driver has been a pain point for users running Kubernetes clusters. Kubernetes v1.28 added a way for the <code>kubelet</code> to query the CRI implementation and find which cgroup driver to use. That automated detection is now <strong>strongly recommended</strong> and support for it has graduated to stable in v1.34. If your CRI container runtime does not support the ability to report the cgroup driver it needs, you should upgrade or change your container runtime. The <code>cgroupDriver</code> configuration setting in the <code>kubelet</code> configuration file is now deprecated. The corresponding command-line option <code>--cgroup-driver</code> was previously deprecated, as Kubernetes recommends using the configuration file instead. Both the configuration setting and command-line option will be removed in a future release, that removal will not happen before the v1.36 minor release.</p>\n<p>This work was done as part of <a href=\"https://kep.k8s.io/4033\">KEP #4033</a> led by SIG Node.</p>\n<h4>Kubernetes to end containerd 1.x support in v1.36</h4>\n<p>While Kubernetes v1.34 still supports containerd 1.7 and other LTS releases of containerd, as a consequence of automated cgroup driver detection, the Kubernetes SIG Node community has formally agreed upon a final support timeline for containerd v1.X. The last Kubernetes release to offer this support will be v1.35 (aligned with containerd 1.7 EOL). This is an early warning that if you are using containerd 1.X, consider switching to 2.0+ soon. You are able to monitor the <code>kubelet_cri_losing_support</code> metric to determine if any nodes in your cluster are using a containerd version that will soon be outdated.</p>\n<p>This work was done as part of <a href=\"https://kep.k8s.io/4033\">KEP #4033</a> led by SIG Node.</p>\n<h4>PreferClose traffic distribution is deprecated</h4>\n<p>The <code>spec.trafficDistribution</code> field within a Kubernetes <a href=\"https://kubernetes.io/docs/concepts/services-networking/service/\">Service</a> allows users to express preferences for how traffic should be routed to Service endpoints.</p>\n<p><a href=\"https://kep.k8s.io/3015\">KEP-3015</a> deprecates <code>PreferClose</code> and introduces two additional values: <code>PreferSameZone</code> and <code>PreferSameNode</code>. <code>PreferSameZone</code> is an alias for the existing <code>PreferClose</code> to clarify its semantics. <code>PreferSameNode</code> allows connections to be delivered to a local endpoint when possible, falling back to a remote endpoint when not possible.</p>\n<p>This feature was introduced in v1.33 behind the <code>PreferSameTrafficDistribution</code> feature gate. It has graduated to beta in v1.34 and is enabled by default.</p>\n<p>This work was done as part of <a href=\"https://kep.k8s.io/3015\">KEP #3015</a> led by SIG Network.</p>\n<h2>Release notes</h2>\n<p>Check out the full details of the Kubernetes v1.34 release in our <a href=\"https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.34.md\">release notes</a>.</p>\n<h2>Availability</h2>\n<p>Kubernetes v1.34 is available for download on or on the <a href=\"https://kubernetes.io/releases/download/\">Kubernetes download page</a>.</p>\n<p>To get started with Kubernetes, check out these <a href=\"https://kubernetes.io/docs/tutorials/\">interactive tutorials</a> or run local Kubernetes clusters using <a href=\"https://minikube.sigs.k8s.io/\">minikube</a>. You can also easily install v1.34 using <a href=\"https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/\">kubeadm</a>.</p>\n<h2>Release Team</h2>\n<p>Kubernetes is only possible with the support, commitment, and hard work of its community. Each release team is made up of dedicated community volunteers who work together to build the many pieces that make up the Kubernetes releases you rely on. This requires the specialized skills of people from all corners of our community, from the code itself to its documentation and project management.</p>\n<p><a href=\"https://github.com/cncf/memorials/blob/main/rodolfo-martinez.md\">We honor the memory of Rodolfo \"Rodo\" Martínez Vega</a>, a dedicated contributor whose passion for technology and community building left a mark on the Kubernetes community. Rodo served as a member of the Kubernetes Release Team across multiple releases, including v1.22-v1.23 and v1.25-v1.30, demonstrating unwavering commitment to the project's success and stability.<br>\nBeyond his Release Team contributions, Rodo was deeply involved in fostering the Cloud Native LATAM community, helping to bridge language and cultural barriers in the space. His work on the Spanish version of Kubernetes documentation and the CNCF Glossary exemplified his dedication to making knowledge accessible to Spanish-speaking developers worldwide. Rodo's legacy lives on through the countless community members he mentored, the releases he helped deliver, and the vibrant LATAM Kubernetes community he helped cultivate.</p>\n<p>We would like to thank the entire <a href=\"https://github.com/kubernetes/sig-release/blob/master/releases/release-1.34/release-team.md\">Release Team</a> for the hours spent hard at work to deliver the Kubernetes v1.34 release to our community. The Release Team's membership ranges from first-time shadows to returning team leads with experience forged over several release cycles. A very special thanks goes out to our release lead, Vyom Yadav, for guiding us through a successful release cycle, for his hands-on approach to solving challenges, and for bringing the energy and care that drives our community forward.</p>\n<h2>Project Velocity</h2>\n<p>The CNCF K8s <a href=\"https://k8s.devstats.cncf.io/d/11/companies-contributing-in-repository-groups?orgId=1&#x26;var-period=m&#x26;var-repogroup_name=All\">DevStats</a> project aggregates a number of interesting data points related to the velocity of Kubernetes and various sub-projects. This includes everything from individual contributions to the number of companies that are contributing and is an illustration of the depth and breadth of effort that goes into evolving this ecosystem.</p>\n<p>During the v1.34 release cycle, which spanned 15 weeks from 19th May 2025 to 27th August 2025, Kubernetes received contributions from as many as 106 different companies and 491 individuals. In the wider cloud native ecosystem, the figure goes up to 370 companies, counting 2235 total contributors.</p>\n<p>Note that \"contribution\" counts when someone makes a commit, code review, comment, creates an issue or PR, reviews a PR (including blogs and documentation) or comments on issues and PRs.<br>\nIf you are interested in contributing, visit <a href=\"https://www.kubernetes.dev/docs/guide/#getting-started\">Getting Started</a> on our contributor website.</p>\n<p>Source for this data:</p>\n<ul>\n<li><a href=\"https://k8s.devstats.cncf.io/d/11/companies-contributing-in-repository-groups?orgId=1&#x26;from=1747609200000&#x26;to=1756335599000&#x26;var-period=d28&#x26;var-repogroup_name=Kubernetes&#x26;var-repo_name=kubernetes%2Fkubernetes\">Companies contributing to Kubernetes</a></li>\n<li><a href=\"https://k8s.devstats.cncf.io/d/11/companies-contributing-in-repository-groups?orgId=1&#x26;from=1747609200000&#x26;to=1756335599000&#x26;var-period=d28&#x26;var-repogroup_name=All&#x26;var-repo_name=kubernetes%2Fkubernetes\">Overall ecosystem contributions</a></li>\n</ul>\n<h2>Event Update</h2>\n<p>Explore upcoming Kubernetes and cloud native events, including KubeCon + CloudNativeCon, KCD, and other notable conferences worldwide. Stay informed and get involved with the Kubernetes community!</p>\n<p><strong>August 2025</strong></p>\n<ul>\n<li><a href=\"https://community.cncf.io/events/details/cncf-kcd-colombia-presents-kcd-colombia-2025/\"><strong>KCD - Kubernetes Community Days: Colombia</strong></a>: Aug 28, 2025 | Bogotá, Colombia</li>\n</ul>\n<p><strong>September 2025</strong></p>\n<ul>\n<li><a href=\"https://community.cncf.io/events/details/cncf-cloud-native-sydney-presents-cloudcon-sydney-sydney-international-convention-centre-910-september/\"><strong>CloudCon Sydney</strong></a>: Sep 9–10, 2025 | Sydney, Australia.</li>\n<li><a href=\"https://community.cncf.io/events/details/cncf-kcd-sf-bay-area-presents-kcd-san-francisco-bay-area/\"><strong>KCD - Kubernetes Community Days: San Francisco Bay Area</strong></a>: Sep 9, 2025 | San Francisco, USA</li>\n<li><a href=\"https://community.cncf.io/events/details/cncf-kcd-washington-dc-presents-kcd-washington-dc-2025/\"><strong>KCD - Kubernetes Community Days: Washington DC</strong></a>: Sep 16, 2025 | Washington, D.C., USA</li>\n<li><a href=\"https://community.cncf.io/events/details/cncf-kcd-sofia-presents-kubernetes-community-days-sofia/\"><strong>KCD - Kubernetes Community Days: Sofia</strong></a>: Sep 18, 2025 | Sofia, Bulgaria</li>\n<li><a href=\"https://community.cncf.io/events/details/cncf-kcd-el-salvador-presents-kcd-el-salvador/\"><strong>KCD - Kubernetes Community Days: El Salvador</strong></a>: Sep 20, 2025 | San Salvador, El Salvador</li>\n</ul>\n<p><strong>October 2025</strong></p>\n<ul>\n<li><a href=\"https://community.cncf.io/events/details/cncf-kcd-warsaw-presents-kcd-warsaw-2025/\"><strong>KCD - Kubernetes Community Days: Warsaw</strong></a>: Oct 9, 2025 | Warsaw, Poland</li>\n<li><a href=\"https://community.cncf.io/events/details/cncf-kcd-uk-presents-kubernetes-community-days-uk-edinburgh-2025/\"><strong>KCD - Kubernetes Community Days: Edinburgh</strong></a>: Oct 21, 2025 | Edinburgh, United Kingdom</li>\n<li><a href=\"https://community.cncf.io/events/details/cncf-kcd-sri-lanka-presents-kcd-sri-lanka-2025/\"><strong>KCD - Kubernetes Community Days: Sri Lanka</strong></a>: Oct 26, 2025 | Colombo, Sri Lanka</li>\n</ul>\n<p><strong>November 2025</strong></p>\n<ul>\n<li><a href=\"https://community.cncf.io/events/details/cncf-kcd-porto-presents-kcd-porto-2025/\"><strong>KCD - Kubernetes Community Days: Porto</strong></a>: Nov 3, 2025 | Porto, Portugal</li>\n<li><a href=\"https://events.linuxfoundation.org/kubecon-cloudnativecon-north-america/\"><strong>KubeCon + CloudNativeCon North America 2025</strong></a>: Nov 10-13, 2025 | Atlanta, USA</li>\n<li><a href=\"https://sessionize.com/kcd-hangzhou-and-oicd-2025/\"><strong>KCD - Kubernetes Community Days: Hangzhou</strong></a>: Nov 15, 2025 | Hangzhou, China</li>\n</ul>\n<p><strong>December 2025</strong></p>\n<ul>\n<li><a href=\"https://community.cncf.io/events/details/cncf-kcd-suisse-romande-presents-kcd-suisse-romande/\"><strong>KCD - Kubernetes Community Days: Suisse Romande</strong></a>: Dec 4, 2025 | Geneva, Switzerland</li>\n</ul>\n<p>You can find the latest event details <a href=\"https://community.cncf.io/events/#/list\">here</a>.</p>\n<h2>Upcoming Release Webinar</h2>\n<p>Join members of the Kubernetes v1.34 Release Team on <strong>Wednesday, September 24th 2025 at 4:00 PM (UTC)</strong>, to learn about the release highlights of this release. For more information and registration, visit the <a href=\"https://community.cncf.io/events/details/cncf-cncf-online-programs-presents-cloud-native-live-kubernetes-v134-release/\">event page</a> on the CNCF Online Programs site.</p>\n<h2>Get Involved</h2>\n<p>The simplest way to get involved with Kubernetes is by joining one of the many <a href=\"https://github.com/kubernetes/community/blob/master/sig-list.md\">Special Interest Groups</a> (SIGs) that align with your interests. Have something you’d like to broadcast to the Kubernetes community? Share your voice at our weekly <a href=\"https://github.com/kubernetes/community/tree/master/communication\">community meeting</a>, and through the channels below. Thank you for your continued feedback and support.</p>\n<ul>\n<li>Follow us on Bluesky <a href=\"https://bsky.app/profile/kubernetes.io\">@Kubernetesio</a> for the latest updates</li>\n<li>Join the community discussion on <a href=\"https://discuss.kubernetes.io/\">Discuss</a></li>\n<li>Join the community on <a href=\"http://slack.k8s.io/\">Slack</a></li>\n<li>Post questions (or answer questions) on <a href=\"http://stackoverflow.com/questions/tagged/kubernetes\">Stack Overflow</a></li>\n<li>Share your Kubernetes <a href=\"https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform\">story</a></li>\n<li>Read more about what’s happening with Kubernetes on the <a href=\"https://kubernetes.io/blog/\">blog</a></li>\n<li>Learn more about the <a href=\"https://github.com/kubernetes/sig-release/tree/master/release-team\">Kubernetes Release Team</a></li>\n</ul>\n","pageTitle":"Kubernetes v1.34: Of Wind & Will (O' WaW) - by Graziano Casto","pageDescription":"Kubernetes v1.34: Of Wind & Will (O' WaW) - by Graziano Casto","pageLink":"https://castograziano.com/articles/kubernetes-v134-announcement","pageImage":"https://castograziano.com/casto_graziano_personal_website.png"},"schema":{"@context":"https://schema.org","@type":"BlogPosting","headline":"Kubernetes v1.34: Of Wind & Will (O' WaW)","datePublished":"2025-08-27T00:00:00.000Z","dateModified":"2025-08-27T00:00:00.000Z","author":[{"@type":"Person","name":"Graziano Casto","url":"https://castograziano.com/about-me"}]}},"__N_SSG":true}