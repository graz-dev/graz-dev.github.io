{"pageProps":{"data":{"article":{"title":"Kubernetes v1.35 Sneak Peek","date":"2025-11-26"},"children":"<p><a href=\"https://kubernetes.io/blog/2025/11/26/kubernetes-v1-35-sneak-peek/\">Published by Kubernetes co-authored with Aakanksha Bhende, Arujjwal Negi, Chad M. Crowell, Swathi Rao</a></p>\n<p>As the release of Kubernetes v1.35 approaches, the Kubernetes project continues to evolve. Features may be deprecated, removed, or replaced to improve the project's overall health. This blog post outlines planned changes for the v1.35 release that the release team believes you should be aware of to ensure the continued smooth operation of your Kubernetes cluster(s), and to keep you up to date with the latest developments. The information below is based on the current status of the v1.35 release and is subject to change before the final release date.</p>\n<h3>Deprecations and removals for Kubernetes v1.35</h3>\n<h4>cgroup v1 support</h4>\n<p>On Linux nodes, container runtimes typically rely on cgroups (short for \"control groups\"). Support for using cgroup v2 has been stable in Kubernetes since v1.25, providing an alternative to the original v1 cgroup support. While cgroup v1 provided the initial resource control mechanism, it suffered from well-known inconsistencies and limitations. Adding support for cgroup v2 allowed use of a unified control group hierarchy, improved resource isolation, and served as the foundation for modern features, making legacy cgroup v1 support ready for removal. The removal of cgroup v1 support will only impact cluster administrators running nodes on older Linux distributions that do not support cgroup v2; on those nodes, the <code>kubelet</code> will fail to start. Administrators must migrate their nodes to systems with cgroup v2 enabled. More details on compatibility requirements will be available in a blog post soon after the v1.35 release.</p>\n<p>To learn more, read <a href=\"https://kubernetes.io/docs/concepts/architecture/cgroups/\">about cgroup v2</a>;<br>\nyou can also track the switchover work via <a href=\"https://kep.k8s.io/5573\">KEP-5573: Remove cgroup v1 support</a>.</p>\n<h4>Deprecation of ipvs mode in kube-proxy</h4>\n<p>Many releases ago, the Kubernetes project implemented an <a href=\"https://kubernetes.io/docs/reference/networking/virtual-ips/#proxy-mode-ipvs\">ipvs</a> mode in <code>kube-proxy</code>. It was adopted as a way to provide high-performance service load balancing, with better performance than the existing <code>iptables</code> mode. However, maintaining feature parity between ipvs and other kube-proxy modes became difficult, due to technical complexity and diverging requirements. This created significant technical debt and made the ipvs backend impractical to support alongside newer networking capabilities.</p>\n<p>The Kubernetes project intends to deprecate kube-proxy <code>ipvs</code> mode in the v1.35 release, to streamline the <code>kube-proxy</code> codebase. For Linux nodes, the recommended <code>kube-proxy</code> mode is already <a href=\"https://kubernetes.io/docs/reference/networking/virtual-ips/#proxy-mode-nftables\">nftables</a>.</p>\n<p>You can find more in <a href=\"https://kep.k8s.io/5495\">KEP-5495: Deprecate ipvs mode in kube-proxy</a></p>\n<h4>Kubernetes is deprecating containerd v1.y support</h4>\n<p>While Kubernetes v1.35 still supports containerd 1.7 and other LTS releases of containerd, as a consequence of automated cgroup driver detection, the Kubernetes SIG Node community has formally agreed upon a final support timeline for containerd v1.X. Kubernetes v1.35 is the last release to offer this support (aligned with containerd 1.7 EOL).</p>\n<p>This is a final warning that if you are using containerd 1.X, you must switch to 2.0 or later before upgrading Kubernetes to the next version. You are able to monitor the <code>kubelet_cri_losing_support</code> metric to determine if any nodes in your cluster are using a containerd version that will soon be unsupported.</p>\n<p>You can find more in the <a href=\"https://kubernetes.io/blog/2025/09/12/kubernetes-v1-34-cri-cgroup-driver-lookup-now-ga/#announcement-kubernetes-is-deprecating-containerd-v1-y-support\">official blog post</a> or in <a href=\"https://kep.k8s.io/4033\">KEP-4033: Discover cgroup driver from CRI</a></p>\n<p>The following enhancements are some of those likely to be included in the v1.35 release. This is not a commitment, and the release content is subject to change.</p>\n<h4>Node declared features</h4>\n<p>When scheduling Pods, Kubernetes uses node labels, taints, and tolerations to match workload requirements with node capabilities. However, managing feature compatibility becomes challenging during cluster upgrades due to version skew between the control plane and nodes. This can lead to Pods being scheduled on nodes that lack required features, resulting in runtime failures.</p>\n<p>The <em>node declared features</em> framework will introduce a standard mechanism for nodes to declare their supported Kubernetes features. With the new alpha feature enabled, a Node reports the features it can support, publishing this information to the control plane through a new <code>.status.declaredFeatures</code> field. Then, the <code>kube-scheduler</code>, admission controllers and third-party components can use these declarations. For example, you can enforce scheduling and API validation constraints, ensuring that Pods run only on compatible nodes.</p>\n<p>This approach reduces manual node labeling, improves scheduling accuracy, and prevents incompatible pod placements proactively. It also integrates with the Cluster Autoscaler for informed scale-up decisions. Feature declarations are temporary and tied to Kubernetes feature gates, enabling safe rollout and cleanup.</p>\n<p>Targeting alpha in v1.35, <em>node declared features</em> aims to solve version skew scheduling issues by making node capabilities explicit, enhancing reliability and cluster stability in heterogeneous version environments.</p>\n<p>To learn more about this before the official documentation is published, you can read <a href=\"https://kep.k8s.io/5328\">KEP-5328</a>.</p>\n<h4>In-place update of Pod resources</h4>\n<p>Kubernetes is graduating in-place updates for Pod resources to General Availability (GA). This feature allows users to adjust <code>cpu</code> and <code>memory</code> resources without restarting Pods or Containers. Previously, such modifications required recreating Pods, which could disrupt workloads, particularly for stateful or batch applications. Previous Kubernetes releases already allowed you to change infrastructure resources settings (requests and limits) for existing Pods. This allows for smoother <a href=\"https://kubernetes.io/docs/concepts/workloads/autoscaling/vertical-pod-autoscale/\">vertical scaling</a>, improves efficiency, and can also simplify solution development.</p>\n<p>The Container Runtime Interface (CRI) has also been improved, extending the <code>UpdateContainerResources</code> API for Windows and future runtimes while allowing <code>ContainerStatus</code> to report real-time resource configurations. Together, these changes make scaling in Kubernetes faster, more flexible, and disruption-free. The feature was introduced as alpha in v1.27, graduated to beta in v1.33, and is targeting graduation to stable in v1.35.</p>\n<p>You can find more in <a href=\"https://kep.k8s.io/1287\">KEP-1287: In-place Update of Pod Resources</a></p>\n<h4>Pod certificates</h4>\n<p>When running microservices, Pods often require a strong cryptographic identity to authenticate with each other using mutual TLS (mTLS). While Kubernetes provides Service Account tokens, these are designed for authenticating to the API server, not for general-purpose workload identity.</p>\n<p>Before this enhancement, operators had to rely on complex, external projects like SPIFFE/SPIRE or cert-manager to provision and rotate certificates for their workloads. But what if you could issue a unique, short-lived certificate to your Pods natively and automatically? KEP-4317 is designed to enable such native workload identity. It opens up various possibilities for securing pod-to-pod communication by allowing the <code>kubelet</code> to request and mount certificates for a Pod via a projected volume.</p>\n<p>This provides a built-in mechanism for workload identity, complete with automated certificate rotation, significantly simplifying the setup of service meshes and other zero-trust network policies. This feature was introduced as alpha in v1.34 and is targeting beta in v1.35.</p>\n<p>You can find more in <a href=\"https://kep.k8s.io/4317\">KEP-4317: Pod Certificates</a></p>\n<h4>Numeric values for taints</h4>\n<p>Kubernetes is enhancing <a href=\"https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/\">taints and tolerations</a> by adding numeric comparison operators, such as <code>Gt</code> (Greater Than) and <code>Lt</code> (Less Than).</p>\n<p>Previously, tolerations supported only exact (<code>Equal</code>) or existence (<code>Exists</code>) matches, which were not suitable for numeric properties such as reliability SLAs.</p>\n<p>With this change, a Pod can use a toleration to \"opt-in\" to nodes that meet a specific numeric threshold. For example, a Pod can require a Node with an SLA taint value greater than 950 (<code>operator: Gt</code>, <code>value: \"950\"</code>).</p>\n<p>This approach is more powerful than Node Affinity because it supports the NoExecute effect, allowing Pods to be automatically evicted if a node's numeric value drops below the tolerated threshold.</p>\n<p>You can find more in <a href=\"https://kep.k8s.io/5471\">KEP-5471: Enable SLA-based Scheduling</a></p>\n<h4>User namespaces</h4>\n<p>When running Pods, you can use <code>securityContext</code> to drop privileges, but containers inside the pod often still run as root (UID 0). This simplicity poses a significant challenge, as that container UID 0 maps directly to the host's root user.</p>\n<p>Before this enhancement, a container breakout vulnerability could grant an attacker full root access to the node. But what if you could dynamically remap the container's root user to a safe, unprivileged user on the host? KEP-127 specifically allows such native support for Linux User Namespaces. It opens up various possibilities for pod security by isolating container and host user/group IDs. This allows a process to have root privileges (UID 0) within its namespace, while running as a non-privileged, high-numbered UID on the host.</p>\n<p>Released as alpha in v1.25 and beta in v1.30, this feature continues to progress through beta maturity, paving the way for truly \"rootless\" containers that drastically reduce the attack surface for a whole class of security vulnerabilities.</p>\n<p>You can find more in <a href=\"https://kep.k8s.io/127\">KEP-127: User Namespaces</a></p>\n<h4>Support for mounting OCI images as volumes</h4>\n<p>When provisioning a Pod, you often need to bundle data, binaries, or configuration files for your containers. Before this enhancement, people often included that kind of data directly into the main container image, or required a custom init container to download and unpack files into an <code>emptyDir</code>. You can still take either of those approaches, of course.</p>\n<p>But what if you could populate a volume directly from a data-only artifact in an OCI registry, just like pulling a container image? Kubernetes v1.31 added support for the <code>image</code> volume type, allowing Pods to pull and unpack OCI container image artifacts into a volume declaratively.</p>\n<p>This allows for seamless distribution of data, binaries, or ML models using standard registry tooling, completely decoupling data from the container image and eliminating the need for complex init containers or startup scripts. This volume type has been in beta since v1.33 and will likely be enabled by default in v1.35.</p>\n<p>You can try out the beta version of <a href=\"https://kubernetes.io/docs/concepts/storage/volumes/#image\"><code>image</code> volumes</a>, or you can learn more about the plans from <a href=\"https://kep.k8s.io/4639\">KEP-4639: OCI Volume Source</a>.</p>\n<h3>Want to know more?</h3>\n<p>New features and deprecations are also announced in the Kubernetes release notes. We will formally announce what's new in <a href=\"https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.35.md\">Kubernetes v1.35</a> as part of the CHANGELOG for that release.</p>\n<p>The Kubernetes v1.35 release is planned for <strong>December 17, 2025</strong>. Stay tuned for updates!</p>\n<p>You can also see the announcements of changes in the release notes for:</p>\n<ul>\n<li><a href=\"https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.34.md\">Kubernetes v1.34</a></li>\n<li><a href=\"https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.33.md\">Kubernetes v1.33</a></li>\n<li><a href=\"https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.32.md\">Kubernetes v1.32</a></li>\n<li><a href=\"https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.31.md\">Kubernetes v1.31</a></li>\n<li><a href=\"https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.30.md\">Kubernetes v1.30</a></li>\n</ul>\n<h3>Get involved</h3>\n<p>The simplest way to get involved with Kubernetes is by joining one of the many <a href=\"https://github.com/kubernetes/community/blob/master/sig-list.md\">Special Interest Groups</a> (SIGs) that align with your interests. Have something you’d like to broadcast to the Kubernetes community? Share your voice at our weekly <a href=\"https://github.com/kubernetes/community/tree/master/communication\">community meeting</a>, and through the channels below. Thank you for your continued feedback and support.</p>\n<ul>\n<li>Follow us on Bluesky <a href=\"https://bsky.app/profile/kubernetes.io\">@kubernetes.io</a> for the latest updates</li>\n<li>Join the community discussion on <a href=\"https://discuss.kubernetes.io/\">Discuss</a></li>\n<li>Join the community on <a href=\"http://slack.k8s.io/\">Slack</a></li>\n<li>Post questions (or answer questions) on <a href=\"https://serverfault.com/questions/tagged/kubernetes\">Server Fault</a> or <a href=\"http://stackoverflow.com/questions/tagged/kubernetes\">Stack Overflow</a></li>\n<li>Share your Kubernetes <a href=\"https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform\">story</a></li>\n<li>Read more about what’s happening with Kubernetes on the <a href=\"https://kubernetes.io/blog/\">blog</a></li>\n<li>Learn more about the <a href=\"https://github.com/kubernetes/sig-release/tree/master/release-team\">Kubernetes Release Team</a></li>\n</ul>\n","pageTitle":"Kubernetes v1.35 Sneak Peek - by Graziano Casto","pageDescription":"Kubernetes v1.35 Sneak Peek - by Graziano Casto","pageLink":"https://castograziano.com/articles/kubernetes-v135-sneak-peak","pageImage":"https://castograziano.com/casto_graziano_personal_website.png"},"schema":{"@context":"https://schema.org","@type":"BlogPosting","headline":"Kubernetes v1.35 Sneak Peek","datePublished":"2025-11-26T00:00:00.000Z","dateModified":"2025-11-26T00:00:00.000Z","author":[{"@type":"Person","name":"Graziano Casto","url":"https://castograziano.com/about-me"}]}},"__N_SSG":true}