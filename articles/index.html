<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><link rel="preload" href="/_next/static/media/graziano-casto.17cc67b9.jpg" as="image" fetchpriority="high"/><title>Blog Articles by Graziano Casto</title><meta name="description" content="Explore a collection of blog articles written by Graziano Casto."/><link rel="canonical" href="https://www.castograziano.comhttps://castograziano.com/articles"/><meta property="og:title" content="Blog Articles by Graziano Casto"/><meta property="og:description" content="Explore a collection of blog articles written by Graziano Casto."/><meta property="og:url" content="https://www.castograziano.comhttps://castograziano.com/articles"/><meta property="og:image" content="https://castograziano.com/graziano_casto_personal_website.png"/><meta property="og:type" content="article"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:title" content="Blog Articles by Graziano Casto"/><meta name="twitter:description" content="Explore a collection of blog articles written by Graziano Casto."/><meta name="twitter:image" content="https://castograziano.com/graziano_casto_personal_website.png"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="icon" href="/images/favicon.ico"/><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png"/><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png"/><meta name="next-head-count" content="19"/><link rel="preload" href="/_next/static/css/f47e3e4f61fa5b21.css" as="style"/><link rel="stylesheet" href="/_next/static/css/f47e3e4f61fa5b21.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-78c92fac7aa8fdd8.js"></script><script src="/_next/static/chunks/webpack-0b5d8249fb15f5f3.js" defer=""></script><script src="/_next/static/chunks/framework-2c16ac744b6cdea6.js" defer=""></script><script src="/_next/static/chunks/main-69b16c27ce463005.js" defer=""></script><script src="/_next/static/chunks/pages/_app-666f3c6bf21653ac.js" defer=""></script><script src="/_next/static/chunks/pages/articles-4eeecc9788e5ccb4.js" defer=""></script><script src="/_next/static/SYKRgUHRNYLLx0g7j2Qzh/_buildManifest.js" defer=""></script><script src="/_next/static/SYKRgUHRNYLLx0g7j2Qzh/_ssgManifest.js" defer=""></script></head><body><div id="__next"><script>!function(){try{var d=document.documentElement,c=d.classList;c.remove('light','dark');var e=localStorage.getItem('theme');if('system'===e||(!e&&true)){var t='(prefers-color-scheme: dark)',m=window.matchMedia(t);if(m.media!==t||m.matches){d.style.colorScheme = 'dark';c.add('dark')}else{d.style.colorScheme = 'light';c.add('light')}}else if(e){c.add(e|| '')}if(e==='light'||e==='dark')d.style.colorScheme=e}catch(e){}}()</script><div class="flex w-full"><div class="fixed inset-0 flex justify-center sm:px-8"><div class="flex w-full max-w-7xl lg:px-8"><div class="w-full bg-white ring-1 ring-zinc-100 dark:bg-zinc-900 dark:ring-zinc-300/20"></div></div></div><div class="relative flex w-full flex-col"><header class="pointer-events-none relative z-50 flex flex-none flex-col" style="height:var(--header-height);margin-bottom:var(--header-mb)"><div class="top-0 z-10 h-16 pt-6" style="position:var(--header-position)"><div class="sm:px-8 top-[var(--header-top,theme(spacing.6))] w-full" style="position:var(--header-inner-position)"><div class="mx-auto w-full max-w-7xl lg:px-8"><div class="relative px-4 sm:px-8 lg:px-12"><div class="mx-auto max-w-2xl lg:max-w-5xl"><div class="relative flex gap-4"><div class="flex flex-1"><div class="h-10 w-10 rounded-full bg-white/90 p-0.5 shadow-lg shadow-zinc-800/5 ring-1 ring-zinc-900/5 backdrop-blur dark:bg-zinc-800/90 dark:ring-white/10"><a aria-label="Home" class="pointer-events-auto" href="/"><img alt="" fetchpriority="high" width="1872" height="1914" decoding="async" data-nimg="1" class="rounded-full bg-zinc-100 object-cover dark:bg-zinc-800 h-9 w-9" style="color:transparent" src="/_next/static/media/graziano-casto.17cc67b9.jpg"/></a></div></div><div class="flex flex-1 justify-end md:justify-center"><div class="pointer-events-auto md:hidden" data-headlessui-state=""><button class="group flex items-center rounded-full bg-white/90 px-4 py-2 text-sm font-medium text-zinc-800 shadow-lg shadow-zinc-800/5 ring-1 ring-zinc-900/5 backdrop-blur dark:bg-zinc-800/90 dark:text-zinc-200 dark:ring-white/10 dark:hover:ring-white/20" type="button" aria-expanded="false" data-headlessui-state="">Menu<svg viewBox="0 0 8 6" aria-hidden="true" class="ml-3 h-auto w-2 stroke-zinc-500 group-hover:stroke-zinc-700 dark:group-hover:stroke-zinc-400"><path d="M1.75 1.75 4 4.25l2.25-2.5" fill="none" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></button></div><div hidden="" style="position:fixed;top:1px;left:1px;width:1px;height:0;padding:0;margin:-1px;overflow:hidden;clip:rect(0, 0, 0, 0);white-space:nowrap;border-width:0;display:none"></div><nav class="pointer-events-auto hidden md:block"><ul class="flex rounded-full bg-white/90 px-3 text-sm font-medium text-zinc-800 shadow-lg shadow-zinc-800/5 ring-1 ring-zinc-900/5 backdrop-blur dark:bg-zinc-800/90 dark:text-zinc-200 dark:ring-white/10"><li><a class="relative block px-3 py-2 transition hover:text-teal-500 dark:hover:text-teal-400" href="/about-me/">About</a></li><li><a class="relative block px-3 py-2 transition hover:text-teal-500 dark:hover:text-teal-400" href="/articles/">Articles</a></li><li><a class="relative block px-3 py-2 transition hover:text-teal-500 dark:hover:text-teal-400" href="/speaking/">Speaking</a></li><li><a class="relative block px-3 py-2 transition hover:text-teal-500 dark:hover:text-teal-400" href="/projects/">Projects</a></li></ul></nav></div><div class="flex justify-end md:flex-1"><div class="pointer-events-auto"><button type="button" aria-label="Toggle theme" class="group rounded-full bg-white/90 px-3 py-2 shadow-lg shadow-zinc-800/5 ring-1 ring-zinc-900/5 backdrop-blur transition dark:bg-zinc-800/90 dark:ring-white/10 dark:hover:ring-white/20"><svg viewBox="0 0 24 24" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" aria-hidden="true" class="h-6 w-6 fill-zinc-100 stroke-zinc-500 transition group-hover:fill-zinc-200 group-hover:stroke-zinc-700 dark:hidden [@media(prefers-color-scheme:dark)]:fill-teal-50 [@media(prefers-color-scheme:dark)]:stroke-teal-500 [@media(prefers-color-scheme:dark)]:group-hover:fill-teal-50 [@media(prefers-color-scheme:dark)]:group-hover:stroke-teal-600"><path d="M8 12.25A4.25 4.25 0 0 1 12.25 8v0a4.25 4.25 0 0 1 4.25 4.25v0a4.25 4.25 0 0 1-4.25 4.25v0A4.25 4.25 0 0 1 8 12.25v0Z"></path><path d="M12.25 3v1.5M21.5 12.25H20M18.791 18.791l-1.06-1.06M18.791 5.709l-1.06 1.06M12.25 20v1.5M4.5 12.25H3M6.77 6.77 5.709 5.709M6.77 17.73l-1.061 1.061" fill="none"></path></svg><svg viewBox="0 0 24 24" aria-hidden="true" class="hidden h-6 w-6 fill-zinc-700 stroke-zinc-500 transition dark:block [@media(prefers-color-scheme:dark)]:group-hover:stroke-zinc-400 [@media_not_(prefers-color-scheme:dark)]:fill-teal-400/10 [@media_not_(prefers-color-scheme:dark)]:stroke-teal-500"><path d="M17.25 16.22a6.937 6.937 0 0 1-9.47-9.47 7.451 7.451 0 1 0 9.47 9.47ZM12.75 7C17 7 17 2.75 17 2.75S17 7 21.25 7C17 7 17 11.25 17 11.25S17 7 12.75 7Z" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></button></div></div></div></div></div></div></div></div></header><main class="flex-auto"><div class="sm:px-8 mt-16 sm:mt-32"><div class="mx-auto w-full max-w-7xl lg:px-8"><div class="relative px-4 sm:px-8 lg:px-12"><div class="mx-auto max-w-2xl lg:max-w-5xl"><header class="max-w-2xl"><h1 class="text-4xl font-bold tracking-tight text-zinc-800 sm:text-5xl dark:text-zinc-100">Writing on Developer Experience, Cloud Native, AI and more.</h1><p class="mt-6 text-base text-zinc-600 dark:text-zinc-400">All of my long-form thoughts on developer experience, cloud native, AI and more.</p></header><div class="mt-16 sm:mt-20"><div class="rounded-2xl border border-zinc-100 p-6 dark:border-zinc-700/40"><h2 class="text-sm font-semibold text-zinc-900 dark:text-zinc-100">All Articles</h2><div class="mt-6 space-y-8"><a class="block" href="/articles/kcd-nyc-pe-observability-roundtable/"><article class="group relative flex flex-col items-start"><div class="group relative flex flex-col items-start"><h2 class="text-base font-semibold tracking-tight text-zinc-800 dark:text-zinc-100">From Chaos to Clarity: Navigating Observability in the Platform Engineering Era (and a Dash of AI)</h2><time class="relative z-10 order-first mb-3 flex items-center text-sm text-zinc-400 dark:text-zinc-500 pl-3.5" dateTime="2025-08-20"><span class="absolute inset-y-0 left-0 flex items-center" aria-hidden="true"><span class="h-4 w-0.5 rounded-full bg-zinc-200 dark:bg-zinc-500"></span></span>August 20, 2025</time><p class="relative z-10 mt-2 text-sm text-zinc-600 dark:text-zinc-400">A comprehensive recap of the KCD New York roundtable discussion on Platform Engineering and Observability, exploring key challenges, solutions, and the role of AI in modern observability practices.</p><div aria-hidden="true" class="relative z-10 mt-2 flex items-center text-sm font-medium text-teal-500">Read article<svg viewBox="0 0 16 16" fill="none" aria-hidden="true" class="ml-1 h-4 w-4 stroke-current"><path d="M6.75 5.75 9.25 8l-2.5 2.25" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></div></div></article></a><a class="block" href="/articles/wasm-next-universal-runtime/"><article class="group relative flex flex-col items-start"><div class="group relative flex flex-col items-start"><h2 class="text-base font-semibold tracking-tight text-zinc-800 dark:text-zinc-100">WebAssembly: From Browser Plugin to the Next Universal Runtime</h2><time class="relative z-10 order-first mb-3 flex items-center text-sm text-zinc-400 dark:text-zinc-500 pl-3.5" dateTime="2025-08-04"><span class="absolute inset-y-0 left-0 flex items-center" aria-hidden="true"><span class="h-4 w-0.5 rounded-full bg-zinc-200 dark:bg-zinc-500"></span></span>August 4, 2025</time><p class="relative z-10 mt-2 text-sm text-zinc-600 dark:text-zinc-400">Explore WebAssembly&#x27;s evolution from a browser performance booster to a universal runtime reshaping cloud, edge, and distributed computing with near-native performance across platforms.</p><div aria-hidden="true" class="relative z-10 mt-2 flex items-center text-sm font-medium text-teal-500">Read article<svg viewBox="0 0 16 16" fill="none" aria-hidden="true" class="ml-1 h-4 w-4 stroke-current"><path d="M6.75 5.75 9.25 8l-2.5 2.25" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></div></div></article></a><a class="block" href="/articles/kubernetes-v134-sneak-peak/"><article class="group relative flex flex-col items-start"><div class="group relative flex flex-col items-start"><h2 class="text-base font-semibold tracking-tight text-zinc-800 dark:text-zinc-100">Kubernetes v1.34 Sneak Peek</h2><time class="relative z-10 order-first mb-3 flex items-center text-sm text-zinc-400 dark:text-zinc-500 pl-3.5" dateTime="2025-07-28"><span class="absolute inset-y-0 left-0 flex items-center" aria-hidden="true"><span class="h-4 w-0.5 rounded-full bg-zinc-200 dark:bg-zinc-500"></span></span>July 28, 2025</time><p class="relative z-10 mt-2 text-sm text-zinc-600 dark:text-zinc-400">Discover the exciting enhancements coming in Kubernetes v1.34, including stable Dynamic Resource Allocation, improved scheduling capabilities, and enhanced security features for production workloads.</p><div aria-hidden="true" class="relative z-10 mt-2 flex items-center text-sm font-medium text-teal-500">Read article<svg viewBox="0 0 16 16" fill="none" aria-hidden="true" class="ml-1 h-4 w-4 stroke-current"><path d="M6.75 5.75 9.25 8l-2.5 2.25" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></div></div></article></a><a class="block" href="/articles/ai-native-platforms/"><article class="group relative flex flex-col items-start"><div class="group relative flex flex-col items-start"><h2 class="text-base font-semibold tracking-tight text-zinc-800 dark:text-zinc-100">AI-Native Platforms: The Unstoppable Alliance of GenAI and Platform Engineering</h2><time class="relative z-10 order-first mb-3 flex items-center text-sm text-zinc-400 dark:text-zinc-500 pl-3.5" dateTime="2025-06-11"><span class="absolute inset-y-0 left-0 flex items-center" aria-hidden="true"><span class="h-4 w-0.5 rounded-full bg-zinc-200 dark:bg-zinc-500"></span></span>June 11, 2025</time><p class="relative z-10 mt-2 text-sm text-zinc-600 dark:text-zinc-400">Discover how Generative AI is transforming platform engineering from static toolsets into intelligent, dynamic, and self-optimizing ecosystems that unlock 100% of platform potential.</p><div aria-hidden="true" class="relative z-10 mt-2 flex items-center text-sm font-medium text-teal-500">Read article<svg viewBox="0 0 16 16" fill="none" aria-hidden="true" class="ml-1 h-4 w-4 stroke-current"><path d="M6.75 5.75 9.25 8l-2.5 2.25" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></div></div></article></a><a class="block" href="/articles/cognitive-architecture-how-llms-are-reshaping-software-architecture/"><article class="group relative flex flex-col items-start"><div class="group relative flex flex-col items-start"><h2 class="text-base font-semibold tracking-tight text-zinc-800 dark:text-zinc-100">Cognitive Architecture: How LLMs Are Reshaping Software Architecture</h2><time class="relative z-10 order-first mb-3 flex items-center text-sm text-zinc-400 dark:text-zinc-500 pl-3.5" dateTime="2025-06-11"><span class="absolute inset-y-0 left-0 flex items-center" aria-hidden="true"><span class="h-4 w-0.5 rounded-full bg-zinc-200 dark:bg-zinc-500"></span></span>June 11, 2025</time><p class="relative z-10 mt-2 text-sm text-zinc-600 dark:text-zinc-400">Discover how Large Language Models are transforming software design from traditional service-oriented architectures to cognitive, agent-driven systems that reason and act autonomously.</p><div aria-hidden="true" class="relative z-10 mt-2 flex items-center text-sm font-medium text-teal-500">Read article<svg viewBox="0 0 16 16" fill="none" aria-hidden="true" class="ml-1 h-4 w-4 stroke-current"><path d="M6.75 5.75 9.25 8l-2.5 2.25" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></div></div></article></a><a class="block" href="/articles/java-next-act-native-speed-for-cloud-era/"><article class="group relative flex flex-col items-start"><div class="group relative flex flex-col items-start"><h2 class="text-base font-semibold tracking-tight text-zinc-800 dark:text-zinc-100">Java&#x27;s Next Act: Native Speed for a Cloud-Native World</h2><time class="relative z-10 order-first mb-3 flex items-center text-sm text-zinc-400 dark:text-zinc-500 pl-3.5" dateTime="2025-04-30"><span class="absolute inset-y-0 left-0 flex items-center" aria-hidden="true"><span class="h-4 w-0.5 rounded-full bg-zinc-200 dark:bg-zinc-500"></span></span>April 30, 2025</time><p class="relative z-10 mt-2 text-sm text-zinc-600 dark:text-zinc-400">Explore how the Graal Stack is reinventing Java for the cloud era, combining GraalVM, Micronaut, and GraalOS to deliver ultra-fast, lightweight, and serverless-ready applications that address traditional Java limitations.</p><div aria-hidden="true" class="relative z-10 mt-2 flex items-center text-sm font-medium text-teal-500">Read article<svg viewBox="0 0 16 16" fill="none" aria-hidden="true" class="ml-1 h-4 w-4 stroke-current"><path d="M6.75 5.75 9.25 8l-2.5 2.25" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></div></div></article></a><a class="block" href="/articles/breaking-the-context-barrier-of-llms-infiniretri-vs-rag/"><article class="group relative flex flex-col items-start"><div class="group relative flex flex-col items-start"><h2 class="text-base font-semibold tracking-tight text-zinc-800 dark:text-zinc-100">Breaking the Context Barrier of LLMs: InfiniRetri vs RAG</h2><time class="relative z-10 order-first mb-3 flex items-center text-sm text-zinc-400 dark:text-zinc-500 pl-3.5" dateTime="2025-03-27"><span class="absolute inset-y-0 left-0 flex items-center" aria-hidden="true"><span class="h-4 w-0.5 rounded-full bg-zinc-200 dark:bg-zinc-500"></span></span>March 27, 2025</time><p class="relative z-10 mt-2 text-sm text-zinc-600 dark:text-zinc-400">Compare InfiniRetri and RAG approaches for LLM information retrieval, exploring their unique strengths, limitations, and potential hybrid solutions for overcoming context limitations.</p><div aria-hidden="true" class="relative z-10 mt-2 flex items-center text-sm font-medium text-teal-500">Read article<svg viewBox="0 0 16 16" fill="none" aria-hidden="true" class="ml-1 h-4 w-4 stroke-current"><path d="M6.75 5.75 9.25 8l-2.5 2.25" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></div></div></article></a><a class="block" href="/articles/are-you-sure-you-really-want-to-be-a-devrel/"><article class="group relative flex flex-col items-start"><div class="group relative flex flex-col items-start"><h2 class="text-base font-semibold tracking-tight text-zinc-800 dark:text-zinc-100">Are You Sure You Really Want to Become a DevRel?</h2><time class="relative z-10 order-first mb-3 flex items-center text-sm text-zinc-400 dark:text-zinc-500 pl-3.5" dateTime="2025-02-28"><span class="absolute inset-y-0 left-0 flex items-center" aria-hidden="true"><span class="h-4 w-0.5 rounded-full bg-zinc-200 dark:bg-zinc-500"></span></span>February 28, 2025</time><p class="relative z-10 mt-2 text-sm text-zinc-600 dark:text-zinc-400">A personal reflection on two years as a Developer Relations Engineer, sharing honest insights about the role&#x27;s challenges, rewards, and how context shapes the DevRel experience in different organizations.</p><div aria-hidden="true" class="relative z-10 mt-2 flex items-center text-sm font-medium text-teal-500">Read article<svg viewBox="0 0 16 16" fill="none" aria-hidden="true" class="ml-1 h-4 w-4 stroke-current"><path d="M6.75 5.75 9.25 8l-2.5 2.25" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></div></div></article></a><a class="block" href="/articles/ai-agents-future-of-automation-or-overhyped-buzzword/"><article class="group relative flex flex-col items-start"><div class="group relative flex flex-col items-start"><h2 class="text-base font-semibold tracking-tight text-zinc-800 dark:text-zinc-100">AI Agents: Future of Automation or Overhyped Buzzword?</h2><time class="relative z-10 order-first mb-3 flex items-center text-sm text-zinc-400 dark:text-zinc-500 pl-3.5" dateTime="2025-02-20"><span class="absolute inset-y-0 left-0 flex items-center" aria-hidden="true"><span class="h-4 w-0.5 rounded-full bg-zinc-200 dark:bg-zinc-500"></span></span>February 20, 2025</time><p class="relative z-10 mt-2 text-sm text-zinc-600 dark:text-zinc-400">Explore the reality behind AI agents - autonomous systems that perceive, decide, and act independently. Discover their potential to revolutionize automation while understanding the challenges and limitations.</p><div aria-hidden="true" class="relative z-10 mt-2 flex items-center text-sm font-medium text-teal-500">Read article<svg viewBox="0 0 16 16" fill="none" aria-hidden="true" class="ml-1 h-4 w-4 stroke-current"><path d="M6.75 5.75 9.25 8l-2.5 2.25" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></div></div></article></a><a class="block" href="/articles/energy-efficiency-of-jvm-and-the-role-of-graalvm/"><article class="group relative flex flex-col items-start"><div class="group relative flex flex-col items-start"><h2 class="text-base font-semibold tracking-tight text-zinc-800 dark:text-zinc-100">The Energy Efficiency of JVMs and the Role of GraalVM</h2><time class="relative z-10 order-first mb-3 flex items-center text-sm text-zinc-400 dark:text-zinc-500 pl-3.5" dateTime="2025-01-29"><span class="absolute inset-y-0 left-0 flex items-center" aria-hidden="true"><span class="h-4 w-0.5 rounded-full bg-zinc-200 dark:bg-zinc-500"></span></span>January 29, 2025</time><p class="relative z-10 mt-2 text-sm text-zinc-600 dark:text-zinc-400">Explore the correlation between energy efficiency and code performance in the JVM ecosystem, discovering why GraalVM stands out as a top-tier runtime for optimizing both performance and environmental impact.</p><div aria-hidden="true" class="relative z-10 mt-2 flex items-center text-sm font-medium text-teal-500">Read article<svg viewBox="0 0 16 16" fill="none" aria-hidden="true" class="ml-1 h-4 w-4 stroke-current"><path d="M6.75 5.75 9.25 8l-2.5 2.25" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></div></div></article></a><a class="block" href="/articles/shape-your-platform-strategy-with-the-journey-map/"><article class="group relative flex flex-col items-start"><div class="group relative flex flex-col items-start"><h2 class="text-base font-semibold tracking-tight text-zinc-800 dark:text-zinc-100">Platform Journey Map: Shape Your Platform Strategy</h2><time class="relative z-10 order-first mb-3 flex items-center text-sm text-zinc-400 dark:text-zinc-500 pl-3.5" dateTime="2025-01-09"><span class="absolute inset-y-0 left-0 flex items-center" aria-hidden="true"><span class="h-4 w-0.5 rounded-full bg-zinc-200 dark:bg-zinc-500"></span></span>January 9, 2025</time><p class="relative z-10 mt-2 text-sm text-zinc-600 dark:text-zinc-400">Discover the Platform Journey Map, a visual game designed to spark meaningful discussions about platform strategy and priorities, moving beyond technology debates to focus on people and clear strategic direction.</p><div aria-hidden="true" class="relative z-10 mt-2 flex items-center text-sm font-medium text-teal-500">Read article<svg viewBox="0 0 16 16" fill="none" aria-hidden="true" class="ml-1 h-4 w-4 stroke-current"><path d="M6.75 5.75 9.25 8l-2.5 2.25" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></div></div></article></a><a class="block" href="/articles/finding-observability-and-devex-traquility-with-platform-engineering/"><article class="group relative flex flex-col items-start"><div class="group relative flex flex-col items-start"><h2 class="text-base font-semibold tracking-tight text-zinc-800 dark:text-zinc-100">Finding Observability and DevEx Tranquility with Platform Engineering</h2><time class="relative z-10 order-first mb-3 flex items-center text-sm text-zinc-400 dark:text-zinc-500 pl-3.5" dateTime="2025-01-07"><span class="absolute inset-y-0 left-0 flex items-center" aria-hidden="true"><span class="h-4 w-0.5 rounded-full bg-zinc-200 dark:bg-zinc-500"></span></span>January 7, 2025</time><p class="relative z-10 mt-2 text-sm text-zinc-600 dark:text-zinc-400">Learn how to navigate the stormy seas of monitoring data by integrating observability as a first-class citizen in platform engineering, avoiding complexity while providing meaningful insights.</p><div aria-hidden="true" class="relative z-10 mt-2 flex items-center text-sm font-medium text-teal-500">Read article<svg viewBox="0 0 16 16" fill="none" aria-hidden="true" class="ml-1 h-4 w-4 stroke-current"><path d="M6.75 5.75 9.25 8l-2.5 2.25" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></div></div></article></a><a class="block" href="/articles/2024-wrapped-my-year-as-devrel-buddy/"><article class="group relative flex flex-col items-start"><div class="group relative flex flex-col items-start"><h2 class="text-base font-semibold tracking-tight text-zinc-800 dark:text-zinc-100">2024 Wrapped: My Year as a DevRel Buddy</h2><time class="relative z-10 order-first mb-3 flex items-center text-sm text-zinc-400 dark:text-zinc-500 pl-3.5" dateTime="2024-12-30"><span class="absolute inset-y-0 left-0 flex items-center" aria-hidden="true"><span class="h-4 w-0.5 rounded-full bg-zinc-200 dark:bg-zinc-500"></span></span>December 30, 2024</time><p class="relative z-10 mt-2 text-sm text-zinc-600 dark:text-zinc-400">A comprehensive review of my first year as a Developer Relations Engineer, highlighting community engagement, speaking opportunities, and the journey of sharing knowledge across international tech events.</p><div aria-hidden="true" class="relative z-10 mt-2 flex items-center text-sm font-medium text-teal-500">Read article<svg viewBox="0 0 16 16" fill="none" aria-hidden="true" class="ml-1 h-4 w-4 stroke-current"><path d="M6.75 5.75 9.25 8l-2.5 2.25" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></div></div></article></a><a class="block" href="/articles/a-workshop-to-help-map-your-strategy/"><article class="group relative flex flex-col items-start"><div class="group relative flex flex-col items-start"><h2 class="text-base font-semibold tracking-tight text-zinc-800 dark:text-zinc-100">Platform Engineering: A Workshop to Help Map Your Strategy</h2><time class="relative z-10 order-first mb-3 flex items-center text-sm text-zinc-400 dark:text-zinc-500 pl-3.5" dateTime="2024-11-21"><span class="absolute inset-y-0 left-0 flex items-center" aria-hidden="true"><span class="h-4 w-0.5 rounded-full bg-zinc-200 dark:bg-zinc-500"></span></span>November 21, 2024</time><p class="relative z-10 mt-2 text-sm text-zinc-600 dark:text-zinc-400">Discover how Mia-Platform&#x27;s game-like workshop helps organizations define their internal developer platform strategy by bringing together stakeholders to answer the crucial &#x27;why&#x27; behind platform engineering initiatives.</p><div aria-hidden="true" class="relative z-10 mt-2 flex items-center text-sm font-medium text-teal-500">Read article<svg viewBox="0 0 16 16" fill="none" aria-hidden="true" class="ml-1 h-4 w-4 stroke-current"><path d="M6.75 5.75 9.25 8l-2.5 2.25" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></div></div></article></a><a class="block" href="/articles/greenops-and-finops/"><article class="group relative flex flex-col items-start"><div class="group relative flex flex-col items-start"><h2 class="text-base font-semibold tracking-tight text-zinc-800 dark:text-zinc-100">GreenOps and FinOps: The Perfect Pitch Toward Business and Environmental Goals</h2><time class="relative z-10 order-first mb-3 flex items-center text-sm text-zinc-400 dark:text-zinc-500 pl-3.5" dateTime="2024-07-25"><span class="absolute inset-y-0 left-0 flex items-center" aria-hidden="true"><span class="h-4 w-0.5 rounded-full bg-zinc-200 dark:bg-zinc-500"></span></span>July 25, 2024</time><p class="relative z-10 mt-2 text-sm text-zinc-600 dark:text-zinc-400">Learn how integrating GreenOps and FinOps methodologies helps organizations achieve both business objectives and environmental sustainability goals while optimizing IT costs and reducing carbon footprint.</p><div aria-hidden="true" class="relative z-10 mt-2 flex items-center text-sm font-medium text-teal-500">Read article<svg viewBox="0 0 16 16" fill="none" aria-hidden="true" class="ml-1 h-4 w-4 stroke-current"><path d="M6.75 5.75 9.25 8l-2.5 2.25" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></div></div></article></a><a class="block" href="/articles/greenops-kube-green/"><article class="group relative flex flex-col items-start"><div class="group relative flex flex-col items-start"><h2 class="text-base font-semibold tracking-tight text-zinc-800 dark:text-zinc-100">Using GreenOps to Improve Your Operational Efficiency and Save the Planet</h2><time class="relative z-10 order-first mb-3 flex items-center text-sm text-zinc-400 dark:text-zinc-500 pl-3.5" dateTime="2024-06-26"><span class="absolute inset-y-0 left-0 flex items-center" aria-hidden="true"><span class="h-4 w-0.5 rounded-full bg-zinc-200 dark:bg-zinc-500"></span></span>June 26, 2024</time><p class="relative z-10 mt-2 text-sm text-zinc-600 dark:text-zinc-400">Discover how GreenOps practices can help reduce your infrastructure&#x27;s environmental impact while improving operational efficiency, featuring practical solutions like kube-green for sustainable Kubernetes operations.</p><div aria-hidden="true" class="relative z-10 mt-2 flex items-center text-sm font-medium text-teal-500">Read article<svg viewBox="0 0 16 16" fill="none" aria-hidden="true" class="ml-1 h-4 w-4 stroke-current"><path d="M6.75 5.75 9.25 8l-2.5 2.25" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></div></div></article></a><a class="block" href="/articles/what-cloud-native-mean/"><article class="group relative flex flex-col items-start"><div class="group relative flex flex-col items-start"><h2 class="text-base font-semibold tracking-tight text-zinc-800 dark:text-zinc-100">What the Fu** Does Cloud Native Mean?</h2><time class="relative z-10 order-first mb-3 flex items-center text-sm text-zinc-400 dark:text-zinc-500 pl-3.5" dateTime="2024-04-30"><span class="absolute inset-y-0 left-0 flex items-center" aria-hidden="true"><span class="h-4 w-0.5 rounded-full bg-zinc-200 dark:bg-zinc-500"></span></span>April 30, 2024</time><p class="relative z-10 mt-2 text-sm text-zinc-600 dark:text-zinc-400">A critical examination of what &#x27;Cloud Native&#x27; truly means, challenging the misconception that simply running applications in the cloud automatically qualifies them as modern, scalable, or resilient.</p><div aria-hidden="true" class="relative z-10 mt-2 flex items-center text-sm font-medium text-teal-500">Read article<svg viewBox="0 0 16 16" fill="none" aria-hidden="true" class="ml-1 h-4 w-4 stroke-current"><path d="M6.75 5.75 9.25 8l-2.5 2.25" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></div></div></article></a><a class="block" href="/articles/green-cloud-native-apps/"><article class="group relative flex flex-col items-start"><div class="group relative flex flex-col items-start"><h2 class="text-base font-semibold tracking-tight text-zinc-800 dark:text-zinc-100">Green Cloud Native Apps: Optimize Kubernetes for Renewable Energy</h2><time class="relative z-10 order-first mb-3 flex items-center text-sm text-zinc-400 dark:text-zinc-500 pl-3.5" dateTime="2024-03-20"><span class="absolute inset-y-0 left-0 flex items-center" aria-hidden="true"><span class="h-4 w-0.5 rounded-full bg-zinc-200 dark:bg-zinc-500"></span></span>March 20, 2024</time><p class="relative z-10 mt-2 text-sm text-zinc-600 dark:text-zinc-400">Learn how to enhance the environmental sustainability of cloud-native applications using open-source tools and Kubernetes optimizations to reduce carbon footprint and align with renewable energy sources.</p><div aria-hidden="true" class="relative z-10 mt-2 flex items-center text-sm font-medium text-teal-500">Read article<svg viewBox="0 0 16 16" fill="none" aria-hidden="true" class="ml-1 h-4 w-4 stroke-current"><path d="M6.75 5.75 9.25 8l-2.5 2.25" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></div></div></article></a><a class="block" href="/articles/being-vs-doing-agile/"><article class="group relative flex flex-col items-start"><div class="group relative flex flex-col items-start"><h2 class="text-base font-semibold tracking-tight text-zinc-800 dark:text-zinc-100">Are You Truly Agile or Just a Monkey Scrum Practitioner?</h2><time class="relative z-10 order-first mb-3 flex items-center text-sm text-zinc-400 dark:text-zinc-500 pl-3.5" dateTime="2024-03-15"><span class="absolute inset-y-0 left-0 flex items-center" aria-hidden="true"><span class="h-4 w-0.5 rounded-full bg-zinc-200 dark:bg-zinc-500"></span></span>March 15, 2024</time><p class="relative z-10 mt-2 text-sm text-zinc-600 dark:text-zinc-400">Explore the fundamental differences between Agile and Scrum, understanding how Agile represents a mindset and way of life while Scrum provides specific project management practices.</p><div aria-hidden="true" class="relative z-10 mt-2 flex items-center text-sm font-medium text-teal-500">Read article<svg viewBox="0 0 16 16" fill="none" aria-hidden="true" class="ml-1 h-4 w-4 stroke-current"><path d="M6.75 5.75 9.25 8l-2.5 2.25" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></div></div></article></a><a class="block" href="/articles/my-digital-corner/"><article class="group relative flex flex-col items-start"><div class="group relative flex flex-col items-start"><h2 class="text-base font-semibold tracking-tight text-zinc-800 dark:text-zinc-100">Welcome to My Digital Corner!</h2><time class="relative z-10 order-first mb-3 flex items-center text-sm text-zinc-400 dark:text-zinc-500 pl-3.5" dateTime="2024-02-01"><span class="absolute inset-y-0 left-0 flex items-center" aria-hidden="true"><span class="h-4 w-0.5 rounded-full bg-zinc-200 dark:bg-zinc-500"></span></span>February 1, 2024</time><p class="relative z-10 mt-2 text-sm text-zinc-600 dark:text-zinc-400">A personal introduction to my digital space where I share insights about cloud-native technologies, my professional journey, and the importance of communication and storytelling in tech.</p><div aria-hidden="true" class="relative z-10 mt-2 flex items-center text-sm font-medium text-teal-500">Read article<svg viewBox="0 0 16 16" fill="none" aria-hidden="true" class="ml-1 h-4 w-4 stroke-current"><path d="M6.75 5.75 9.25 8l-2.5 2.25" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></div></div></article></a></div></div></div></div></div></div></div></main><footer class="mt-32 flex-none"><div class="sm:px-8"><div class="mx-auto w-full max-w-7xl lg:px-8"><div class="border-t border-zinc-100 pb-16 pt-10 dark:border-zinc-700/40"><div class="relative px-4 sm:px-8 lg:px-12"><div class="mx-auto max-w-2xl lg:max-w-5xl"><div class="flex flex-col items-center justify-between gap-6 sm:flex-row"><div class="flex flex-wrap justify-center gap-x-6 gap-y-1 text-sm font-medium text-zinc-800 dark:text-zinc-200"><a class="transition hover:text-teal-500 dark:hover:text-teal-400" href="/about-me/">About</a><a class="transition hover:text-teal-500 dark:hover:text-teal-400" href="/projects/">Projects</a><a class="transition hover:text-teal-500 dark:hover:text-teal-400" href="/speaking/">Speaking</a></div><p class="text-sm text-zinc-500 dark:text-zinc-400">© <!-- -->2025<!-- --> Graziano Casto. All rights reserved.</p></div></div></div></div></div></div></footer></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"data":{"pageTitle":"Blog Articles by Graziano Casto","pageDescription":"Explore a collection of blog articles written by Graziano Casto.","pageLink":"https://castograziano.com/articles","pageImage":"https://castograziano.com/graziano_casto_personal_website.png","articles":[{"slug":"kcd-nyc-pe-observability-roundtable","content":"\n[Published by Cloud Native PE Community](https://cloudnativeplatforms.com/blog/kcd-nyc-platform-engineering-and-observability-roundtable/)\n\n\u003e There was a great energy at KCD New York this year, and for Graziano Casto, a personal highlight was leading a roundtable on observability. It was a fascinating discussion that really got him thinking about the challenges we’re all facing in the platform engineering space. Here is Graziano’s recap of the key problems and promising ideas that came up.\n\n### Introduction\n\nIt was an absolute pleasure recently to moderate a roundtable at [KCD New York](https://community.cncf.io/events/details/cncf-kcd-new-york-presents-kcd-new-york-2025/), diving deep into the fascinating (and let’s be honest, sometimes frustrating) world where **Platform Engineering meets Observability**. As my first time moderating a roundtable, I was genuinely thrilled by the energy and candid participation from everyone in the room. A huge thank you to all the participants: [Michel Murabito](https://www.linkedin.com/in/mich-murabito/), [Colin Lacy](https://www.linkedin.com/in/colinjlacy/), [Tiara Sykes](https://www.linkedin.com/in/tiara-sykes/), [Andrew Espira](https://www.linkedin.com/in/andrew-espira/), [Mariia Rudenko](https://www.linkedin.com/in/mariia-r-748931163/), [Aderianna Williams](https://www.linkedin.com/in/at-williams/), [Marino Wijay](https://www.linkedin.com/in/mwijay/) and [Maria Ashby](https://www.linkedin.com/in/maria-ashby/) whose insights made this discussion truly invaluable. We had an incredibly insightful exchange, and I walked away with some serious food for thought.\n\n![](../../assets/newsletter/12-kcd-new-york/photo-4.jpeg)\n\nWe kicked off by acknowledging a universal truth in today’s cloud-native landscape: managing a full observability stack often feels like trying to hit a moving target. The more we aim to observe, the more inherent complexity seems to creep in. We continuously pile on tools, data, and dashboards–be it metrics, traces, logs, or profiling – and suddenly, we’re swimming in a sea of cognitive load, entropy, and quite often, plain old confusion. So, instead of me listing the common headaches, I threw it open to the room: “When you think about managing a full observability stack, across logs, metrics, traces, and so on, what are your biggest pain points? If you had to name the biggest challenge your team is facing with observability right now, what would it be?”\n\nThe responses flowed freely, revealing a shared understanding that while observability promises clarity, its real-world implementation often introduces its own unique set of challenges. And, quite organically, our conversation drifted into the exciting (and slightly unsettling) realm of Generative AI, specifically discussing how Large Language Models (LLMs) can be synergistically integrated within platforms to serve as enablers in resolving some of these very challenges.\n\n### The Observability Headaches: Where Do We Feel the Pinch?\n\nOne of the loudest points of contention was the persistent struggle to correlate telemetry data with the actual services generating them, and the broader challenge of **telemetry data correlation**. It’s like having all the pieces of a complex puzzle but no clear idea how they fit together. You might spot a spike in CPU utilization – a metric – but then you’re left guessing which microservice is the culprit. Then begins the detective work: diving into logs to pinpoint an error, and finally tracing requests to understand the flow. The fundamental problem is that these critical data points often reside in disparate systems, use inconsistent identifiers, and demand a significant amount of manual effort and intuition to connect the dots. This fragmentation makes it incredibly difficult to quickly identify the root cause of a problem when seconds count.\n\nAdding to this complexity is the sheer volume of alerts and the difficulty in correlating them with the actual underlying problem. We’ve all experienced it: a dozen alerts fire simultaneously, each pointing to a symptom, yet none clearly indicating the core issue. This leads to what’s known as **alert fatigue**, resulting in missed critical incidents, wasted time triaging false positives, and ultimately, a palpable loss of trust in the alerting system itself. The challenge isn’t merely about receiving notifications; it’s about receiving meaningful alerts that directly pinpoint the underlying problem, not just its outward manifestations.\n\nFurthermore, a significant unspoken burden that often comes with observability is the **cost** of both creating and maintaining the entire observability stack. From licensing fees for proprietary tools to the infrastructure costs of storing massive volumes of telemetry data and the operational overhead of managing these complex systems, the financial outlay can be substantial. This constant investment of resources, both human and monetary, can become a major pain point, often weighing heavily on budget decisions and resource allocation.\n\nThen there’s the pervasive issue of **making insights accessible** and visualizing them in a way that provides the right insight to the right person. Raw telemetry data, in its unadulterated form, is overwhelming. Different roles within an organization – SREs, developers, product managers – need distinct views and varying levels of detail. A developer might require granular trace data, while a product manager needs high-level business metrics. The constant battle involves creating and maintaining these tailored dashboards and ensuring everyone knows where to find what they need. This often leads to information silos and missed opportunities for proactive improvement.\n\nA recurring theme throughout our discussion was the persistent problem of **siloed teams** and the resulting **lack of standardization**. When different teams adopt disparate observability tools, inconsistent naming conventions, or even varied logging formats, it inevitably creates a fragmented and chaotic landscape. This makes it incredibly challenging to gain a holistic view of the system, collaborate effectively during incidents, and leverage best practices across the entire organization. It’s a classic case of “everyone doing their own thing”, leading to pervasive inefficiencies and increased complexity.\n\nFinally, a crucial point that resonated deeply was the importance of **developer education**. Observability isn’t merely about deploying tools; it’s about cultivating a specific mindset. Developers need to grasp why observability is vital, how to effectively instrument their code, how to interpret telemetry data, and critically, how to leverage observability tools to troubleshoot their applications. This knowledge gap can lead to poorly instrumented services, ignored alerts, and a general underutilization of the powerful observability stacks organizations invest heavily in.\n\n### Internal Developer Platforms: The Unified Solution\n\nSo, with these common headaches laid out, how do we begin to alleviate them? This is precisely where the concept of an **Internal Developer Platform (IDP)** steps in as a truly powerful solution, providing a cohesive answer to many of these challenges.\n\nAn IDP, at its core, inherently solves the problem of standardization. By providing clear standards and abstractions through “golden paths” for building and deploying applications, it ensures consistency across the organization. However, it’s crucial that these **golden paths** don’t become “golden cages”. A well-designed IDP empowers developers with the necessary autonomy to cover edge cases, allowing them to step outside the perimeter of the provided golden paths when needed for specific requirements. This balance is vital for both consistency and innovation.\n\nMoving beyond standardization, IDPs also play a crucial role in addressing the challenges faced by siloed teams that might be working on different components of the same system and often lack a shared performance baseline. During our discussion, we introduced the concept of leveraging generative models within these platforms. Specifically, the role of Generative AI, particularly **Large Language Models (LLMs)**, in the observability space emerged as a truly futuristic and exciting prospect. The idea is that LLMs can help close the gap between users and telemetry data. Imagine being able to ask natural language questions like, “Why is our checkout service slow right now?” and have an LLM sift through mountains of metrics, logs, and traces to provide a concise, actionable answer. Or, “What were the top 3 errors in our authentication service last night?” and get a summary, perhaps even with links to relevant log lines. These models can also be instrumental in enabling teams to define and compare their telemetry data against customized thresholds, ensuring that the entire system is monitored according to a collectively defined baseline, fostering a shared understanding of system health.\n\nFrom here, we delved into how these models further enhance the transparency and clarity of insights. LLMs, integrated within the IDP, can analyze vast amounts of telemetry data and provide various stakeholders with personalized insights and alerts. This capability opens the door to entirely new interfaces beyond the traditional dashboards, making complex operational data more accessible and actionable for different roles. Unfortunately, we didn’t have the opportunity to delve deeper into the intricate topic of telemetry data correlation during the roundtable, but I have written an article that explores this topic further, which you can find [here](https://www.linkedin.com/pulse/9-serving-observability-first-dish-graziano-casto-05rhf).\n\n### The Open Question: Balancing Trust and Cost with Benefits in the LLM Era\n\nHowever, as with any powerful new technology, the discussion around LLMs quickly led to a critical open question for the community:\n\n**How do we effectively balance the significant benefits that LLMs bring – such as improved automation and deeper insights – against the inherent costs? These costs include not only the economic investment required for these models but also the crucial aspect of trust, both in the accuracy of the results and in entrusting our sensitive data to an LLM, particularly when utilized as a service.**\n\nThis is a conversation that needs to continue. As we push the boundaries of what’s possible with AI in operations, we must collectively figure out how to build systems that are not only efficient and intelligent but also fundamentally secure, trustworthy, and cost-effective.\n\n### Wrapping Up\n\nMy first moderating experience at KCD New York was an absolute blast, and the insights from the roundtable on Platform Engineering and Observability were truly invaluable. It’s clear that while observability brings its own set of complexities, Internal Developer Platforms offer a robust framework for overcoming these challenges by promoting standardization, providing contextualized insights, and empowering developers. And looking ahead, the potential of LLMs to revolutionize how we interact with our telemetry data is incredibly exciting, even if it comes with some important questions we need to answer as a community.\n\nWhat are your thoughts on these challenges and solutions? And how do you see the role of LLMs evolving in the observability space, especially concerning the trust and cost trade-offs? Let’s keep the conversation going!\n","metadata":{"title":"From Chaos to Clarity: Navigating Observability in the Platform Engineering Era (and a Dash of AI)","excerpt":"A comprehensive recap of the KCD New York roundtable discussion on Platform Engineering and Observability, exploring key challenges, solutions, and the role of AI in modern observability practices.","date":"2025-08-20","author":"Graziano Casto"}},{"slug":"wasm-next-universal-runtime","content":"\n[Published by DZONE co-authored with Alex Casalboni (Developer Advocate @ Edgee)](https://dzone.com/articles/webassembly-from-browser-plugin-to-the-next-univer)\n\n\nFor decades, the digital world has converged on a single, universal computing platform: the web browser. This remarkable piece of software, present on nearly every device, promised a \"write once, run anywhere\" paradigm, but with a crucial limitation, it only spoke one language natively: JavaScript. While incredibly versatile, JavaScript's nature as a dynamically typed, interpreted language created a performance ceiling. For computationally intensive tasks, developers often hit a wall, unable to achieve the raw speed of native applications. This limitation also meant that the vast, mature ecosystems of code written in languages like C++, C, and Rust were largely inaccessible on the web without cumbersome and often inefficient cross-compilation to JavaScript.\n\nInto this landscape emerged [**WebAssembly**](https://dzone.com/articles/what-is-webassembly) **(Wasm)**. Often referred to as a fourth standard language for the web alongside HTML, CSS, and JavaScript, Wasm was not designed to replace JavaScript but to be its powerful companion. It is a binary instruction format, a low-level, assembly-like language that **serves as a portable compilation target**. This simple yet profound idea meant that developers could take existing code written in high-performance languages, compile it into a compact Wasm binary, and run it directly within the browser at near-native speeds. This breakthrough unlocked a new class of applications that were previously impractical for the web, from sophisticated in-browser tools to full-fledged 3D gaming engines.\n\nThe design of WebAssembly was forged in the demanding and often hostile environment of the public internet, leading to a set of foundational principles that would define its destiny. It had to be **fast**, with a compact binary format that could be decoded and executed far more efficiently than parsing text-based JavaScript. It had to be **secure**, running inside a tightly controlled, memory-safe sandbox that isolated it from the host system and other browser tabs. And it had to be **portable**, a universal format independent of any specific operating system or hardware architecture.\n\nThese very principles, essential for its success in the browser, were also the seeds of a much grander vision. This article charts the remarkable journey of WebAssembly, following its evolution from a browser-based performance booster into a foundational technology that is reshaping our approach to cloud, edge, and distributed computing, promising a future built on a truly universal runtime.\n\n### Beyond the Browser With the WebAssembly System Interface (WASI)\n\nWebAssembly's potential was too significant to remain confined within the browser. Developers and architects quickly recognized that a portable, fast, and secure runtime could be immensely valuable for server-side applications. However, a critical piece of the puzzle was missing.\n\nWasm modules running in the browser can interact with its environment through a rich set of Web APIs, allowing it to fetch data, manipulate the screen, or play audio. Server-side applications have a **completely different set of needs**: they must read and write files, access environment variables, open network sockets, and interact with the system clock. Without a standardized way to perform these basic operations, server-side Wasm would be a collection of incompatible, proprietary solutions, shattering its promise of portability.\n\nThe solution is the **WebAssembly System Interface (WASI)**, an evolving set of APIs. It's crucial to understand that WASI is not a single, monolithic standard but is currently in a significant transition, from the stable but limited **WASI Preview 1** (which lacks standardized networking) to the fundamentally redesigned **WASI Preview 2**. This newer version is built upon the still-in-proposal Component Model and introduces modular APIs for features like HTTP and sockets.  Looking ahead, the next iteration, [WASI Preview 3](https://wasi.dev/roadmap#upcoming-wasi-03-releases), is anticipated for release in **August 2025**, promising further advancements such as native async and streaming support.\n\nThis layer of abstraction is the key to preserving Wasm's \"write once, run anywhere\" superpower. The WASI standard allows developers to write code in their preferred programming language (including [Rust](https://dzone.com/articles/rust-and-webassembly-for-web-apps), C/C++, C#, Go, JavaScript, TypeScript, and Python), compile it into a single Wasm binary, and run it on any operating system or CPU architecture using a compliant runtime.. In the browser, the JavaScript engine acts as the host runtime; outside the browser, this role is filled by standalone runtimes such as Wasmtime, Wasmer, or WasmEdge, which implement the WASI standard to provide secure access to system resources.\n\nMore than just enabling server-side execution, WASI introduced a fundamentally different and more secure way for programs to interact with the system. Traditional applications, following a model established by POSIX, typically inherit the permissions of the user who runs them. If a user can access a file, any program they run can also access that file, which creates a broad and implicit grant of authority.\n\nWASI, in contrast, implements a **capability-based security model**. By default, a Wasm module running via WASI can do nothing. It has no access to the filesystem, no ability to make network connections, and no visibility into system clocks or environment variables. To perform any of these actions, the host runtime must explicitly grant the module a 'capability'. For example, to allow a module to read files, the host must grant it a capability for a specific directory. The module receives a handle to that directory and can operate only within its confines. Any attempt to access a path outside of it will fail at the runtime level with a 'permission denied' error, even if the user running the process has permissions for that file. This enforces the **Principle of Least Privilege** at a granular level, a stark contrast to the traditional POSIX model where a process inherits all the ambient permissions of the user.\n\nThis \" *deny-by-default* \" posture represents a paradigm shift in application security. The decision to build WASI around a capability-based model was not merely a technical convenience; it was a deliberate architectural choice that transformed Wasm from a simple performance tool into a foundational building block for trustworthy computing. The browser sandbox provided an implicit security boundary designed to protect users from malicious websites. Simply mirroring traditional OS permissions on the server would have compromised this security-first ethos. Instead, by externalizing permission management from the application to the host runtime, WASI makes security an explicit, auditable contract.\n\nThis has profound implications, making Wasm uniquely suited for scenarios where the code being executed cannot be fully trusted. This includes multi-tenant serverless platforms running customer-submitted functions, extensible applications with third-party plugin systems, and edge devices executing logic from various sources. WASI did not just allow Wasm to run on the server; it defined how it would run: securely, with granular permissions, and by default, with no authority at all.\n\n### A Different Kind of Isolation: Wasm vs. Containers\n\nFor many developers today, the container has become the default unit of application deployment, a standardized box for packaging and running software. The rise of WebAssembly has introduced a new model, prompting a comparison that is less about which technology is superior and more about understanding two fundamentally different philosophies for achieving portability and isolation.\n\nThe container philosophy centers on porting the entire **environment**. A container image, such as one built with [Docker](https://dzone.com/articles/docker-use-cases-15-most-common-ways-to-use-docker), packages an application along with a complete slice of its user-space operating system: a filesystem, system libraries, configuration files, and all other dependencies. It achieves isolation from the host and other containers by leveraging OS-level virtualization features, primarily Linux namespaces and control groups (cgroups), which create the illusion of a private machine. The container's promise is that this self-contained environment will run consistently everywhere a container engine is installed.\n\nThe WebAssembly philosophy, in contrast, is about porting only the **application logic**. A Wasm module is a single, self-contained binary file containing just the compiled application code. It brings no operating system, no filesystem, and no system bundled libraries. Instead, it relies on the host runtime to provide a standardized environment and to mediate access to system resources through the WASI interface. Wasm's promise is that the application logic, compiled once, will run consistently everywhere a compliant Wasm runtime is present.\n\nThis philosophical divergence leads to significant practical trade-offs in size, speed, and security. Because a container must package a slice of an operating system, its image size is measured in (hundreds of) megabytes, even for simple applications. A Wasm module, containing only the application code, is orders of magnitude smaller, typically measured in kilobytes or a few megabytes. This dramatic difference impacts everything from storage costs and network transfer times to the density of workloads that can run on a single machine.\n\nThe most critical distinction, particularly for modern cloud architectures, is startup speed. A container must initialize its packaged environment: a process that involves setting up namespaces, mounting the filesystem, and booting the application. This \"cold start\" can take hundreds of milliseconds, or even several seconds. A Wasm module, on the other hand, is instantiated by an already-running runtime, a process that can take less than a millisecond (for compiled languages like Rust, C or Go). This near-instantaneous startup effectively eliminates the cold start problem, making Wasm an ideal technology for event-driven, scale-to-zero architectures like serverless functions, where responsiveness is paramount.\n\nThe security models also differ profoundly. Containers provide isolation at the OS kernel level. This means all containers on a host share the same kernel, which represents a large and complex attack surface. Security vulnerabilities often center on kernel exploits or misconfigurations that allow a process to \"escape\" its container and gain access to the host system. WebAssembly introduces an additional, finer-grained layer of isolation: the application-level sandbox. The attack surface is not the entire OS kernel, but the much smaller and more rigorously defined boundary of the Wasm runtime and the WASI interface. Combined with its capability-based security model, this makes Wasm \"secure by default\" and a far safer choice for running untrusted or third-party code.\n\n| Feature                 | **WebAssembly (WASM)**                                              | **Containers**                                                   |\n| ----------------------- | ------------------------------------------------------------------- | ---------------------------------------------------------------- |\n| **Unit of Portability** | Application Logic (a `.wasm` binary)                                | Application Environment (an OCI image with an OS filesystem)     |\n| **Isolation Model**     | Application-level Sandbox (deny-by-default)                         | OS-level Virtualization (namespaces, cgroups)                    |\n| **Security Boundary**   | Wasm Runtime \u0026 WASI Interface (small, well-defined)                 | Host OS Kernel (large, complex attack surface)                   |\n| **Startup Time**        | Sub-millisecond (\"zero cold start\")                                 | Hundreds of milliseconds to seconds (\"cold start\" problem)       |\n| **Size / Footprint**    | Kilobytes to Megabytes                                              | Megabytes to Gigabytes                                           |\n| **Platform Dependency** | Runtime-dependent (any OS/arch with a Wasm runtime)                 | OS and Architecture-dependent (e.g. `linux/amd64`)               |\n| **Ideal Use Case**      | Serverless functions, microservices, edge computing, plugin systems | Lift-and-shift legacy apps, complex stateful services, databases |\n\nUltimately, these two technologies are not adversaries but complements. It is common to run Wasm workloads inside containers as a first step toward integrating them into existing infrastructure. Each technology is optimized for different scenarios. Containers excel at lifting and shifting existing, complex applications that depend on a full POSIX-compliant environment, such as databases or legacy monolithic services. WebAssembly shines in the world of greenfield, cloud-native development, offering a lighter, faster, and more secure foundation for building the next generation of microservices and serverless functions.\n\n### New Foundations for Platform Engineering: The Cloud and the Edge\n\nFor WebAssembly to fulfill its potential as a server-side technology, it must integrate seamlessly into the dominant paradigm for cloud infrastructure management: Kubernetes. This integration is not just possible; it is already well underway, enabled by the extensible architecture of the cloud-native ecosystem. At its core, Kubernetes orchestrates workloads by communicating with a high-level container runtime, such as containerd, on each of its worker nodes. This high-level runtime is responsible for managing images and container lifecycles, but it delegates the actual task of running a process to a low-level runtime. For traditional Linux containers, this runtime is typically *runc*.\n\nThe key to running Wasm on Kubernetes lies in replacing this final link in the chain. Projects like [runwasi](https://github.com/containerd/runwasi) provide a \"shim\", a small piece of software that acts as a bridge, allowing containerd to communicate with a WebAssembly runtime (like Wasmtime or WasmEdge) just as it would with *runc*. This makes the Wasm runtime appear to Kubernetes as just another way to run workloads. The final piece of the integration is a Kubernetes object called a *RuntimeClass*, which acts as a label. By applying this label to a workload definition, developers can instruct the Kubernetes scheduler to deploy that specific workload to nodes configured with the Wasm shim, enabling Wasm modules and traditional containers to run side-by-side within the same cluster. Projects like [SpinKube](https://www.spinkube.dev/) are emerging to automate this entire setup process, making it easier for organizations to adopt Wasm without rebuilding their infrastructure from scratch.\n\nThis deep integration enables new and more efficient approaches to platform engineering: the discipline of building and managing the internal platforms that development teams use to ship software. In this pattern, the platform team provides standardized components that encapsulate common, cross-cutting concerns like logging, metrics, network access, and security policies. Application developers, in turn, focus solely on writing a \"user\" component that contains pure business logic. At deployment time, these two pieces are composed into a single, tiny, and secure Wasm binary. This creates a powerful separation of concerns. Developers are freed from boilerplate code and infrastructure details, while the platform team can enforce standards, patch vulnerabilities, and evolve the platform's capabilities centrally and transparently, without requiring application teams to rebuild or redeploy their code.\n\nWhile these patterns are transforming the cloud, it is at the network's edge where WebAssembly's advantages become not just beneficial, but essential. Edge computing involves moving computation away from centralized data centers and closer to where data is generated and consumed: on IoT devices, in factory machinery, at retail locations, or within telecommunication networks. These environments are often severely resource-constrained, with limited CPU, memory, and power, making heavyweight containers impractical or impossible to run.\n\nWebAssembly is a near-perfect fit for this world. Its incredibly small binary size and minimal resource footprint allow it to run on devices where containers cannot. Its near-instantaneous startup times are critical for the event-driven, real-time processing required in many edge scenarios. And its true platform independence, the ability for a single compiled binary to run on any CPU architecture, be it x86, ARM, or RISC-V, is a necessity in the heterogeneous hardware landscape of the edge. This has unlocked a new wave of applications, from running machine learning inference models to executing dynamic logic within Content Delivery Networks (CDNs) with ultra-low latency.\n\nThe ability of WebAssembly to operate seamlessly across these diverse environments reveals its most profound impact. Historically, software development has been siloed; building for the browser, the cloud, and embedded devices required different tools, different languages, and different deployment models. Containers helped unify deployment in the cloud, but they are foreign to the browser and too cumbersome for much of the edge. WebAssembly is the first technology to provide a single, consistent application runtime that spans this entire compute continuum. The true strength of WebAssembly lies in how its ecosystem bridges the historically separate worlds of the browser, cloud, and edge. While the final.wasm module is often tailored for its specific environment, Wasm as a standard provides a common compilation target. This allows developers to deploy applications across a vast spectrum: from a rich user interface in a web browser, to large-scale processing orchestrated by Kubernetes, and even to tiny, resource-constrained IoT devices. This reality enables a future where developers write their core business logic once and can deploy it to the most appropriate location: close to the user for low latency, in the cloud for heavy computation, or in the browser for interactivity without needing to rewrite or repackage it. This capability breaks down the architectural barriers that have long defined distributed systems, paving the way for a truly fluid and unified model of computation.\n\n### The Future is Composable: The WebAssembly Component Model\n\nDespite its portability and security, a final, fundamental challenge has historically limited WebAssembly's potential: true interoperability. While a single Wasm module is a self-contained unit, getting multiple modules to communicate with each other effectively has been remarkably difficult. The core Wasm specification only allows for the passing of simple numeric types, integers and floats, between modules. Exchanging more complex data structures like strings, lists, or objects requires developers to manually manage pointers and memory layouts, a process that is deeply tied to the conventions of the source language and compiler. This \"impedance mismatch\" means that a Wasm module compiled from Rust cannot easily call a function in a module compiled from Go, as they represent data in fundamentally incompatible ways. This has been the primary barrier to creating a vibrant, language-agnostic ecosystem of reusable Wasm libraries, forcing developers into fragile, language-specific linking models where modules must share a single linear memory space.\n\nThe [WebAssembly Component Model](https://component-model.bytecodealliance.org/) is the ambitious proposal designed to solve this final challenge. **It is critical, however, to understand its current status: the Component Model is an active proposal under development, not a finalized W3C standard**. While tooling and runtimes are rapidly implementing it, the specification is still subject to change. It is an evolution of the core standard that elevates Wasm from a format for individual, isolated modules into a system for building complex applications from smaller, interoperable, and language-agnostic parts. The most effective analogy for the Component Model is that it turns Wasm modules into standardized \"LEGO bricks\". Each component is a self-contained, reusable piece of software with well-defined connection points, allowing them to be snapped together to build something larger.\n\nTwo key concepts make this possible: **WIT** and “ **worlds** ”. The WebAssembly Interface Type (WIT) is an Interface Definition Language (IDL) used to describe the \"shape\" of the connectors on these metaphorical LEGO bricks. A WIT file defines the high-level functions and rich data types such as strings, lists, variants, and records that a component either **exports** (provides to others) or **imports** (requires from its environment).\n\nCrucially, the standard **WASI interfaces** themselves (e.g. for filesystems or sockets) are also defined using WIT. This means developers can use the exact same language to extend the default system capabilities with their own **domain-specific interfaces**, creating a unified and powerful way to describe any interaction.\n\nA \"world\" is a WIT definition that describes the complete set of interfaces a component interacts with, effectively declaring all of its capabilities and dependencies. Tooling built around the Component Model, such as [wit-bindgen](https://github.com/bytecodealliance/wit-bindgen), then automatically generates the necessary \"binding code\" for each language. This code handles the complex task of translating data between a language's native representation (e.g., a Rust String or a Python list) and a standardized, language-agnostic memory layout known as the [Canonical ABI](https://component-model.bytecodealliance.org/advanced/canonical-abi.html). The result is seamless interoperability: a component written in C++ can call a function exported by a component written in TinyGo, passing complex data back and forth as if they were native libraries in the same language, without either needing any knowledge of the other's internal implementation.\n\nThis enables a fundamentally different approach to software composition compared to the container world. Container-based architectures are typically composed at design time. Developers build discrete services, package them into containers, and then define how they interact, usually over a network via APIs, using orchestration configurations like Kubernetes manifests or Docker Compose files. This is a model for composing distributed systems. The WebAssembly Component Model enables granular composition at runtime. Components communicate through fast, standardized in-memory interfaces rather than network protocols, allowing them to be linked together within the same process. This creates a model for building applications from secure, sandboxed, and interchangeable parts.\n\nA prime example is [wasmCloud](https://wasmcloud.com/docs/concepts/linking-components/linking-at-runtime/). In this platform, components (called actors) declare dependencies on abstract interfaces, like a key-value store. At runtime, they are dynamically linked to providers that offer concrete implementations (e.g. a Redis provider).\n\nThe key advantage is that these links can be changed on the fly. You can swap the Redis provider for a different one without restarting or recompiling the application, perfectly realizing the goal of building flexible systems from truly interchangeable parts.\n\nThis shift from source-level libraries to compiled, sandboxed components as the fundamental unit of software reuse represents a paradigm shift. It is the technical realization of architectural concepts like Packaged Business Capabilities (PBCs), where distinct business functions are encapsulated as autonomous, deployable software components. A Wasm component provides a near-perfect implementation of a PBC: it is a compiled, portable, and secure artifact that encapsulates specific logic. The Component Model, therefore, is not just a technical upgrade for linking code. It is the foundation for a future where software is no longer just written, but composed. Developers will be able to assemble applications from a universal ecosystem of secure, pre-built components that provide best-of-breed solutions for specific tasks, fundamentally altering the nature of the software supply chain and accelerating innovation across all languages and platforms.\n\n### Conclusion: From a Faster Web to a Universal Runtime\n\nWebAssembly's journey has been one of remarkable and accelerating evolution. Born from the practical need to overcome performance bottlenecks in the web browser, its core principles of speed, portability, and security proved to be far more powerful than its creators may have initially envisioned. What began as a way to run C++ code alongside JavaScript has grown into a technology that is fundamentally reshaping our conception of software.\n\nThe introduction of the WebAssembly System Interface (WASI) was the pivotal moment, transforming Wasm from a browser-centric tool into a viable, universal runtime for server-side computing. Its capability-based security model offered a fresh, \"secure-by-default\" alternative to traditional application architectures. This new foundation allowed Wasm to emerge as a compelling counterpart to containers, offering an unparalleled combination of lightweight footprint, near-instantaneous startup, and a hardened security sandbox that is ideally suited for the demands of serverless functions and the resource-constrained world of edge computing. Today, Wasm is not just a technology for the browser, the cloud, or the edge; it is the first to provide a single, consistent runtime that spans this entire continuum, breaking down long-standing silos in software development.\n\nNow, with the advent of the Component Model, WebAssembly is poised for its next great leap. By solving the final, critical challenge of language-agnostic interoperability, it lays the groundwork for a future where applications are not monoliths to be built, but solutions to be composed from a global ecosystem of secure, reusable, and portable software components. WebAssembly is more than just a faster way to run code; it is a foundational shift toward a more modular, more secure, and truly universal paradigm for the next era of computing.","metadata":{"title":"WebAssembly: From Browser Plugin to the Next Universal Runtime","excerpt":"Explore WebAssembly's evolution from a browser performance booster to a universal runtime reshaping cloud, edge, and distributed computing with near-native performance across platforms.","date":"2025-08-04","author":"Graziano Casto"}},{"slug":"kubernetes-v134-sneak-peak","content":"\n[Published by Kubernetes co-authored with Agustina Barbetta, Alejandro Josue Leon Bellido, Melony Qin, Dipesh Rawat](https://kubernetes.io/blog/2025/07/28/kubernetes-v1-34-sneak-peek/)\n\nKubernetes v1.34 is coming at the end of August 2025. This release will not include any removal or deprecation, but it is packed with an impressive number of enhancements. Here are some of the features we are most excited about in this cycle!\n\nPlease note that this information reflects the current state of v1.34 development and may change before release.\n\n### Featured enhancements of Kubernetes v1.34 \n\nThe following list highlights some of the notable enhancements likely to be included in the v1.34 release, but is not an exhaustive list of all planned changes. This is not a commitment and the release content is subject to change.\n\n#### The core of DRA targets stable\n\n[Dynamic Resource Allocation](https://kubernetes.io/docs/concepts/scheduling-eviction/dynamic-resource-allocation/) (DRA) provides a flexible way to categorize, request, and use devices like GPUs or custom hardware in your Kubernetes cluster.\n\nSince the v1.30 release, DRA has been based around claiming devices using *structured parameters* that are opaque to the core of Kubernetes. The relevant enhancement proposal, [KEP-4381](https://kep.k8s.io/4381), took inspiration from dynamic provisioning for storage volumes. DRA with structured parameters relies on a set of supporting API kinds: ResourceClaim, DeviceClass, ResourceClaimTemplate, and ResourceSlice API types under `resource.k8s.io`, while extending the `.spec` for Pods with a new `resourceClaims` field. The core of DRA is targeting graduation to stable in Kubernetes v1.34.\n\nWith DRA, device drivers and cluster admins define device classes that are available for use. Workloads can claim devices from a device class within device requests. Kubernetes allocates matching devices to specific claims and places the corresponding Pods on nodes that can access the allocated devices. This framework provides flexible device filtering using CEL, centralized device categorization, and simplified Pod requests, among other benefits.\n\nOnce this feature has graduated, the `resource.k8s.io/v1` APIs will be available by default.\n\n#### ServiceAccount tokens for image pull authentication\n\nThe [ServiceAccount](https://kubernetes.io/docs/concepts/security/service-accounts/) token integration for `kubelet` credential providers is likely to reach beta and be enabled by default in Kubernetes v1.34. This allows the `kubelet` to use these tokens when pulling container images from registries that require authentication.\n\nThat support already exists as alpha, and is tracked as part of [KEP-4412](https://kep.k8s.io/4412).\n\nThe existing alpha integration allows the `kubelet` to use short-lived, automatically rotated ServiceAccount tokens (that follow OIDC-compliant semantics) to authenticate to a container image registry. Each token is scoped to one associated Pod; the overall mechanism replaces the need for long-lived image pull Secrets.\n\nAdopting this new approach reduces security risks, supports workload-level identity, and helps cut operational overhead. It brings image pull authentication closer to modern, identity-aware good practice.\n\n#### Pod replacement policy for Deployments\n\nAfter a change to a [Deployment](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/), terminating pods may stay up for a considerable amount of time and may consume additional resources. As part of [KEP-3973](https://kep.k8s.io/3973), the `.spec.podReplacementPolicy` field will be introduced (as alpha) for Deployments.\n\nIf your cluster has the feature enabled, you'll be able to select one of two policies:\n\n`TerminationStarted`\n\nCreates new pods as soon as old ones start terminating, resulting in faster rollouts at the cost of potentially higher resource consumption.\n\n`TerminationComplete`\n\nWaits until old pods fully terminate before creating new ones, resulting in slower rollouts but ensuring controlled resource consumption.\n\nThis feature makes Deployment behavior more predictable by letting you choose when new pods should be created during updates or scaling. It's beneficial when working in clusters with tight resource constraints or with workloads with long termination periods.\n\nIt's expected to be available as an alpha feature and can be enabled using the `DeploymentPodReplacementPolicy` and `DeploymentReplicaSetTerminatingReplicas` feature gates in the API server and kube-controller-manager.\n\n#### Production-ready tracing for kubelet and API Server\n\nTo address the longstanding challenge of debugging node-level issues by correlating disconnected logs,[KEP-2831](https://kep.k8s.io/2831) provides deep, contextual insights into the `kubelet`.\n\nThis feature instruments critical `kubelet` operations, particularly its gRPC calls to the Container Runtime Interface (CRI), using the vendor-agnostic OpenTelemetry standard. It allows operators to visualize the entire lifecycle of events (for example: a Pod startup) to pinpoint sources of latency and errors. Its most powerful aspect is the propagation of trace context; the `kubelet` passes a trace ID with its requests to the container runtime, enabling runtimes to link their own spans.\n\nThis effort is complemented by a parallel enhancement, [KEP-647](https://kep.k8s.io/647), which brings the same tracing capabilities to the Kubernetes API server. Together, these enhancements provide a more unified, end-to-end view of events, simplifying the process of pinpointing latency and errors from the control plane down to the node. These features have matured through the official Kubernetes release process.[KEP-2831](https://kep.k8s.io/2831) was introduced as an alpha feature in v1.25, while [KEP-647](https://kep.k8s.io/647) debuted as alpha in v1.22. Both enhancements were promoted to beta together in the v1.27 release. Looking forward, Kubelet Tracing ([KEP-2831](https://kep.k8s.io/2831)) and API Server Tracing ([KEP-647](https://kep.k8s.io/647)) are now targeting graduation to stable in the upcoming v1.34 release.\n\n#### PreferSameZone and PreferSameNode traffic distribution for Services\n\nThe `spec.trafficDistribution` field within a Kubernetes [Service](https://kubernetes.io/docs/concepts/services-networking/service/) allows users to express preferences for how traffic should be routed to Service endpoints.\n\n[KEP-3015](https://kep.k8s.io/3015) deprecates `PreferClose` and introduces two additional values: `PreferSameZone` and `PreferSameNode`.`PreferSameZone` is equivalent to the current `PreferClose`.`PreferSameNode` prioritizes sending traffic to endpoints on the same node as the client.\n\nThis feature was introduced in v1.33 behind the `PreferSameTrafficDistribution` feature gate. It is targeting graduation to beta in v1.34 with its feature gate enabled by default.\n\n#### Support for KYAML: a Kubernetes dialect of YAML\n\nKYAML aims to be a safer and less ambiguous YAML subset, and was designed specifically for Kubernetes. Whatever version of Kubernetes you use, you'll be able use KYAML for writing manifests and/or Helm charts. You can write KYAML and pass it as an input to **any** version of `kubectl`, because all KYAML files are also valid as YAML. With kubectl v1.34, we expect you'll also be able to request KYAML output from `kubectl` (as in `kubectl get -o kyaml …`). If you prefer, you can still request the output in JSON or YAML format.\n\nKYAML addresses specific challenges with both YAML and JSON. YAML's significant whitespace requires careful attention to indentation and nesting, while its optional string-quoting can lead to unexpected type coercion (for example: [\"The Norway Bug\"](https://hitchdev.com/strictyaml/why/implicit-typing-removed/)). Meanwhile, JSON lacks comment support and has strict requirements for trailing commas and quoted keys.\n\n[KEP-5295](https://kep.k8s.io/5295) introduces KYAML, which tries to address the most significant problems by:\n\n- Always double-quoting value strings\n- Leaving keys unquoted unless they are potentially ambiguous\n- Always using `{}` for mappings (associative arrays)\n- Always using `[]` for lists\n\nThis might sound a lot like JSON, because it is! But unlike JSON, KYAML supports comments, allows trailing commas, and doesn't require quoted keys.\n\nWe're hoping to see KYAML introduced as a new output format for `kubectl` v1.34. As with all these features, none of these changes are 100% confirmed; watch this space!\n\nAs a format, KYAML is and will remain a **strict subset of YAML**, ensuring that any compliant YAML parser can parse KYAML documents. Kubernetes does not require you to provide input specifically formatted as KYAML, and we have no plans to change that.\n\n#### Fine-grained autoscaling control with HPA configurable tolerance\n\n[KEP-4951](https://kep.k8s.io/4951) introduces a new feature that allows users to configure autoscaling tolerance on a per-HPA basis, overriding the default cluster-wide 10% tolerance setting that often proves too coarse-grained for diverse workloads. The enhancement adds an optional `tolerance` field to the HPA's `spec.behavior.scaleUp` and `spec.behavior.scaleDown` sections, enabling different tolerance values for scale-up and scale-down operations, which is particularly valuable since scale-up responsiveness is typically more critical than scale-down speed for handling traffic surges.\n\nReleased as alpha in Kubernetes v1.33 behind the `HPAConfigurableTolerance` feature gate, this feature is expected to graduate to beta in v1.34. This improvement helps to address scaling challenges with large deployments, where for scaling in, a 10% tolerance might mean leaving hundreds of unnecessary Pods running. Using the new, more flexible approach would enable workload-specific optimization for both responsive and conservative scaling behaviors.\n\n### Want to know more?\n\nNew features and deprecations are also announced in the Kubernetes release notes. We will formally announce what's new in [Kubernetes v1.34](https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.34.md) as part of the CHANGELOG for that release.\n\nThe Kubernetes v1.34 release is planned for **Wednesday 27th August 2025**. Stay tuned for updates!\n\n### Get involved\n\nThe simplest way to get involved with Kubernetes is to join one of the many [Special Interest Groups](https://github.com/kubernetes/community/blob/master/sig-list.md) (SIGs) that align with your interests. Have something you'd like to broadcast to the Kubernetes community? Share your voice at our weekly [community meeting](https://github.com/kubernetes/community/tree/master/communication), and through the channels below. Thank you for your continued feedback and support.\n\n- Follow us on Bluesky [@kubernetes.io](https://bsky.app/profile/kubernetes.io) for the latest updates\n- Join the community discussion on [Discuss](https://discuss.kubernetes.io/)\n- Join the community on [Slack](http://slack.k8s.io/)\n- Post questions (or answer questions) on [Server Fault](https://serverfault.com/questions/tagged/kubernetes) or [Stack Overflow](http://stackoverflow.com/questions/tagged/kubernetes)\n- Share your Kubernetes [story](https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform)\n- Read more about what's happening with Kubernetes on the [blog](https://kubernetes.io/blog/)\n- Learn more about the [Kubernetes Release Team](https://github.com/kubernetes/sig-release/tree/master/release-team)","metadata":{"title":"Kubernetes v1.34 Sneak Peek","excerpt":"Discover the exciting enhancements coming in Kubernetes v1.34, including stable Dynamic Resource Allocation, improved scheduling capabilities, and enhanced security features for production workloads.","date":"2025-07-28","author":"Graziano Casto"}},{"slug":"ai-native-platforms","content":"\n[Published by DZONE](https://dzone.com/articles/ai-native-platforms-genai-platform-engineering)\n\nLet's be honest. Building developer platforms, especially for **AI-native** teams, is a complex art, a constant challenge. It's about finding a delicate balance: granting maximum autonomy to development teams without spiraling into chaos, and providing incredibly powerful, cutting-edge tools without adding superfluous complexity to their already dense workload. Our objective as Platform Engineers has always been to pave the way, remove obstacles, and accelerate innovation. But what if the next, inevitable phase of platform evolution wasn't just about what we build and provide, but what Generative AI can help us co-build, co-design, and co-manage?\n\nWe're not talking about a mere incremental improvement, a minor optimization, or a marginal new feature. We're facing a genuine **paradigm shift**, a conceptual earthquake where artificial intelligence is no longer merely the final product of our efforts, the result of our development toils, but becomes the silent partner, the tireless ally that is already reimagining, rewriting, and redefining our entire development experience. This is the real gamble, the challenge that awaits us: transforming our platforms from simple toolsets, however sophisticated, into **intelligent, dynamic, and self-optimizing ecosystems**. A place where productivity isn't just high, but exceptionally high, and innovation flows frictionlessly.\n\n### What if we unlock 100% of our Platform’s potential?\n\nYour primary goal, like that of any good Platform Engineer, is already to make developers' lives simpler, faster, and, let's admit it, significantly more enjoyable. Now, imagine endowing your platform with genuine intelligence, with the ability to understand, anticipate, and even generate. GenAI, in this context, isn't just an additional feature that layers onto existing ones; it's the **catalyst** that is already fundamentally redefining the **Developer Experience (DevEx)**, exponentially accelerating the entire **software development lifecycle**, and, even more fascinating, creating **new, intuitive, and natural interfaces** for interacting with the platform's intrinsic capabilities.\n\nLet's momentarily consider the most common and frustrating pain points that still afflict the average developer: the exhaustive and often fruitless hunt through infinite and fragmented documentation, the obligation to memorize dozens, if not hundreds, of specific and often cryptic CLI commands, or the tedious and repetitive generation of boilerplate code. With the intelligent integration of GenAI, your platform magically evolves into a true **intelligent co-pilot**. Imagine a developer who can simply express a request in natural language, as if speaking to an expert colleague: \"Provision a new staging environment for my authentication microservice, complete with a PostgreSQL database, a dedicated Kafka topic, and integration with our monitoring system.\" The GenAI-powered platform not only understands the deep meaning and context of the request, not only translates the intention into a series of technical actions, but **executes the operation autonomously**, providing immediate feedback and magically configuring everything needed. This isn't mere automation, which we already know; it's a **conversational interaction**, deep and contextual, that almost completely zeroes out the developer's cognitive load, freeing their mind and creative energies to focus on innovation, not on the complex and often tedious infrastructural \"plumbing\".\n\nBut the impact extends far beyond simple commands. GenAI can act as an **omnipresent expert**, an always-available and incredibly informed figure, providing real-time, contextual assistance. Imagine being stuck on a dependency error, a hard-to-diagnose configuration problem, or a security vulnerability. Instead of spending hours searching forums or asking colleagues, you can ask the platform directly. And it, magically, suggests practical solutions, directs you to relevant internal best practices (perhaps your own guides, finally usable in an intelligent way!), or even proposes complete code patches to solve the problem. It can proactively identify potential security vulnerabilities in the code you've just generated or modified, suggest intelligent refactorings to improve performance, or even scaffold entire new modules or microservices based on high-level descriptions. This drastically accelerates the entire software development lifecycle, making best practices inherent to the process and transforming bottlenecks into opportunities for automation. Your platform is no longer a mere collection of passive tools, but an **intelligent and proactive partner** at every single stage of the developer's workflow, from conception to implementation, from testing to deployment.\n\nCrucially, for this to work, the GenAI model must be **fed with the right platform context**. By ingesting all platform documentation, internal APIs, service catalogs, and architectural patterns, the AI becomes an unparalleled tool for **discoverability of platform items**. Developers can now query in natural language to find the right component, service, or golden path for their needs. Furthermore, this contextual understanding allows the AI to **interrogate and access all data and assets** within the platform itself, as well as from the applications being developed on it, providing insights and recommendations in real-time. This elevates the concept of a **composable architecture**, already enabled by your platform, to an entirely new level. With an AI co-pilot that not only knows all available platform items but also understands how to use them optimally and how others have used them effectively, the development of new composable applications or rapid Proofs of Concept (PoCs) becomes faster than ever before.\n\nThe new interfaces enabled by GenAI go beyond mere suggestion. Think of natural language **chatbot interfaces** for giving commands, where the platform responds like a virtual assistant. Crucially, thanks to advancements like **Model Context Protocol (MCP)** or similar **tool-use capabilities**, the GenAI-powered platform can move beyond just \"suggesting\" and actively \"doing\". It can execute complex workflows, interact with external APIs, and trigger actions within your infrastructure. This fosters a true **cognitive architecture** where the model isn't just generating text but is an active participant in your operations, capable of generating architectural diagrams, provisioning resources, or even deploying components based on a simple natural language description. The vision is that of a \"platform agent\" or an \"AI persona\" that learns and adapts to the specific needs of the team and the individual developer, constantly optimizing their path and facilitating the adoption of best practices.\n\n### Platforms: the launchpad for AI-powered applications\n\nThis synergy is two-way, a deep symbiotic relationship. If, on one hand, GenAI infuses new intelligence and vitality into platforms, on the other, your **Internal Developer Platforms** are, and will increasingly become, the **essential launchpad** for the unstoppable explosion of **AI-powered applications**. The complex and often winding journey of an artificial intelligence model—from the very first phase of experimentation and prototyping, through intensive training, to serving in production and scalable inference—is riddled with often daunting infrastructural complexities. Dedicated GPU clusters, specialized Machine Learning frameworks, complex data pipelines, and scalable, secure, and performant serving endpoints are by no means trivial for every single team to manage independently.\n\nAnd this is where your platform uniquely shines. It has the power to **abstract away all the thorny and technical details** of AI infrastructure, providing **self-service and on-demand provisioning** of the exact compute resources (CPU, various types of GPUs), storage (object storage, data lakes), and networking required for every single phase of the model's lifecycle. Imagine a developer who has just finished training a new model and needs to deploy an inference service. Instead of interacting with the Ops team for days or weeks, they simply request it through an intuitive self-service portal on the platform, and within minutes, the platform automatically provisions the necessary hardware (perhaps a dedicated GPU instance), deploys the model to a scalable endpoint (e.g., a serverless service or a container on a dedicated cluster), and, transparently, even generates a secure API key for access and consumption. This process **eliminates days or weeks of manual configuration**, of tickets and waiting times, transforming a complex and often frustrating MLOps challenge into a fluid, instant, and completely self-service operation. The platform manages not only serving but the entire lifecycle: from data preparation, to training clusters, to evaluation and A/B testing phases, all the way to post-deployment monitoring.\n\nFurthermore, platforms provide crucial **golden paths** for AI application development at the application layer. There's no longer a need for every team to reinvent the wheel for common AI patterns. Your platform can offer **pre-built templates and codified best practices** for integrating Large Language Models (LLMs), implementing patterns like Retrieval-Augmented Generation (RAG) with connectors to your internal data sources, or setting up complete pipelines for model monitoring and evaluation. Think of robust libraries and opinionated frameworks for prompt engineering, for managing model and dataset versions, for specific AI model observability (e.g., tools for bias detection, model interpretation, or drift management). The platform becomes a hub for collaboration on AI assets, facilitating the sharing and reuse of models, datasets, and components, **including the development of AI agents**. By embedding best practices and pre-integrating the most common and necessary AI services, every single developer, even one without a deep Machine Learning background, is empowered to infuse their applications with intelligent, cutting-edge capabilities. This not only **democratizes AI development** across the organization but unlocks unprecedented innovation that was previously limited to a few specialized teams.\n\n### The future is symbiotic: your next move\n\nThe era of **AI-native** development isn't an option; it's an imminent reality, and it urgently demands **AI-native platforms**. The marriage of GenAI and Platform Engineering isn't just an evolutionary step; it's a **revolutionary leap** destined to redefine the very foundations of our craft. GenAI makes platforms intrinsically smarter, more intuitive, more responsive, and consequently, incredibly more powerful. Platforms, in turn, provide the robust, self-service infrastructure and the well-paved roads necessary to massively accelerate the adoption and deployment of AI across the enterprise, transforming potential into reality.\n\nAre you ready to stop building for AI and start building with AI? Now is the time to act. Identify the most painful bottlenecks in your current DevEx and think about how GenAI could transform them. Prioritize the creation of self-service capabilities for AI infrastructure, making model deployment as simple as that of a traditional microservice. Cultivate a culture of \"platform as a product\", where AI is not just a consumer, but a fundamental feature of the platform itself.\n\nThe future of software development isn't just about AI-powered applications; it's about an **AI-powered development experience** that completely redefines the concepts of productivity, creativity, and the very act of value creation. Embrace this unstoppable alliance, and unlock the next fascinating frontier of innovation. The time of static platforms is over. The era of intelligent platforms has just begun.\n\n","metadata":{"title":"AI-Native Platforms: The Unstoppable Alliance of GenAI and Platform Engineering","excerpt":"Discover how Generative AI is transforming platform engineering from static toolsets into intelligent, dynamic, and self-optimizing ecosystems that unlock 100% of platform potential.","date":"2025-06-11","author":"Graziano Casto"}},{"slug":"cognitive-architecture-how-llms-are-reshaping-software-architecture","content":"\n[Published by DZONE](https://dzone.com/articles/cognitive-architecture-llms-changing-software-development)\n\nSoftware architecture has long been rooted in object-oriented and, later, service-oriented paradigms. These models have helped teams build modular systems, isolating behavior into manageable services that communicate over well-defined APIs. As systems grew, microservices brought benefits like scalability and decoupling, but also introduced significant complexity in orchestration. \n\nToday, we're witnessing a fundamental shift. The growing influence of foundation models, particularly large language models (LLMs), is changing how we approach software design. These models aren't just code libraries; they can understand context, reason about goals, and generate human-like responses. This has led to the rise of agent-oriented programming, where autonomous agents, not statically programmed services, drive system behavior. In this new paradigm, agents are constructed from language models, structured prompts, memory layers, and external tools. \n\nWhat drives them is the **cognitive loop**: a cycle where an agent processes input, reasons over its state, takes actions using tools, and updates its memory. As small language models (SLMs) become more capable, this model is evolving to balance performance with flexibility and cost-efficiency.\n\n### The cognitive architecture\n\nAt the core of a cognitive architecture is a language model, effectively the brain of the system. This model is responsible for interpreting input, reasoning about goals, and planning actions. But reasoning alone is not enough. Just like the human brain depends on sensory organs and muscles to perceive and act on the world, an intelligent agent must be able to access and manipulate external systems in a structured way. This is the essence of agentic AI: giving models the ability to act, not just think.\n\nOne emerging approach to enable this interaction is the [Model Context Protocol (MCP)](https://www.anthropic.com/news/model-context-protocol), an open standard developed by Anthropic. MCP aims to provide a standardized interface through which models can retrieve contextual information and invoke tools in their environment. However, it’s important to note that MCP is still an early attempt, promising but not yet an established standard. It represents a broader effort across the AI community to define patterns and protocols that allow agents to interface safely and reliably with external components such as APIs, databases, and services. \n\nIn systems that use MCP or similar abstractions, the architecture separates reasoning from execution: the model focuses on understanding, planning, and decision-making, while dedicated tooling (like an MCP server) handles the actual execution of external operations. This creates a cognitive loop: the model observes inputs (from the user, sensors, or past interactions), interprets them using memory and reasoning, then takes action through tools that generate new inputs and continue the cycle.\n\nThe choice of model driving this architecture is essential. Large language models (LLMs) and small language models (SLMs) offer distinct trade-offs depending on the complexity of the task, the resource constraints, and the required level of reasoning. LLMs such as GPT-4, Claude, and Gemini are trained on massive corpora and exhibit broad generalization, abstraction, and conversational capabilities. They can manage multi-turn dialogues, resolve ambiguity, and reason across diverse domains. \n\nHowever, they come at a high computational cost and typically require substantial infrastructure to operate efficiently. On the other hand, SLMs like DistilBERT, TinyLLaMA, and Phi-2 are optimized for speed and efficiency. They are lightweight, often open-source, and can be deployed on edge devices or environments with limited resources. While their reasoning capabilities are more narrow and their context windows smaller, they are highly effective for specialized, domain-specific tasks where determinism and performance are prioritized over generalization. This naturally leads to hybrid system designs, where LLMs are responsible for global coordination and strategy, while SLMs handle routine or narrowly scoped operations. \n\nBelow is a comparison highlighting the core differences between the two:\n\n| Feature | Large Language Models (LLMs) | Small Language Models (SLMs) | \n| ------- | ---------------------------- | ---------------------------- |\n| **Examples** | GPT-4, Claude, Gemini | DistilBERT, TinyLLaMA, Phi-2 | \n| **Model Size** | Billions to trillions of parameters | Tens to hundreds of millions of parameters | \n| **Reasoning Ability** | High, can handle abstract, multi-step tasks | Limited to focused, well-defined tasks |\n| **Context Window** | Large (32k–128k tokens) | Small to medium (512–8k tokens) | \n| **Inference Cost** | High | Low |\n| **Deployment** | Cloud, high-performance infrastructure | Edge, browser, lightweight servers | \n| **Use cases** | Complex workflows, multi-agent coordination | Classification, log parsing, quick lookups | \n\nMost cognitive systems benefit from hybrid designs, where an LLM oversees high-level reasoning and coordination, while SLMs handle specialized, well-scoped operations combining performance, adaptability, and cost-efficiency.\n\n### From multi-service to multi-agent architectures: patterns for making agents work together\n\nAs cognitive architectures mature, they evolve from handling isolated use cases to coordinating distributed tasks across multiple agents. This mirrors the shift from monolithic applications to microservice-based designs — only here, the components are intelligent agents that understand goals, reason about actions, and collaborate toward shared outcomes. \n\nIn multi-agent architectures, each agent can be powered by the same or different language models, and they may have overlapping or distinct toolsets. Often, agents are also assigned specific personas or domains of expertise, allowing them to handle different parts of a broader workflow. The structure of multi-agent systems generally falls along a spectrum between two extremes: **vertical** and **horizontal** coordination. \n\nIn vertical architectures, one agent plays the role of leader, orchestrating others and delegating responsibilities in a top-down manner. Communication typically flows through this central agent, though in some cases, all agents may share a joint conversational thread overseen by the leader. These systems work well for hierarchical workflows that benefit from clear task separation and control. In contrast, horizontal architectures treat all agents as peers. Each agent can see the shared context and respond accordingly, contributing ideas, solving tasks, or calling tools independently. These systems are better suited for collaborative environments where feedback, shared reasoning, and open discussion improve task outcomes. \n\nWhether organized hierarchically or as peers, these agents can exchange information through memory structures, direct messaging, or via orchestration protocols such as [A2A (Agent-to-Agent)](https://github.com/google-a2a/A2A). As a result, systems gain modularity and resilience: agents can be updated or swapped without affecting the overall design, and capabilities can grow organically by expanding the agent set.\n\n### Agent-Oriented systems: an interactive shopping assistant example\n\nTo illustrate the capabilities of cognitive architectures, let’s consider an **interactive shopping assistant** for an e-commerce platform. Unlike a traditional product recommendation system focused on speed and structured queries, this assistant prioritizes a flexible, conversational user experience, allowing users to describe their preferences naturally and refine their choices through dialogue.\n\nImagine a user looking for a new outfit who might start with a free-form request like: “I’m looking for a red floral summer dress in medium size.” Instead of routing this through predefined APIs and checkboxes, we create an agent powered by an LLM that can understand such nuanced requests, extract product attributes, query the catalog, filter results, and engage in a dynamic conversation to refine the search.\n\nHere’s how this could work with an agent-oriented approach, leveraging frameworks like [Google's Agent Development Kit (ADK)](https://google.github.io/adk-docs/), LangChain, or AutoGen. Let's assume we use ADK and have several tools available to the agent:\n\n- `product_search(query_parameters: dict)`: This tool interacts with the product catalog. Instead of fetching the entire catalog, it takes structured parameters (e.g., `{'color': 'red', 'pattern': 'floral', 'category': 'dress', 'size': 'medium'}`) and returns a filtered list of products. This addresses the context window limitation by allowing the agent to perform targeted searches.\n- `image_recognition(image_url: str)`: This tool processes an uploaded image to identify attributes like color, style, and patterns.\n- `refine_search(product_id: str, new_parameters: dict)`: Allows the agent to modify an existing search or product selection based on user feedback.\n- `user_profile_update(preferences: dict)`: Stores user preferences in a vector memory or database for personalized recommendations in future interactions.\n\nThe agent’s workflow would incorporate a **planning phase** and the ability to handle **multi-turn interactions**:\n\n1. **Initial request and intent understanding (LLM)**: The user says, “I’m looking for a red floral summer dress in medium size.” The LLM agent, acting as the brain, processes this free-form text. It identifies the user’s intent (find a dress) and extracts key attributes: `color: red, pattern: floral, category: dress, size: medium`.\n   - **Planning**: The agent determines the best course of action. It decides to use the product_search tool first.\n2. **Tool invocation and execution**: The agent constructs a structured query based on the extracted attributes and calls the product_search tool: `product_search({'color': 'red', 'pattern': 'floral', 'category': 'dress', 'size': 'medium'})`.\n3. **Tool output and response generation (LLM)**: The `product_search` tool returns a list of matching dresses. The LLM then synthesizes these results into a human-readable response, perhaps showing a few top recommendations with product names and prices. For example: \"I found a few red floral summer dresses for you! How about the 'Crimson Bloom Maxi Dress' or the 'Garden Party Midi Dress'?\"\n4. **Refinement and dialogue (LLM and tools)**: The user responds, “I like the Garden Party Midi Dress, but do you have it in blue instead of red?”\n   - **Reasoning and planning**: The LLM understands this is a refinement request. It recognizes the `product_id` (Garden Party Midi Dress) and the new `color: blue`. It plans to use the `refine_search` tool.\n   - **Tool invocation**: The agent calls `refine_search({'product_id': 'Garden Party Midi Dress', 'color': 'blue'})`.\n   - **Guardrails/validation**: If the `refine_search` tool returns no results, the agent is programmed with a fallback: \"Unfortunately, the 'Garden Party Midi Dress' isn't available in blue. Would you like to see other blue floral dresses?\" This demonstrates a guardrail to ensure a graceful fallback rather than a generic error.\n5. **Image-based search (optional)**: If the user uploads a picture and says, “Find me something like this,” the agent could leverage the `image_recognition` tool to extract visual attributes, then use `product_search` with those attributes.\n\nHere’s a simplified Python example demonstrating the ADK agent with multiple tools:\n\n```python\nimport os\nimport requests\nfrom google.adk.agents import Agent\n\ndef product_search(query_parameters: dict) -\u003e dict:\n    \"\"\"\n    Searches the product catalog based on structured query parameters.\n\n    Args:\n        query_parameters (dict): A dictionary of parameters like {'color': 'red', 'category': 'dress'}.\n\n    Returns:\n        dict: The search response or an error message.\n    \"\"\"\n    try:\n        products_api_url = os.getenv(\"PRODUCTS_SEARCH_API_PATH\")\n        if not products_api_url:\n            raise ValueError(\"PRODUCTS_SEARCH_API_PATH not defined.\")\n        \n        response = requests.get(products_api_url, params=query_parameters)\n        response.raise_for_status()\n        return {\"status\": \"success\", \"report\": {\"data\": response.json()}}\n    except Exception as e:\n        return {\"status\": \"error\", \"error_message\": f\"Error searching products: {str(e)}\"}\n\ndef refine_search(product_id: str, new_parameters: dict) -\u003e dict:\n    \"\"\"\n    Refines an existing product search or modifies parameters for a specific product.\n    \n    Args:\n        product_id (str): The ID of the product to refine.\n        new_parameters (dict): New parameters to apply (e.g., {'color': 'blue'}).\n\n    Returns:\n        dict: The updated product information or an error.\n    \"\"\"\n    # This would typically interact with a product details API or an update mechanism\n    print(f\"Refining product {product_id} with parameters: {new_parameters}\")\n    # Simulate a successful refinement for demonstration\n    return {\"status\": \"success\", \"report\": {\"message\": f\"Refined search for {product_id} with new parameters.\"}}\n\n\nroot_agent = Agent(\n    name=\"interactive_shopping_assistant\",\n    model=\"gemini-2.0-flash\", # Or a more capable LLM like Gemini 1.5 Pro for complex reasoning\n    description=(\n        \"An agent that provides a conversational interface for product discovery and recommendations.\"\n    ),\n    instruction=(\n        \"You are a helpful interactive shopping assistant. Understand user preferences from freeform text or images, \"\n        \"use available tools to find products, and engage in multi-turn dialogues to refine results. \"\n        \"If a search yields no results, suggest alternative options gracefully.\"\n    ),\n    tools=[product_search, refine_search], # Add other tools like image_recognition as needed\n)\n\n# Example of agent processing a request (conceptual)\n# user_input_1 = \"I'm looking for a red floral summer dress in medium size.\"\n# agent_response_1 = root_agent.process_input(user_input_1) \n# print(agent_response_1)\n# \n# user_input_2 = \"I like the Garden Party Midi Dress, but do you have it in blue instead of red?\"\n# agent_response_2 = root_agent.process_input(user_input_2)\n# print(agent_response_2)\n```\n\nThe agentic approach simplifies orchestration logic by allowing the LLM to interpret intent, sequence tool usage, and manage context within a single reasoning loop. This makes the architecture highly adaptive to shifting user expectations and business needs, especially when new product attributes or complex search patterns emerge. \n\nThe ability to integrate user feedback in real-time, refine searches conversationally, and handle diverse input modalities (like text or images) demonstrates why cognitive architectures excel in scenarios requiring flexibility and natural interaction.\n\n### Benefits and challenges of cognitive architectures\n\nOne of the most significant advantages of cognitive architectures is the **natural interface** they provide. Users and developers can interact with systems through plain language rather than structured APIs or formal input schemas. This allows faster iteration and reduces the complexity typically associated with tightly coupled service orchestration. But the real shift comes from the agent’s ability to **reason**. \n\nReasoning is a core part of human intelligence: it allows us to make informed decisions, adapt to unexpected situations, and learn from new information. The same capabilities are essential for agents. Without reasoning, an agent might take user input too literally, fail to account for multi-step implications, or ignore relevant context. With reasoning, agents can plan, reflect, revise, and make decisions autonomously.\n\nIn practice, most agent architectures include a dedicated **planning phase**, where the model chooses how to act before executing any specific steps. This planning can follow various strategies, such as task decomposition, multi-option evaluation, retrieval-augmented guidance, or plan refinement. More advanced techniques, like representing plans as graphs (e.g., in Plan Like a Graph or PLaG), allow agents to execute steps in parallel, improving performance for workflows with many independent subtasks. The ability to **adapt** is another key benefit. Agents don’t require redeployment to change behavior: often, changing a prompt or swapping a tool is enough. They can integrate feedback, adjust strategies in real time, and operate in environments where the full task definition is not known upfront.\n\nOf course, there are tradeoffs. Agents must manage **limited context windows**, which can impact long-running or multi-step tasks. Reasoning itself requires larger models, which increases the cost. Using SLMs can reduce this overhead, but it comes with limited planning and abstraction capabilities. There’s also the challenge of **unpredictability**. Traditional systems are deterministic and easy to debug. In contrast, agents reason probabilistically, and tracing their decisions isn’t straightforward. Ensuring consistent outputs often means combining language model-based reasoning with **guardrails, fallback logic, or rules-based validators**, as seen in our shopping assistant example, where the agent gracefully handles unavailable product variations.\n\nFinally, **observability** remains a critical frontier. As reasoning becomes a central part of system behavior, we need better tools to trace decisions, evaluate alternatives, and debug unexpected outputs. This will be key to deploying robust, production-grade agentic systems at scale.\n\n### Conclusion\n\nThe movement toward agentic architectures signals a deeper change in how we think about software. Instead of writing detailed instructions and managing services manually, we are increasingly enabling intelligent agents to reason, act, and learn on our behalf. By carefully combining LLMs and SLMs, developers can design systems that are not only more powerful but also more adaptable. The cognitive capabilities of modern models allow us to abstract complexity and work closer to natural human thinking.\n\nYet this power comes with new responsibilities. As we step into a world of cognitive software, we must rethink reliability, cost management, and transparency. The future of software may not be written in code alone: it may be prompted, reasoned, and evolved through agents that think alongside us.\n\n\n\n\n\n\n\n\n\n\n","metadata":{"title":"Cognitive Architecture: How LLMs Are Reshaping Software Architecture","excerpt":"Discover how Large Language Models are transforming software design from traditional service-oriented architectures to cognitive, agent-driven systems that reason and act autonomously.","date":"2025-06-11","author":"Graziano Casto"}},{"slug":"java-next-act-native-speed-for-cloud-era","content":"\n[Published by DZONE](https://dzone.com/articles/java-native-speed-cloud-native-future)\n\nOnce the unshakable king of enterprise development, Java is now facing its most important existential moment since its creation. **It’s not that Java has failed; it’s that the world around it has changed.** And Java, with its traditional runtime model and heavyweight architecture, hasn’t always kept up.\n\nIn a cloud-native world where agility, speed, and efficiency matter more than ever, traditional Java stacks often struggle. They’re too slow to start, too heavy on memory, and too rigid for the ephemeral, elastic environments of today’s infrastructure.\n\nBut that’s not the end of the story.\n\nJava’s renaissance is happening, not through incremental tweaks to the JVM, but through native compilation. With tools like GraalVM, modern frameworks like Micronaut, and new ideas like containerless execution, Java is being reshaped for a new generation of cloud-native applications.\n\n### The problem with \"old Java\"\n\nFor years, Java’s greatest strength was the [Java Virtual Machine (JVM)](https://dzone.com/articles/jvm-architecture-explained). Its portability, mature performance optimizations, and expansive ecosystem made it unbeatable for large-scale systems. But what worked in the 2000s doesn’t always work today.\n\nCloud-native development demands lightweight applications that start instantly, scale elastically, and run efficiently in containers or serverless environments. **Every second of startup time and every megabyte of memory now translates into real costs.**\n\nTraditional JVM applications, especially those built with heavyweight frameworks like Spring, often take several seconds to start and consume hundreds of megabytes of memory, even for simple services. In modern microservices or serverless setups, that’s not just inefficient: it’s a nonstarter.\n\nAnd Java’s once-flexible dynamic features, runtime reflection, classpath scanning, bytecode manipulation, now create friction for ahead-of-time optimization. They make memory usage harder to control and hurt startup times.\n\n### Native compilation with GraalVM: Java reimagined\n\n[GraalVM](https://dzone.com/articles/graalvm-features-and-future) changes the game by introducing ahead-of-time (AOT) compilation for Java. With its **Native Image** tool, developers can compile Java code into native executables that don’t require a JVM to run.\n\nThe performance benefits are striking: native images can **reduce startup times** from seconds to milliseconds, shrink memory consumption by up to 90%, and eliminate just-in-time (JIT) compilation overhead entirely. In practice, that means Java applications can scale faster, idle leaner, and respond more predictably, ideal characteristics for modern cloud platforms, serverless environments, and edge computing nodes.\n\nBut GraalVM isn’t just about speed: it's also about **sustainability**.\n\nAs explored in [my article on JVM energy efficiency](https://dzone.com/articles/energy-efficiency-jvms-role-graalvm), GraalVM-native applications often consume significantly less power than their traditional JVM counterparts. That’s because they minimize CPU wake-ups, reduce memory pressure, and avoid the runtime overhead of garbage collection warmup or JIT tuning. For organizations operating at hyperscale or targeting low-power edge devices, this translates directly into lower energy bills and reduced carbon footprints — an increasingly important concern in both public and private sectors.\n\nThere’s also a **security benefit**. Native image compilation produces closed-world executables that strip out unused classes and dynamic capabilities, reducing the surface area for attacks. No classpath scanning, no runtime reflection, no dynamic proxies: just a tight, lean binary with only what your app needs.\n\nGraalVM isn’t a niche optimization: it’s the gateway to **making Java competitive again** in environments traditionally dominated by Go, Rust, and Node.js. It marries Java’s expressive power and ecosystem maturity with the raw performance and deployment agility of natively compiled binaries. And in a world where sustainability, responsiveness, and cost control are converging priorities, GraalVM-native Java stands out as **not just a smart technical choice but a strategic one.**\n\n### Micronaut: java, built for the cloud\n\nOf course, a fast runtime isn’t enough. You need a framework that’s designed for this new native world. [Micronaut](https://dzone.com/articles/a-quick-guide-to-microservices-with-the-micronaut) is that framework.\n\nMicronaut isn’t just a modern framework; it’s an architectural rethink tailored for the demands of today’s microservice environments. Unlike legacy Java frameworks that lean heavily on runtime reflection, dynamic proxies, and runtime dependency injection, Micronaut is built from the ground up with **ahead-of-time (AOT) principles at its core.** This design philosophy isn’t just about style: it has a measurable impact.\n\nIn [benchmarking studies](https://www.mdpi.com/2076-3417/13/3/1343) comparing Micronaut to Spring Boot and Quarkus, Micronaut consistently delivered the fastest application startup times and the smallest executable sizes when packaged as traditional JARs. This is no accident. Micronaut performs dependency injection, configuration resolution, and route compilation at build time, avoiding the costly runtime initialization that plagues traditional JVM apps. The result is fast cold starts, low memory footprints, and minimal CPU usage, perfect for auto-scaling in cloud and serverless environments.\n\nMicronaut's native image support via GraalVM is also first-class. In performance testing under average load, Micronaut native images achieved lower RAM and CPU consumption than Spring Boot and were competitive with Quarkus. This efficiency is especially valuable for edge devices and constrained environments, where every millisecond and megabyte counts. It excels in build-time optimizations, flexible modular design, and seamless GraalVM integration. It’s ideal for developers who want a native-first Java experience that scales efficiently across the cloud continuum: from serverless platforms to IoT edge nodes.\n\n### GraalOS and the vision of containerless java\n\nThe final frontier in this story is **how** we run these native applications.\n\nEnter **GraalOS**: a vision for a containerless, secure Java execution environment designed for native binaries. Still in the early stages, GraalOS was announced as a proof of concept, with plans to bring its capabilities to platforms like Oracle Cloud Infrastructure (OCI).\n\nWhile not yet available in Oracle Functions, the idea behind GraalOS is powerful: eliminate the need for containers altogether, deploy self-contained native binaries, and enable fast-starting, auto-scaling Java workloads that suspend when idle and launch with near-zero cold start time.\n\nIt’s an ambitious rethinking of Java in the serverless era where startup time, memory efficiency, and simplicity reign supreme. And though it’s still in development, it signals a bold direction for cloud-native Java.\n\n### Why this matters\n\nThese aren’t just technical improvements: they’re architectural inflection points.\n\nNative compilation brings Java back into serious contention in a cloud-native world. Where a traditional JVM-based app might take 8 seconds to boot and burn through 300MB of RAM, a GraalVM-compiled native image can start in 50 milliseconds and use a tenth of the memory.\n\nThat means:\n\n- Lower cloud bills\n- Faster scaling\n- Better performance at the edge\n- Simpler, leaner deployments\n\nIt’s not a marginal gain: it’s a foundational shift.\n\n### Developer experience: still Java, just better\n\nIf you’re worried about developer productivity, don’t be.\n\nNative Java development is supported by modern tools: **Maven, Gradle, Micronaut CLI**, and IDE plugins. Native image generation is just a build command away. Debugging, observability, stack traces, and integration with GitHub Actions or container registries? It’s all there.\n\nYou’ll need to be more explicit in your code. AOT compilers love predictability, but that’s a feature, not a flaw. You’ll write cleaner, faster, safer Java.\n\nAnd as the ecosystem grows, the native Java experience continues to improve.\n\n### Final thoughts: Java, competitive again\n\nLet’s be honest. If you’re still deploying monolithic Spring Boot apps on full JVMs inside bloated containers, you’re not wrong, but you are behind.\n\n**The future of Java is native.** GraalVM, Micronaut, and the innovations around native execution platforms are giving Java a second wind: leaner, faster, more cloud-friendly than ever.\n\nThis isn’t just keeping Java alive in the cloud era. It’s making Java **competitive** against Go, Rust, and anything else cloud-native development can throw at it.\n\nSo the question isn’t whether native Java is ready. The question is: **Are you?**","metadata":{"title":"Java's Next Act: Native Speed for a Cloud-Native World","excerpt":"Explore how the Graal Stack is reinventing Java for the cloud era, combining GraalVM, Micronaut, and GraalOS to deliver ultra-fast, lightweight, and serverless-ready applications that address traditional Java limitations.","date":"2025-04-30","author":"Graziano Casto"}},{"slug":"breaking-the-context-barrier-of-llms-infiniretri-vs-rag","content":"\n[Published by DZONE](https://dzone.com/articles/breaking-context-barrier-llms-infiniretriever-vs-rag)\n\nLarge language models (LLMs) are reshaping the landscape of artificial intelligence, yet they face an ongoing challenge — retrieving and utilizing information beyond their training data. Two competing methods have emerged as solutions to this problem: **InfiniRetri**, an approach that exploits the LLM’s own attention mechanism to retrieve relevant context from within long inputs, and **retrieval-augmented generation** (RAG), which dynamically fetches external knowledge from structured databases before generating responses. \n\nEach of these approaches presents unique strengths, limitations, and trade-offs. While InfiniRetri aims to maximize efficiency by working within the model’s existing architecture, RAG enhances factual accuracy by integrating real-time external information. But which one is superior?\n\nUnderstanding how these two methods operate, where they excel, and where they struggle is essential for determining their role in the future of AI-driven text generation.\n\n### How InfiniRetri and RAG retrieve information\n\nInfiniRetri functions by leveraging the native attention mechanisms of transformer-based models to dynamically retrieve relevant tokens from long contexts. Instead of expanding the model’s context window indefinitely, InfiniRetri iteratively selects and retains only the most important tokens, allowing it to handle significantly long inputs while optimizing memory efficiency. \n\nUnlike standard LLMs, which process a finite-length input and discard previous information once the context window is exceeded, InfiniRetri uses a rolling memory system. It processes text in segments, identifying and storing only the most relevant tokens while discarding redundant information. This allows it to efficiently retrieve key details from vast inputs without needing external storage or database lookups. \n\nIn controlled retrieval scenarios such as the Needle-In-a-Haystack (NIH) test, InfiniRetri has demonstrated 100% retrieval accuracy over 1 million tokens, highlighting its ability to track key information over extremely long contexts. However, this does not imply perfect accuracy across all tasks.\n\n![](/images/blog/breaking-the-context-barrier-of-llms-infiniretri-vs-rag/photo-1.png)\n\nOn the other hand, RAG takes an entirely different approach by augmenting the model with an external retrieval step. When presented with a query, RAG first searches a knowledge base — often a vector database, document repository, or search engine — to find relevant supporting documents. \n\nThese retrieved texts are then appended to the LLM’s input, allowing it to generate responses that are grounded in real-time, external information. This method ensures that the model has access to fresh, domain-specific knowledge, making it far less prone to hallucination than purely parametric models.\n\n![](/images/blog/breaking-the-context-barrier-of-llms-infiniretri-vs-rag/photo-2.png)\n\nThe key difference lies in where the retrieval takes place. InfiniRetri retrieves internally from previously processed text, whereas RAG retrieves externally from structured knowledge bases. This has major implications for performance, efficiency, and scalability.\n\n## Which approach is more effective?\n\nPerformance comparisons between InfiniRetri and RAG reveal stark contrasts in efficiency, accuracy, and computational demands. InfiniRetri’s ability to dynamically retrieve information within its own architecture allows it to operate without additional infrastructure — it does not need external storage, retrievers, or fine-tuned embeddings. This makes it an excellent option for long-document processing, where the relevant information is already contained within the provided input.\n\nHowever, InfiniRetri does have limitations. Since it operates solely within the model’s attention mechanism, it depends entirely on the LLM’s pre-existing knowledge. If a piece of information is not included in the model’s training or input, it simply cannot be retrieved. This makes InfiniRetri less effective for answering fact-based or real-time queries that require up-to-date knowledge.\n\nRAG, by contrast, excels in knowledge-intensive tasks. Because it pulls information from an external database, it can supplement the model’s pre-trained knowledge with factual, real-world information. This makes it highly effective for question-answering, legal document processing, and research applications where accuracy is critical. \n\nHowever, RAG’s reliance on external retrieval comes with a price in computational costs that vary depending on the retrieval method used. Additionally, external queries introduce latency, which scales with database size. Each query requires a database search, document retrieval, and augmentation before the LLM can generate a response, making it significantly slower than InfiniRetri for continuous long-text processing.\n\nIn terms of computational efficiency, InfiniRetri has a clear edge. Since it retrieves information internally without requiring API calls to external systems, it runs at lower latency and with fewer infrastructure demands. Meanwhile, RAG, although powerful, is limited by the efficiency of its retriever, which must be fine-tuned to ensure high recall and relevance.\n\n## Which one fits your needs?\n\nWhile both methods are highly effective in their own domains, neither is a one-size-fits-all solution. InfiniRetri is best suited for applications that require efficient long-document retrieval but do not need external knowledge updates. This includes legal document analysis, multi-turn dialogue retention, and long-form summarization. Its iterative approach to selecting and retaining relevant tokens enables efficient long-text processing without overwhelming memory, making it a strong choice for narrative coherence and reasoning-based tasks.\n\nRAG, on the other hand, is ideal for real-world information retrieval where accuracy and fact-checking are paramount. It is highly effective for open-domain question-answering, research-based applications, and industries where hallucination must be minimized. Because it retrieves from external sources, it ensures that responses remain grounded in verifiable facts rather than relying on the model’s static training data.\n\nHowever, RAG requires constant maintenance of its retrieval infrastructure. Updating the external database is crucial for maintaining accuracy, and managing indexing, embeddings, and storage can introduce significant operational complexity. Latency is also a major issue, as retrieval times increase with database size, making it less suitable for real-time applications where speed is critical.\n\n## Will these methods merge?\n\nAs AI research advances, it is likely that the future of retrieval will not be a battle between InfiniRetri and RAG, but rather a combination of both. Hybrid approaches could leverage InfiniRetri’s efficient attention-based retrieval for processing long documents, while still incorporating RAG’s ability to fetch real-time external knowledge when needed.\n\nOne promising direction is adaptive retrieval models, where the LLM first attempts to retrieve internally using InfiniRetri’s method. If it determines that essential information is missing, it could then trigger an external RAG-like retrieval step. This would balance computational efficiency with accuracy, reducing unnecessary retrieval calls while still ensuring fact-based grounding when required.\n\nAnother area of development is intelligent caching mechanisms, where relevant information retrieved externally via RAG could be stored and managed internally using InfiniRetri’s attention techniques. This would allow models to reuse retrieved knowledge over multiple interactions without needing repeated database queries, reducing latency and improving performance.\n\n## Choosing the right tool for the job\n\nThe choice between InfiniRetri and RAG ultimately depends on the specific needs of a given application. If the task requires fast, efficient, and scalable long-context retrieval, InfiniRetri is the clear winner. If the task demands real-time fact-checking and external knowledge augmentation, RAG remains the best choice.\n\nWhile these two approaches have distinct advantages, the reality is that they can serve complementary roles, particularly in hybrid systems that dynamically balance internal attention-based retrieval with external knowledge augmentation based on task requirements. Future retrieval systems will likely integrate elements from both, leading to more powerful and adaptable AI models. Rather than a question of “InfiniRetri vs. RAG,” the real future of LLM retrieval may be InfiniRetri and RAG working together.\n\n## Further reading\n\nFor those who want to explore the full technical details behind these approaches, I encourage you to read the [Infinite Retrieval: Attention Enhanced LLMs in Long-Context Processing](https://arxiv.org/abs/2502.12962?trk=public_post_comment-text) and [Retrieval-Augmented Generation for Large Language Models: A Survey](https://simg.baai.ac.cn/paperfile/25a43194-c74c-4cd3-b60f-0a1f27f8b8af.pdf) research papers on InfiniRetri and RAG to gain a deeper understanding of their methodologies, benchmarks, and real-world applications.\n\n\n","metadata":{"title":"Breaking the Context Barrier of LLMs: InfiniRetri vs RAG","excerpt":"Compare InfiniRetri and RAG approaches for LLM information retrieval, exploring their unique strengths, limitations, and potential hybrid solutions for overcoming context limitations.","date":"2025-03-27","author":"Graziano Casto"}},{"slug":"are-you-sure-you-really-want-to-be-a-devrel","content":"\nIf you came to this article hoping to get a complete overview of what being a Developer Relations professional really means, I’m afraid I’ll have to disappoint you. If you’re expecting a long list of criticisms or judgments about the work of a DevRel, this isn’t the right place either. And if you’re looking for some kind of enlightenment about the role, sorry, but you’re still in the wrong spot.\nWhat you’ll find here is a personal reflection on my two years as a DevRel, without claiming to teach anyone anything and without the intention of sparking debates or pointless discussions. My goal is simply to share how my perception of the role has changed over time, depending on the context in which I lived and worked. Just a lived experience, with no filters.\n\n### To be or not to be, this is the question\n\nLet’s get one thing straight right away: I work as a Developer Relations Engineer at a company that develops a closed-source product, which is part of a broader ecosystem—the Cloud Native space, as defined by the CNCF and illustrated in the well-known landscape.\nWhy am I telling you this? Because everything that follows is based on this context. A DevRel’s experience can vary dramatically depending on the company, the product, and—most importantly—the business goals that drive the Developer Relations initiatives. Changing even one of these variables can completely reshape the role, like adjusting a small detail in a painting and seeing how it affects the entire picture.\n\nSo, the question:\n\n**“I give talks—does that mean I’m doing DevRel?”**\nThe answer is yes.\n\n**“I write articles—does that mean I’m doing DevRel?”**\nYes, you are.\n\n**“I mentor others or help build communities—am I doing DevRel?”**\nAgain, yes. But there’s a catch.\n\nHere’s the thing: **Doing something doesn’t automatically mean being something.**\nSpeaking at conferences, writing articles, and engaging with communities are all activities that fall under the DevRel umbrella. But doing these things alone isn’t enough to make you a DevRel. It’s not just about what you do—it’s about how and why you do it.\n\nDon’t get me wrong: I’m not downplaying the value of giving a talk, contributing to a community, or sharing knowledge. These are vital activities for any DevRel. But without a clear strategy and a defined ROI for your company, these efforts—while valuable—might not deliver the outcomes you hope for. And, more importantly, they won’t truly elevate you into the Developer Relations role.\n\n### My little patch of land as a DevRel\n\n![](/images/blog/are-you-sure-you-really-want-to-be-a-devrel/photo-1.png)\n\nThere’s a harsh truth I had to come to terms with when I started this journey. If you’re doing DevRel, you’re not part of marketing, engineering, or sales. Yet, it’s crucial that you are a bit of everything and, at the same time, nothing in particular. You’ll soon see what I mean by this.\n\nWe’ve all been taught, from early education, to analyze things using the 5 W’s (who, what, where, when, why). Let’s try to apply this framework here. Who is my target, and what do I do to reach them?\n\nMy targets are four: the **current product user base** (our customers), the **potential product user base** (the market the product fits into), the **industry** the product belongs to, and the **community** of people the product is aimed at.\n\nTo reach these targets, the activities vary and hold different weights and values for each persona. The goal with the current user base is to make their lives easier. But easier in what way? Helping them discover new features, understand how to integrate the product in new contexts, break down the barriers of first-time use, and make the product more accessible. The key word here is **training**. Whether through courses, comprehensive documentation, or other resources, the point is always the same: make it easier to approach and progressively discover the product’s potential.\n\nThe current user base and potential user base often overlap. Here, the keyword is **solutions**. The main goal is to communicate the product’s value. And what better way to do that than by solving a problem? Demos, tutorials, and Proof of Concepts (PoCs) become key tools to show how the product fits into a real-world context, allowing the audience to relate to the problem and understand the value of the proposed solution.\n\nThe industry and community hold strategic value for two reasons: the community is crucial for the brand and positioning, helping to build credibility with a broader audience; the industry is important for staying in tune with technological advances, influencing roadmaps and product messaging. The keywords here are **sharing** and **relationships**. Sharing experiences with the community, even beyond product evangelism, strengthens trust and reputation. At the same time, engaging in an industry (in my case, the Cloud Native development world, represented by the Cloud Native Computing Foundation) helps build relationships, understand how others tackle similar problems, and stay updated on technological trends.\n\nIn a few words: I aim to improve the lives of my product’s users. At the same time, I work to influence the potential user base by highlighting the product’s strengths and helping position it in the market. All of this, while never stopping to listen to the industry and contribute to communities.\n\n### The DevRel in the corporate context\n\nAlright, we understand what you do, but what value does someone in your role bring to the company structure? How do you interact with other teams? How do you fit into the broader company picture?\n\n![](/images/blog/are-you-sure-you-really-want-to-be-a-devrel/photo-2.png)\n\nBased on what we discussed earlier, we can summarize the role of a DevRel as that of an orchestrator who facilitates the flow of information between the product and the outside world, and vice versa.\n\nThe first key responsibility of a DevRel is to generate value for the product. On one hand, there’s the task of training; on the other, user feedback plays an equally crucial role.\n\nA DevRel often collects feedback directly, but a significant portion of these insights comes from other departments, such as Sales and Solutions Engineering, who gather them during customer or prospect management cycles. In this context, the added value of the DevRel is to act as a bridge, ensuring that this feedback is not only collected but also utilized. How? It depends. Recurring feedback can lead to the creation of supporting materials for Solutions Engineers, such as demos, documentation, or tutorials to better explain specific features. Reports of deficiencies, whether real or perceived, can be communicated to the development team, influencing the product roadmap or improving existing implementations.\n\nAt the same time, working with communities helps spread product-related messages and strengthen the corporate brand, contributing to the company’s thought leadership. Additionally, monitoring and interacting with the industry helps identify and understand market trends, bringing this information back to the company to refine product positioning and messaging.\n\nIn this context, the real advancement is making the entire operation scalable. After all, Rome wasn’t built in a day, and certainly not by one person. A good DevRel knows that to build something lasting, it’s essential to enable and involve the entire company community. Through the power of inner sourcing, the DevRel works to ensure that even those who don’t formally hold the role can contribute to Developer Relations activities, making the work more sustainable and, above all, scalable. Because, in the end, the voice of many is always more powerful than the voice of just one, right?\n\n### So, does the DevRel not need to generate leads?\n\nFalling into the trap of oversimplifying the DevRel role as just a lead generator is a mistake that, sooner or later, we all stumble upon. Yet, as we've seen, there’s much more to this role: the real challenges go far beyond just gathering contacts. Leads do come, of course, but they are the natural outcome of a broader and more complex job.\n\nThe true task of a good DevRel is to maintain the right balance between the product and the people: always keeping the focus on users, while never losing sight of the quality of what’s being built. Only by doing this can you truly unlock both your potential and that of the product, contributing, day by day, to making—if only a little—the lives of its users better.\n\nIs it really worth embarking on this journey in the world of DevRel? The answer is up to you. As for me, after exploring various aspects of software development, I believe I’ve found my place here: a role that allows me to nurture both the human and technological sides of this work, with the freedom to constantly experiment.\n\nBut, as always, the final judgment will be left to future generations.\n\n\n\n\n\n\n","metadata":{"title":"Are You Sure You Really Want to Become a DevRel?","excerpt":"A personal reflection on two years as a Developer Relations Engineer, sharing honest insights about the role's challenges, rewards, and how context shapes the DevRel experience in different organizations.","date":"2025-02-28","author":"Graziano Casto"}},{"slug":"ai-agents-future-of-automation-or-overhyped-buzzword","content":"\n[Published by DZONE](https://dzone.com/articles/ai-agents-future-of-automation-or-overhyped-buzzword)\n\nIn a world obsessed with artificial intelligence, there's a new player in town — AI agents. But before you roll your eyes and think, \"Great, another tech term to pretend I understand at meetings,\" let’s break it down.\n\n### What the heck are AI Agents?\n\nImagine you have a really smart assistant — not just one that tells you the weather or suggests a new Netflix show — but one that thinks, plans, and acts without you having to spell everything out. That’s what AI agents are all about. Unlike simple chatbots or automation scripts that follow rigid, predefined paths, AI agents are designed to be **autonomous**. They don’t just react; they perceive, decide, and take action based on goals.\n\nAt their core, AI agents have three main components:\n\n1. **A model.** The brain behind the agent, often powered by a language model (like GPT) or a combination of AI techniques.\n2. **Tools.** The “hands” of the agent, allowing it to interact with databases, APIs, and external systems.\n3. **An orchestration layer.** This governs how the agent perceives its environment, plans, and acts.\n\nThink of an AI agent as a chef in a high-end restaurant. It looks at the ingredients available (perception), decides what to cook (reasoning), and then actually prepares the dish (action). This cycle repeats and improves over time, making the agent more efficient and effective.\n\n### Not everything that talks back is an AI Agent\n\nHere’s where we need to clear up some confusion. Just because something is labeled as “AI” doesn’t make it an agent.\n\nA large language model, like the ones behind your favorite chatbots, is an impressive text generator. It predicts words based on patterns it has learned but doesn’t actually understand what it’s saying. It’s like a parrot that repeats words convincingly but doesn’t grasp their meaning. Similarly, chatbots and automated customer service assistants might give helpful responses, but they’re simply regurgitating predefined scripts — they don’t make decisions or adapt dynamically.\n\nAI agents, on the other hand, are **goal-oriented problem solvers**. They don’t just answer questions; they analyze real-time data, make informed decisions, and adapt their behavior to achieve complex objectives. Imagine hiring a new employee — one that doesn’t just do what they’re told but also figures out what needs to be done, identifies the best way to do it, and improves over time. That’s the difference between a basic chatbot and an AI Agent.\n\n### How are AI Agents built?\n\nAI agents are not just simple programs following a script; they are complex systems built with multiple interdependent components. Their architecture can be broken down into three fundamental parts:\n\n##### The model\n\nThis is the core decision-making unit of an AI agent. It typically consists of machine learning models, including large language models (LLMs), neural networks, and other AI techniques. These models process input data, generate predictions, and make informed decisions based on patterns and learned behaviors.\n\n##### The tools \n\nAI agents extend their capabilities through external tools such as APIs, databases, search engines, or specialized functions. These tools allow agents to retrieve real-time information, interact with digital systems, and even execute specific tasks beyond their initial training data.\n\n##### The orchestration layer\n\nThis governs the entire operational cycle of an AI agent. It includes mechanisms for perception (input processing), reasoning (decision-making), and action (executing tasks). The orchestration layer ensures the agent dynamically adapts to new inputs and refines its responses over time.\n\n![](/images/blog/ai-agents-future-of-automation-or-overhyped-buzzword/photo-1.png)\n\n### Cognitive Architecture: the brain of AI Agents\n\nThe cognitive architecture of an AI agent defines how it processes information, reasons through problems, and interacts with its environment. This architecture typically includes the following:\n\n##### 1. Perception Module\n\nThe agent collects raw data from its surroundings, which can include structured databases, real-time web scraping, or even IoT sensor inputs.\n\n##### 2. Memory and knowledge graphs\n\nAI agents store and retrieve relevant information to maintain context over time. This includes both short-term memory (session-based interactions) and long-term memory (historical learning and pattern recognition).\n\n##### 3. Decision making and planning\n\nAgents use frameworks such as Chain-of-Thought (CoT) or Tree-of-Thought (ToT) reasoning to break complex tasks into manageable steps, analyze multiple solutions, and select the best course of action.\n\n##### 4. Action execution\n\nOnce a decision is made, the agent interacts with its environment using predefined tools, API calls, or even physical actuators in robotics-based implementations.\n\n##### 5. Feedback loop and continuous learning\n\nAI agents refine their decision-making process over time through reinforcement learning, self-supervised learning, or user feedback mechanisms.\n\nThink of an AI agent like a self-driving car. The model is the brain that makes driving decisions, the tools include sensors and navigation systems to interact with the road, and the orchestration layer ensures all these components work in sync to drive safely and efficiently. The cognitive architecture enables the car to not only drive but also learn from past trips, anticipate potential obstacles, and adapt to new routes dynamically.\n\n### Why should you care?\n\nAI agents are not just an evolution of AI; they are a fundamental shift in IT operations and decision-making. These agents are being increasingly integrated into Predictive AIOps (Artificial Intelligence for IT Operations), where they autonomously manage, optimize, and troubleshoot systems without human intervention. Unlike traditional automation, which follows pre-defined scripts, AI agents dynamically **predict, adapt, and respond** to system conditions in real time.\n\nSome key benefits of AI agents include:\n\n1. **Proactive issue resolution.** AI agents in AIOps identify potential failures before they occur, reducing downtime and ensuring system resilience.\n2. **Autonomous decision-making.** They optimize system performance, allocate resources, and resolve errors without waiting for human input.\n3. **Scalability and adaptability.** AI agents continuously learn from system data, adjust in real time, and enhance operational efficiency without requiring frequent manual updates.\n4. **Enhanced IT autonomy.** By leveraging reinforcement learning and predictive analytics, AI agents create self-sustaining IT ecosystems, minimizing operational risks and human workload.\n\nOkay, so AI agents sound cool, but what can they actually do?\n\n1. **Adaptive and self-sustaining AI systems.** AI agents are transforming IT management and operational resilience. Instead of just replacing workflows, they now optimize and predict system health, automatically mitigating risks and reducing downtime. Whether it's self-repairing IT infrastructure, real-time cybersecurity monitoring, or orchestrating distributed cloud environments, AI Agents are pushing technology toward self-governing, intelligent automation.\n2. **Dynamic decision-making.** AI agents continuously analyze complex systems in real time, using advanced cognitive architectures to make decisions without predefined rules. This allows them to detect anomalies, mitigate security risks, and reconfigure environments autonomously.\n3. **Autonomous systems in IT and cybersecurity.** AI agents are not just digital assistants but active participants in managing IT infrastructure. They autonomously allocate resources, detect vulnerabilities, and adapt to emerging threats, enhancing system resilience without human oversight.\n4. **Self-learning and predictive adaptation.** AI agents employ reinforcement learning techniques, meaning they refine their behavior based on past experiences. Whether it’s optimizing system performance, predicting potential failures, or automating complex workflows, these agents continuously improve without requiring manual intervention.\n\n### What’s next for AI Agents?\n\nThe future of AI agents is both thrilling and terrifying. Companies are investing in large action models (LAMs) — next-gen AI that doesn’t just generate text but actually does things. We’re talking about AI that can manage entire business processes or run a company’s operations without human intervention.\n\nBut with great power comes great responsibility, right? AI agents will also need governance, ethical considerations, and built-in safeguards to prevent them from going rogue (because, let’s face it, we’ve all seen Terminator).\n\n### Final thoughts: hype or reality?\n\nAI agents aren’t just another tech buzzword — they represent a fundamental shift in how AI interacts with the world. Sure, we’re still in the early days, and there’s a lot of fluff in the market, but make no mistake: **AI agents will change the way we work, live, and do business.**\n\nThe question is: Are you ready for them, or will you be left scrambling to catch up?\n\n### Further reading and sources\n\nFor those interested in diving deeper into the world of AI agents and their applications, I highly recommend exploring the research behind Predictive AIOps and cognitive AI architectures. The insights presented in [Agentic AI in Predictive AIOps: Enhancing IT Autonomy and Performance](https://www.researchgate.net/publication/386575975_Agentic_AI_in_Predictive_AIOps_Enhancing_IT_Autonomy_and_Performance) provide a strong foundation for understanding how AI agents are transforming IT operations and decision-making processes. \n\nAdditionally, the whitepaper [Agents](https://www.kaggle.com/whitepaper-agents) explores the intricate details of AI agent architectures, including cognitive reasoning, decision-making models, and integration with external tools. This paper highlights how AI agents bridge the gap between foundational models and real-world applications, extending their utility far beyond simple automation.\n\nIf you're curious about the frameworks and methodologies that power AI agents, both of these sources will help you gain a more comprehensive understanding of the technology and its implications.\n\nAI agents are not just a futuristic concept; they are already reshaping industries. The key question remains — will you be a passive observer or an active participant in this revolution?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","metadata":{"title":"AI Agents: Future of Automation or Overhyped Buzzword?","excerpt":"Explore the reality behind AI agents - autonomous systems that perceive, decide, and act independently. Discover their potential to revolutionize automation while understanding the challenges and limitations.","date":"2025-02-20","author":"Graziano Casto"}},{"slug":"energy-efficiency-of-jvm-and-the-role-of-graalvm","content":"\n[Published by DZONE](https://dzone.com/articles/energy-efficiency-jvms-role-graalvm)\n\n\nAs the world becomes increasingly conscious of energy consumption and its environmental impact, software development is joining the movement to **go green**. Surprisingly, even the choice of runtime environments and how code is executed can affect energy consumption. This brings us to the world of Java Virtual Machines (JVMs), an integral part of running Java applications, and the rising star in the JVM world, GraalVM. \n\nIn this article, we will explore how code performance and energy efficiency intersect in the JVM ecosystem and why GraalVM stands out in this domain.\n\n### Understanding energy and performance in the JVM context\n\nTo grasp why energy efficiency matters in JVMs, we first need to understand what JVMs do. A JVM is the engine that powers Java applications, converting platform-independent Java bytecode into machine-specific instructions. While this flexibility is a major strength, it also means JVMs carry some overhead, especially compared to compiled languages like C++.\n\nNow, energy consumption in software isn't just about the hardware running hotter or consuming more electricity. It's tied to the performance of the software itself. When code is slow or inefficient, it takes longer to execute, which directly correlates with more CPU cycles, increased power draw, and greater energy usage. This connection between performance and energy efficiency is at the heart of what makes optimizing JVMs so critical.\n\nStudies like those by Leeds Beckett University (2023) and HAL Open Science (2021) reveal how JVM optimizations and configurations significantly impact energy use. As newer JVMs improve performance through better garbage collection, just-in-time (JIT) compilation, and other optimizations, they reduce not just runtime but also energy costs. Yet, even within these advancements, there’s a standout contender reshaping how we think about energy-efficient Java: GraalVM.\n\n![](/images/blog/energy-efficiency-of-jvm-and-the-role-of-graalvm/photo-1.png)\n\n### What makes GraalVM different?\n\nGraalVM is a high-performance runtime designed to improve the efficiency of applications written in Java and other JVM-compatible languages. Unlike traditional JVM implementations, GraalVM incorporates advanced optimization techniques that make it unique in both speed and energy usage. Its native image capability allows applications to be compiled ahead-of-time (AOT) into standalone executables.\n\nTraditional JVMs rely heavily on JIT compilation, which compiles bytecode into machine code at runtime. While this approach allows for adaptive optimizations (learning and optimizing as the program runs), it introduces a delay in startup time and consumes energy during execution. GraalVM’s AOT compilation eliminates this runtime overhead by pre-compiling the code, significantly reducing the startup time and resource consumption.\n\nFurthermore, GraalVM supports polyglot programming, which enables developers to mix languages like JavaScript, Python, and Ruby in a single application. This reduces the need for multiple runtime environments, simplifying deployment and cutting down on the energy costs associated with maintaining diverse infrastructures.\n\n### Energy efficiency in numbers\n\nThe question many might ask is: does GraalVM truly make a difference in energy terms? The combined studies offer some clarity. For example, Leeds Beckett University (2023) and HAL Open Science (2021) benchmarked GraalVM against traditional JVMs like OpenJDK, Amazon Corretto, and Azul Zulu, using diverse workloads. Both studies showed that GraalVM, particularly in its native image configuration, consumed less energy and completed tasks faster across the majority of scenarios.\n\nInterestingly, the energy consumption gains are not linear across all benchmarks. While GraalVM excelled in data-parallel tasks like **Alternating Least Squares (ALS)**, it underperformed in certain highly parallel tasks like **Avrora**. This suggests that the workload type significantly influences the runtime's energy efficiency.\n\nMoreover, the researchers observed that while newer JVMs like HotSpot 15 generally offered better energy performance than older versions like HotSpot 8, GraalVM consistently stood out. Even when compared to JVMs optimized for long-running tasks, GraalVM delivered lower energy costs due to its AOT compilation, which minimized runtime overhead.\n\n![](/images/blog/energy-efficiency-of-jvm-and-the-role-of-graalvm/photo-2.png)\n\n### Insights from JVM configuration studies\n\nBeyond runtime optimizations, how you configure a JVM can have profound effects on energy consumption. Both studies emphasized the role of garbage collection (GC) and JIT compiler settings. For instance, HAL Open Science found that default GC settings were energy-efficient in only half of the experiments. Alternative GC strategies, such as ParallelGC and SerialGC, sometimes outperformed default configurations like G1GC. Similarly, tweaking JIT compilation levels could improve energy efficiency, but such adjustments often required detailed performance evaluations.\n\nOne of the most striking observations was the variability in energy savings based on application characteristics. For data-heavy tasks like H2 database simulations, energy savings were most pronounced when using GraalVM’s default configurations. However, for highly concurrent applications like Reactors, specific configurations of JIT threads delivered significant improvements.\n\n### Carbon footprint reduction\n\nThe environmental implications of these energy savings are immense. Using standardized energy-to-carbon conversion factors, the studies highlighted that GraalVM reduced carbon dioxide emissions more effectively than traditional JVMs. These reductions were particularly significant in cloud environments, where optimizing runtime efficiency lowered operational costs and reduced the carbon footprint of large-scale deployments.\n\n![](/images/blog/energy-efficiency-of-jvm-and-the-role-of-graalvm/photo-3.png)\n\n### Broader implications for software development\n\nThe findings from Leeds Beckett University (2023) and HAL Open Science (2021) are clear: energy efficiency is no longer just about hardware; it’s about making smarter software choices. By adopting greener JVMs like GraalVM, developers can contribute directly to sustainability goals without compromising on performance.\n\nHowever, the road to greener software isn’t just about choosing a runtime. It involves understanding the nuances of workload types, runtime configurations, and application behaviors. Tools like [J-Referral](https://github.com/chakib-belgaid/jreferral), introduced in HAL Open Science’s study, can help developers select the most energy-efficient JVM configurations for their specific needs, simplifying the path to sustainable computing.\n\n### Conclusion\n\nThe correlation between code performance and energy efficiency is clear: faster, optimized software consumes less energy. JVMs have long been at the heart of this discussion, and while traditional JVMs continue to evolve, GraalVM offers a leap forward. By combining high performance, energy efficiency, and versatility, it stands out as a powerful tool for modern developers looking to build applications that are not only fast but also environmentally conscious.\n\nWith studies confirming its efficiency across a broad range of scenarios, GraalVM represents a shift in how we think about software sustainability. The journey to greener software begins with choices like these — choices that balance performance, cost, and environmental responsibility.\n\n### References\n\n1. Vergilio, TG and Do Ha, L and Kor, A-LG (2023) [Comparative Performance and Energy Efficiency Analysis of JVM Variants and GraalVM in Java Applications](https://core.ac.uk/download/590241573.pdf).\n2. Zakaria Ournani, Mohammed Chakib Belgaid, Romain Rouvoy, Pierre Rust, Joel Penhoat. [Evaluating the Impact of Java Virtual Machines on Energy Consumption](https://inria.hal.science/hal-03275286v1/).\n\n\n\n\n\n\n\n","metadata":{"title":"The Energy Efficiency of JVMs and the Role of GraalVM","excerpt":"Explore the correlation between energy efficiency and code performance in the JVM ecosystem, discovering why GraalVM stands out as a top-tier runtime for optimizing both performance and environmental impact.","date":"2025-01-29","author":"Graziano Casto"}},{"slug":"shape-your-platform-strategy-with-the-journey-map","content":"\n[Published by CNCF](https://www.cncf.io/blog/2025/01/09/platform-journey-map-shape-your-platform-strategy/)\n\n\n**People often focus on technology when discussing platforms**, debating which tool is better or which solution to choose. This can cause them to lose sight of the bigger picture, like wearing blinders. \n\nThink of it this way: you could build the most advanced kitchen with the best tools and appliances. But if you don’t involve the chefs or train them, what’s the point? What kind of food would come out? You don’t need a huge oven to make a sandwich. It’s the same with digital platforms. Tools alone won’t work without involving people and having a clear strategy.\n\n![](/images/blog/shape-your-platform-strategy-with-the-journey-map/photo-1.avif)\n\n\n### Platforms: From Automation to a Circular Economy of Software\n\nThe traditional narrative of platform engineering makes us think of platforms as closely tied to concepts like environment-as-a-service or infrastructure automation. This is true, but only if we ignore the fact that a platform initiative exists within a much larger context. \n\nOver the past few years, working with various organizations across different industries to adopt digital platforms, we’ve noticed a common pattern: **the approach to platform initiatives often follows a standard path**.\n\nMost organizations step into the world of platforms aiming to provide self-service tools and processes for provisioning infrastructure, enabling developers to manage it with greater autonomy. A typical example is building a platform to abstract the complexity of managing distributed environments on Kubernetes, allowing even less experienced developers to harness its potential through automation provided by the platform team.\n\nA trend we’ve observed in how these initiatives evolve is the desire to extend the concept of self-service. This shift focuses on other areas of software development, prioritizing developer experience by simplifying coding and deployment. As the initiative matures, the platform increasingly integrates into various aspects of the organization. It centralizes governance across the entire software development lifecycle, optimizes processes and supports a composable approach to building complex software systems.\n\nWhat happens is the creation, consciously or not, of a kind of [circular economy within the platform](https://tag-app-delivery.cncf.io/blog/proposal-platform-engineering-/). The work of some teams is made available for others to reuse, building something new that can, in turn, be shared again. This fuels a marketplace of composable elements made up of data, containers, and APIs.\n\n![](/images/blog/shape-your-platform-strategy-with-the-journey-map/photo-2.avif)\n\n\n### Platform Adoption: Aligning Strategy, Goals, and Metrics for Success \n\nAdopting a platform is a step-by-step journey, and for any organization embarking on this kind of initiative, it’s crucial to conduct a **comprehensive and realistic self-assessment**. This means painting a clear picture of its starting point, identifying the challenges engineers will face in their daily activities, recognizing bottlenecks that impact the business, and acknowledging the processes that work but could clearly be approached more efficiently and effectively. Based on these challenges, organizations can start defining the milestones of their journey – the goals they aim to achieve with their platform.\n\nSome organizations may first need to focus on modernizing their infrastructure and consolidating their software lifecycle before diving into creating paved paths and automations to enable environments as a service. Others, already advanced in technical excellence within their architecture and software lifecycle, might see the introduction of paved paths and automations as a foundational step leading to initiatives that address business-oriented metrics, such as reducing time-to-market. This could involve adopting a composable approach to application development or tailoring the platform to support specific market use cases.\n\nThe key point is that any initiative, platform-related or not, requires **clarity about the starting point and the goals to achieve**. This clarity allows organizations to chart the right course, supported by appropriate metrics, ensuring they stay on track. These metrics also serve as a universal language for discussions among the various stakeholders, who often come from diverse backgrounds and have different areas of focus.\n\nThis is the crux: objectives, strategy, and metrics must come first so that everyone, technical and non-technical alike, can be involved in the initiative from the outset. \n\nBusiness and technical teams need a clear idea of their goals for the platform before focusing on details. This includes deciding which tools to use, whether to build the first MVP with a vendor or in-house, and how to improve an existing platform.\n\n**A platform is not just a technical tool**. Adopting a platform is not merely about discussing whether to choose ArgoCD or Flux for deployment management. A platform is a product with a broader vision, designed to create value for everyone. It aims to make technical work easier while also considering business stakeholders, who must be involved from the very beginning. This ensures that the value boundaries are clearly defined, avoiding misunderstandings and reducing the risk of early failure in the initiative.\n\n![](/images/blog/shape-your-platform-strategy-with-the-journey-map/photo-3.avif)\n\n\n### The strategic tool to guide your Platform Adoption: the Platform Journey Map\n\nAt Mia-Platform, the adoption of digital platforms is our bread and butter. To better support the platform adoption journey, we’ve mapped many of the trends and patterns we’ve observed over the years. From this, we’ve developed a visual tool, almost like a board game, designed to **foster meaningful discussions around platform initiatives among the various stakeholders involved**.\n\nWe decided to call this tool the [Platform Journey Map](https://mia-platform.eu/library/platform-journey-map/) to emphasize its role as a guide in defining the priorities and strategy for your platform, a vision we had in mind when creating it.\n\nThe Platform Journey Map outlines all the steps, from zero to hero, that we have observed through years of field experience. **It analyzes how organizations across various industries adopt digital platforms and use them to develop their products**. The map has different tiles representing the goals companies aim to achieve with a platform. The paved journey is designed to foster meaningful discussion around the following topics:\n\n- **Software Lifecycle**: Infrastructure modernization, software delivery efficiency, runtime optimization.\n- **Modern Architecture and Data**: API platform, microservices transition, legacy modernization, integration patterns, evolutionary architectures, data products.\n- **Platform Engineering**: Infrastructure and DevOps orchestration, environments as a service, templates and paved roads, Platform as a Product, metrics and observability, and Team Topologies.\n- **Business Composability**: Packaged business capabilities, internal developer portal and software catalog, micro frontends, application composition, low-code and no-code governance.\n- **Market Use Case**: Omni-channel experiences, Software as a Service and platform business, Open-X and embedded services.\n\n![](/images/blog/shape-your-platform-strategy-with-the-journey-map/photo-4.png)\n\nThe tool’s purpose is to help organizations plot their own platform adoption journey. The first step is to identify your starting point. Are you at the tiles related to the software lifecycle, or have you already made progress and are positioned closer to architecture modernization? Once this is clear, you can start discussing which tiles you aim to select. Do you want to address business composability? If so, what needs to happen first? Does it make sense to restructure team organization following the principles of Team Topologies? The game’s design invites all stakeholders to participate, raising important questions and sparking meaningful conversations.\n\nOnce you’ve identified the tiles you want to select during your platform adoption campaign, you can dive deeper into strategies and metrics, identifying KPIs to define the adoption success. How should your Platform Team be structured? Which stream of the organization should you prioritize first? Which products or partners should be targeted in this initiative? Are your monitoring systems adequate as they are, or do you need to collect different metrics?\n\nBy playing this game with all platform stakeholders, you ensure that everyone is involved and committed to this effort from day one, reducing the risk of the platform initiative failing before it even starts, whether due to misalignment with the business or discrepancies in the objectives to be achieved.\n\n**Building the journey together ensures that everyone follows the agreed-upon path, avoiding unpleasant surprises along the way**.\n\n### Guidelines for Defining Your Platform Strategy\n\nBefore using this tool, it’s important to understand and follow its simple guidelines. \n\n- it’s a tool designed for **discussion and brainstorming**, meant to help you define the boundaries of your strategy. \n- **There’s no strict, linear path** where your pieces must follow every step in sequence. Instead, think of it as a map, where the focus is on identifying and prioritizing what truly matters for your organization.\n- **Some cards represent overlapping concepts**. For instance, governance might fall under both the API Platform and Templates and Paved Roads categories. This is perfectly fine, just make sure to explain the reasoning behind your choices and use this flexibility to support deeper discussions.\n\nApproach the tool with this mindset, and let it guide you in shaping a strategy tailored to your platform journey.\n\n### Now it’s your turn to use the Platform Journey Map\n\nWe spent countless times developing and enhancing this tool internally before **unveiling it publicly in October 2024 with a dedicated workshop at KCD UK**. The workshop was a great success, and an [interview](https://thenewstack.io/platform-engineering-a-workshop-to-help-map-your-strategy/) published on the topic reached a very huge audience in a very short time. \n\nDespite the excitement, this tool is still in its **early stages**. That’s why we encourage you to [download the tool and its rules](https://mia-platform.eu/library/platform-journey-map/), use it, have fun, and share your feedback to help us make it even better for the community.\n\nHave you encountered different steps in your platform adoption journey? Leave us your feedback at [this email](mailto:platformjourneymap@mia-platform.eu) (platformjourneymap@mia-platform.eu) and share your story! We’re excited to hear from you and integrate your experiences into the tool.\n\n\n\n\n","metadata":{"title":"Platform Journey Map: Shape Your Platform Strategy","excerpt":"Discover the Platform Journey Map, a visual game designed to spark meaningful discussions about platform strategy and priorities, moving beyond technology debates to focus on people and clear strategic direction.","date":"2025-01-09","author":"Graziano Casto"}},{"slug":"finding-observability-and-devex-traquility-with-platform-engineering","content":"\n[Published by DZONE co-authored with Eric D. Schabell (Director Evengelism @ Chronosphere)](https://dzone.com/articles/o11y-guide-find-observability-and-devex-tranquility)\n\n\nMonitoring system behavior is essential for ensuring long-term effectiveness. However, managing an end-to-end observability stack can feel like sailing stormy seas — without a clear plan, you risk blowing off course into system complexities. \n\nBy integrating observability as a first-class citizen within your platform engineering practices, you can simplify this challenge and stay on track in the ever-evolving cloud-native landscape.  \n\nEntering the world of monitoring distributed systems is a journey made up of several stages which we will cover in the rest of this article. Let's start at the beginning, where organizations attempt to navigate the observability seas and discover the complexities involved.\n\n### In the beginning\n\nInitially, attempts at a cohesive platform usually start with a basic monitoring strategy that simply tells you when something isn’t working. Over time the system evolves to gather more detailed insights, trying to answer the why of what went wrong. The ultimate goal is to become proactive, collecting enough data to intervene before a problem occurs. \n\nPrevention is always better than the cure.\n\n### Navigating the platform complexity\n\nAs the system matures, it allows us to make our applications more resilient, but it also becomes more complex. We can break down this complexity into three main areas. \n\nThe first area is adding more tools to the stack, increasing the difficulty of managing them. This struggle is well known by platform engineering teams and a constant pain for the developer teams trying to keep up with this escalating volume of tools.\n\nThe second is the volume of telemetry data, growing exponentially so that it's easy to find ourselves struggling to stay afloat. This [problem is well documented](https://chronosphere.io/learn/observability-data-monolithic-applications/) in how it's not readily apparent in our monolithic application architectures but quickly raises its head in a cloud-native application architecture. \n\nLastly are the people, how they interact with monitoring tools where the challenge lies in ensuring the system delivers relevant information without overwhelming. As almost everyone in the organization has some level of interest in the insights provided by monitoring systems, we'll have to make sure we are tailoring uncovered insights to these users' specific needs.\n\n### An IDP observability journey\n\nUsing an Internal Developer Platform (IDP) as a guide during the journey into observability helps address the above challenges while mitigating issues along the way. An IDP enables the creation of clearly charted routes for developers — whether in the form of templates, containers, or APIs — that simplify the management complexity of observability tools. \n\nFor example, there can be clearly defined configurations for certain tools ensuring they work seamlessly for every developer. For a developer using the platform, it shouldn't matter which monitoring tool is being used as their primary focus is building applications and services. Everything else is abstracted away through the charted routes provided by the IDP. Should at any time in the future the monitoring tools change, the goal is a transparent transition from the developer's perspective.\n\nCentrally managing data on the platform allows for efficient organization and simplifies the visualization of connections between data from various components of a distributed architecture. This enables a paradigm shift, moving from passively collecting monitoring data in the hope that it may one day prove useful, to a more purpose-driven approach. \n\nAnalyzing data flows that govern the architectures being monitored, identifies specific data needed for effective insights. This minimizes the collection of unnecessary data while maximizing the actionable insights that can be generated.\n\nLastly, the IDP serves as a crucial center for governance and centralization, especially when it comes to data visualization. It allows for the configuration of a single location where observability data can be accessed, eliminating the friction that arises when having to switch between different tools. This unified approach streamlines the user experience and makes it easier to access and act on valuable insights.\n\n### Finding tranquility\n\nHow great would it be to work in an organization, as a platform engineer or a developer, where teams started projects with observability as a top priority? \n\nThey would dedicate time and resources to creating a comprehensive telemetry strategy from the outset.\nThey would prioritize observability just as they would prioritize testing, continuous integration, and continuous deployment from day one.\nThe logical starting point to achieve this is to focus on open standards and open protocols for your observability solutions. Using Cloud Native Computing Foundation (CNCF) projects to explore your options ensures that your eventual architecture is using standard components.\n\n[Prometheus](https://prometheus.io/) is a well-known monitoring system and time series database that powers your metrics and alerts with the leading open-source monitoring solution. [OpenTelemetry](https://opentelemetry.io/) provides high-quality, ubiquitous, and portable telemetry to enable effective observability. [Fluent Bit](https://fluentbit.io/) provides you with an end-to-end observability pipeline, with a super fast, lightweight, and highly scalable logging, metrics, and traces processor and forwarder. [Perses](https://perses.dev/) is the new kid on the block, providing an open specification for dashboards focused on Prometheus and other data sources.\n\nThis hands-on, free, self-paced [observability workshop collection](https://o11y-workshops.gitlab.io/) takes you through all of the above tooling.  \n\nStart leveraging the synergies between observability and platform engineering today, helping your developers create better cloud-native applications while simultaneously enhancing their experience working on your platform.","metadata":{"title":"Finding Observability and DevEx Tranquility with Platform Engineering","excerpt":"Learn how to navigate the stormy seas of monitoring data by integrating observability as a first-class citizen in platform engineering, avoiding complexity while providing meaningful insights.","date":"2025-01-07","author":"Graziano Casto"}},{"slug":"2024-wrapped-my-year-as-devrel-buddy","content":"\n\nAs December comes to a close, it’s time to reflect on the past year. Over a year and a half ago, I joined Mia-Platform, but it was only with the arrival of 2024 that I fully embraced my role as a DevRel buddy, working closely with my colleague Michel Murabito.\n\n### Sharing Knowledge: A Community-Driven Journey\n\nOne of the highlights of this year was sharing knowledge with the community. I had the privilege of participating as a speaker in 13 events, including online and in-person conferences across Italy, Sweden, the Netherlands, England, and even California. Along the way, I met incredible individuals who left a lasting impression on me—people like Eric D. Schabell, with whom I shared the stage in Amsterdam, and Fernando Harris, who hosted me twice in his community in Malaga.\n\nI also had the honor of being a guest on one of my favorite podcasts, Gitbar Podcast, joining Mauro Murru, Michel Murabito, and Luca Rainone for a memorable conversation.\n\nAdditionally, I ventured into workshops this year, a new and exciting format for me. This experience was capped by an interview with the remarkable Jennifer Riggins, a writer whose insights and kindness make her a standout in our field.\n\n### Creating Content: Articles and More\n\nThis year wasn’t just about public speaking. I managed to publish several articles, including a collaborative piece with Davide Bianchi for InfoQ’s blog and a series of posts on my personal blog (you can find the link in the first comment!). Writing has been an excellent outlet to dive deeper into topics I’m passionate about and to share those insights with a wider audience.\n\n### Empowering Through Training\n\nInternally, my efforts extended to creating training courses and certification programs focused on the technologies we work with at Mia-Platform. These sessions were attended by over 150 participants throughout the year, underscoring the importance of continuous learning and skill-building within our community.\n\n### Looking Ahead: #MiaPlatformExperts\n\nOne of the year’s most exciting achievements was laying the groundwork for the #MiaPlatformExperts community. Behind the scenes, I contributed to making our hackathons and monthly events truly memorable for our Experts. Next year promises even more exciting initiatives, so stay tuned!\n\n### Reflections and Goals\n\nReflecting on this year, I’m proud of the strides I’ve made in my first full year as a DevRel professional. Looking ahead to 2025, I’ve set two key goals:\n\n- Consistency: Continuing to produce high-quality content, from talks and articles to workshops and training courses.\n- Participation: Becoming more deeply involved in the Cloud Native and Open Source communities, engaging with CNCF working groups, and pushing myself to collaborate more, while keeping imposter syndrome in check.\nHere’s to another year of growth, collaboration, and new opportunities.\n","metadata":{"title":"2024 Wrapped: My Year as a DevRel Buddy","excerpt":"A comprehensive review of my first year as a Developer Relations Engineer, highlighting community engagement, speaking opportunities, and the journey of sharing knowledge across international tech events.","date":"2024-12-30","author":"Graziano Casto"}},{"slug":"a-workshop-to-help-map-your-strategy","content":"\n[Interview published by Jennifer Riggins on The New Stack](https://thenewstack.io/platform-engineering-a-workshop-to-help-map-your-strategy/)\n\n\nGraziano Casto, developer relations engineer at Mia-Platform, offers his own definition of platform engineering: “Platforms are a place where people share value.”\n\nHe urges us to think about the strategy behind the creation of an internal developer platform (IDP). “Because when we talk about platform engineering with developers or technical people, they are thinking of technological stuff,” he told The New Stack at Kubernetes Community Days UK in London in October. “We wanted to bring light on the strategic path a company must take to build their own IDP.”\n\nAs organizations rush to adopt an IDP strategy, they often focus on the what — shared libraries, rules, infrastructure, compliance, security — but don’t always stop to answer the why. If a platform engineering team wants to create a platform that serves developers in the long run, it must bring together stakeholders from around the organization.\n\nMia-Platform created its Platform Journey Map workshop to help teams uncover a common language to discuss the objectives of platform engineering. The workshop brings together product, technology and people, and creates something to revisit as a team’s internal developer platform matures.\n\n### What Is an Internal Developer Platform?\n\nA lot of organizations get stuck on thinking of platform engineering as using a platform to tell developers what to do. In reality, it’s quite the opposite. You have to build something developers want to use.\n\n“When you think of platform right now, you think of something that rules DevOps and infrastructure,” Casto said. But in his experience with customers, “companies approach platform engineering by trying to give developers a tool for self-service capabilities.”\n\nAs you head down the platform rabbit hole, however, you always want to integrate more and more. That could be infrastructure — which he said is usually the easiest thing to integrate first. Then, different stakeholders want to extend the IDP to integrate data, including data pipelines, catalogs and governance, which then demands security and compliance.\n\nSo far, this definition could be the old-school description of platforms that have existed since the beginning of the tech industry. What differentiates an internal developer platform from those traditional top-down platforms, according to Casto, is when an organization reaches a technical level of maturity and wants to use the platform as an enabler for composability.\n\n“They are building over the years something that they want to isolate and could reuse to build something else and accelerate the time to market, to bring new products,” he said. That’s what drove Mia-Platform to create the Platform Journey Map workshop as a way to kick off its work with customers.\n\n### How to Build Your Technical Path\n\nThe key to unlocking your platform engineering strategy isn’t the what, but rather the how and why.\n\n“The game we created is a brainstorming tool to align tech people with business,” Casto said, to get them discussing “the best strategies to integrate technology and accomplish the business strategic vision.”\n\nDespite its look, the Platform Journey Map isn’t necessarily played like a traditional board game. It’s more of a concept map, Casto said, “designed primarily as a tool for discussion and comparison, helping to define the edges of a strategy.”\n\nBut if you read the board clockwise, it flows in the direction of platform maturity, fostering discussion around the following topics:\n\n- Infrastructure modernization, software delivery efficiency, runtime optimization.\n- API platform, microservices transition, legacy modernization, integration patterns, evolutionary architectures, data products.\n- Infrastructure and DevOps orchestration, environments as a service, templates and paved roads, Platform as a Product, metrics and observability, and Team Topologies.\n- Packaged business capabilities, internal developer portal and software catalog, micro frontends, application composition, low-code and no-code governance.\n- Omni-channel experiences, Software as a Service and platform business, Open-X and embedded services.\n\nNot all organizations will follow this journey all the way through to exposing platform features externally. But most will likely have already begun with the first steps of building in the cloud and applying governance to their software development life cycle.\n\nAnd, as it is the preferred way for developers to consume an internal developer platform, an API-based interface should also be an early part of a platform roadmap. Governance should also be considered early and often.\n\nInterestingly, this platform journey game doesn’t address true platform engineering until about a third of the way around the board. Casto argued that you can’t really embrace platform engineering until governance and software life cycles are defined. Only then can you think about how you are offering these business and technical capabilities in a discoverable and composable way.\n\nIn the end, as you explore the gameboard, he emphasized that there is no linear path where your pieces need to follow every step: “Instead, think of it as a map, where you focus only on highlighting what you believe are the true priorities.”\n\nThese, of course, will change over time.\n\n### Platform Engineering Strategies: Real-World Examples\n\nOne of the challenges of platform engineering is that, as each technical organization is different, so will be its platform strategy. That’s why the workshop brings out different discussions from each group of participants.\n\nMia-Platform’s workshop has attendees working with real-world examples, each with:\n\n- A company identikit.\n- The goal the company aims to achieve.\n- The company’s current technological landscape.\n\nEach participant’s role is in the workshop is to:\n\n1. Identify their priorities on the Platform Journey Map. (15 minutes.)\n2. Define an adoption strategy based on those priorities. (15 minutes.)\n3. Determine the KPIs that will measure success. (10 minutes.)\n4. Share their high-level strategy pitch. (4 minutes.)\n\nOne example identikit was a retail company that had a small engineering team supporting online and brick-and-mortar partners, with touchpoints in the American and European markets. Each partner independently chooses its tech stack, which leads to inconsistent and poor code quality. The company is also worried about vendor lock-in.\n\nThe established end goal for the platform was to leverage APIs for a standardized user experience across all partners, with a self-service, discoverable interface that reduces burden on the small IT team.\n\n### Role-Playing Stakeholders\n\nWhen you’re looking to create a platform engineering strategy, you likely already know your company’s purpose, goals and tech stack — but writing it down is a good way to kick off your own workshop to clarify for everyone involved.\n\nDepending on the size of your workshop, break into smaller groups, while the workshop facilitators act as stakeholders, including developers and C-level executives. Then use this board game as a brainstorming tool to identify priorities. These may include things like:\n\n- As a starting point, participants have to build an API platform to add governance over the existing API layer.\n- In order to build self-service capabilities, group members need to add infrastructure and DevOps automation, and to consider templates and “golden paths” for developers.\n- What would the participants’ Platform as a Product strategy look like?\nDifferent teams participating in the workshop are encouraged to challenge each other.\n\n“You have to explain to me your strategy,” Casto said. Imagine, “I’m a technical [person], so you can talk to me about Crossplane, but I don’t know if my CEO will understand this.”\n\nThe workshop attendees are encouraged to collaborate to create a strategy before they put a platform owner in charge of building the platform team to execute it. Within an organization, the workshop ensures that everyone knows who has which capabilities to help identify the best candidates for a platform engineer. That role has to bridge technical and business goals.\n\nThis platform journey should consider constraints like budget and time to deliver — the largest factors in the build versus buy debate.\n\n“Say you want to accomplish this in six months, so you have to understand which is the better solution,” Casto said. “Then the [teams] make a strategy to understand which are the priorities and define the power roles.”\n\nComing back to the retail example, one partner is a greenfield partner, while others are brownfield partners — the group must decide who to integrate with first.\n\n“The last thing that is, for me, the most important part is to understand how to measure the success of your adoption,” Casto said, as measuring platform engineering is a common struggle, yet you cannot improve what you can’t measure.\n\n“It’s important to measure the value you bring to the organization and how you can use this data to spot future opportunities to increase the value of the platforms.”\n\nFor each priority, workshop attendees are tasked with choosing KPIs and pitching their use cases and objectives to the other stakeholders.\n\n### A Game To Play Again and Again\n\nAn internal developer platform is just that, a platform that you continue to build and iterate on top of. Similarly, organizations should run the Platform Journey Map workshop repeatedly, as stakeholder needs will change.\n\n“We have recurring events with customers where we use this tool with them to understand how the adoption is going and where we have opportunities to bring their platform to the next level,” Casto said.\n\nAdapt the workshop to your organization and feel free to skip cells the first go-round. On average, Casto said, he finds the first lap around the map — or the first main delivery of your platform roadmap — takes six to eight months.\n\n“The cells are not strictly defined,” he emphasized. “It’s just a tool to facilitate a discussion with customers, with peers, with solutions architects.”\n\nThe Platform Journey Map, Casto said, is meant to complement, not replace, the Cloud Native Computing Foundation’s Platform Engineering Maturity Model. The CNCF model, Casto said, is meant to challenge the platform team, while the map exercise is meant to align all stakeholders.\n\nThe game was developed about a year ago because Mia-Platform’s solution architect was finding it challenging to explain the adoption journey of an internal developer platform to customers.\n\n“It’s much easier to explain something visual,” Casto said. As the Platform Journey Map gamifies the IDP strategy discussion, it facilitates brainstorming in order to “guide the path for you or your customer without forcing them to say something they don’t want to say.”","metadata":{"title":"Platform Engineering: A Workshop to Help Map Your Strategy","excerpt":"Discover how Mia-Platform's game-like workshop helps organizations define their internal developer platform strategy by bringing together stakeholders to answer the crucial 'why' behind platform engineering initiatives.","date":"2024-11-21","author":"Graziano Casto"}},{"slug":"greenops-and-finops","content":"\n[Published by CNCF](https://www.cncf.io/blog/2024/07/25/greenops-and-finops-the-perfect-pitch-toward-business-and-environmental-goals/)\n\nThe information and communication technology sector alone contributes around [1.4% of global emissions](https://ericsson.com/en/reports-and-papers/industrylab/reports/a-quick-guide-to-your-digital-carbon-footprint), making sustainability a critical issue in today’s tech industry. [Gartner](https://www.gartner.com/en/newsroom/press-releases/2023-01-31-gartner-predicts-70-percent-of-technology-sourcing-leaders-will-have-environmental-sustainability-aligned-performance-objectives-by-2026) predicts that within the next two years, 75% of organizations will replace vendors lacking sustainability goals and timelines. The [UN’s Sustainable Development Goals](https://sdgcompass.org/) further push organizations to create sustainable products and services by 2030. With IT waste for the average enterprise reaching up to 45%, the importance of sustainable digital solutions in optimizing profits and protecting the environment is clear.\n\nIn this article, we will explore the connections between GreenOps and FinOps, showing how integrating these methodologies can help organizations achieve business goals while managing their environmental impact.\n\n### A brief introduction to GreenOps\n\nGreenOps represents a holistic approach that combines business practices and technological innovations to improve cloud efficiency while reducing environmental impact. This model encompasses all efforts an organization undertakes to lower the environmental footprint of their cloud operations. It involves formulating strategies that enhance sustainability and align with business goals, such as reducing waste, transitioning to renewable energy sources, and promoting a culture of environmental responsibility.\n\nWithin the GreenOps operational model, organizations may adopt a [spatial-shift](https://learn.greensoftware.foundation/carbon-awareness/#spatial-shifting) strategy to relocate resources to regions that leverage renewable energy, thus cutting down their carbon emissions. Practices such as deactivating resources during non-peak hours can conserve energy and reduce operational expenses. Additionally, creating energy-efficient frameworks for workloads ensures optimal use of resources, maximizing performance while minimizing waste.\nFurthermore, GreenOps advocates for the continuous monitoring and optimization of cloud resources to discover additional improvement opportunities. This could include adopting more energy-efficient hardware, optimizing software to lower energy consumption, and utilizing advanced analytics to forecast and manage energy usage more efficiently.\n\nGreenOps also emphasizes integrating sustainability goals into the company’s core values and daily operations. This involves educating employees on best practices for sustainable cloud usage, fostering innovation in green technologies, and regularly evaluating and reporting on the company’s environmental impact.\n\nBy implementing GreenOps, organizations can achieve a dual advantage: driving business growth and profitability while contributing to environmental conservation. This approach helps meet regulatory requirements and customer expectations for sustainability, and positions the company as a leader in corporate social responsibility.\n\n### FinOps in a nutshell\n\nFinOps represents a strategic shift that integrates technology, finance, and business practices to enhance efficiency and optimize the financial performance of cloud computing infrastructure.\n\nGiven that cloud computing constitutes a significant operational expense for many companies, a growing number of businesses are adopting FinOps. By promoting shared responsibility for cloud costs, companies can gain a comprehensive understanding of their cloud expenditure, enabling them to optimize these costs, maximize profits, and enhance their competitive edge.\n\nThe [FinOps Foundation](https://www.finops.org/introduction/what-is-finops/) describes the concept as an evolving cloud financial management discipline and cultural practice that enables organizations to achieve maximum business value by helping engineering, finance, technology, and business teams collaborate on data-driven spending decisions.\n\nIn essence, FinOps refers to a discipline and cultural mindset that encourages collaboration between different departments within an organization to optimize IT spending. The primary teams involved in FinOps are the Finance department and DevOps teams. This cross-functional dialogue aims to maximize the value derived from cloud services by balancing speed, cost, and quality, while carefully considering the impact of cloud expenditures on the overall results. In FinOps, every decision regarding cloud spending is evaluated through a data-driven decision-making process, focusing on optimization, cost forecasting, and cost allocation.\nBy adopting FinOps, organizations not only streamline their cloud financial management but also cultivate a culture of transparency and collaboration, leading to better financial outcomes and enhanced business value.\n\n### What's the synergy between GreenOps and FinOps\n\nAlthough FinOps and GreenOps are distinct approaches, they work together seamlessly to help companies optimize their cloud usage and costs while also reducing their environmental footprint. By integrating these two strategies, businesses can achieve both financial efficiency and sustainability. FinOps is a key component of GreenOps. By ensuring optimal use of cloud resources, companies not only save costs but also reduce energy consumption and carbon emissions, making it a win-win situation. This synergy makes FinOps an essential framework for companies aiming to enhance their sustainability efforts.\n\nWhile FinOps focuses on monitoring IT spending and cloud-related expenses, GreenOps tracks the carbon footprint of the infrastructure, whether it’s entirely cloud-based or a hybrid model. Despite their different goals, the two methodologies are interconnected, especially in full cloud environments, with a shared aim of optimizing cloud resources through proper dimensioning.\n\nFor instance, a more powerful virtual machine instance not only costs more but also demands more power, resulting in higher emissions. This correlation highlights the need to integrate FinOps and GreenOps to balance costs, emissions, and power requirements effectively.\n\nFrom a high-level perspective, GreenOps and FinOps appear quite similar, sharing the ultimate goal of efficient cloud usage. Achieving maximum efficiency leads to two main outcomes: reduced costs and lower carbon emissions. The same FinOps practices, such as right-sizing, storage tiering, removing idle and unattached resources, and scheduling compute off times, are also applied in GreenOps to reduce carbon emissions. In his closing remarks at AWS re:Invent, [Werner Vogels](https://www.forrester.com/blogs/aws-silent-nod-to-finops/) highlighted this connection by stating that cost is a close proxy for sustainability, affirming the tightly integrated relationship between FinOps and GreenOps.\n\nBy understanding and implementing both FinOps and GreenOps, organizations can better manage their cloud resources, cutting costs while also minimizing their environmental impact. This dual approach not only meets regulatory and customer expectations for sustainability but also positions companies as leaders in corporate social responsibility.\n\n### Put the theory into practice: a real-world scenario\n\nImagine a scenario where all environments typically used in a software delivery pipeline —development, staging, and production—operate with the same number of pods on an identical infrastructure, each with overlapping computational demands. By keeping development and staging environments active outside working hours, we end up using unnecessary resources for 128 out of the 168 hours in a week. This translates to roughly 103% more usage than necessary. In practical terms, engineers use these environments only about 23% of the time, highlighting the significant resource wastage in many engineering companies.\n\nFortunately, addressing this inefficiency is straightforward. With a few configuration lines and the installation of a Kubernetes operator called [kube‑green](https://kube-green.dev/), we can drastically reduce resource wastage. Kube-green is an add-on for Kubernetes that automates the shutdown of unnecessary resources and restarts them as needed, using a simple configuration known as CRD SleepInfo.\n\nConsider a cluster managing 75 namespaces, where we aim to suspend 48 namespaces outside working hours (our development and staging environments) while keeping deployments named api-gateway active. Without kube-green, this cluster runs about 1050 pods, with 75 GB of memory and 45 CPUs allocated, and consumes 45 GB of memory and 4.5 CPUs. This usage results in approximately 222 kg of CO2 emissions per week.\nBy configuring SleepInfo with kube-green, we can reduce the total pod count by 600, bringing it down to 450. Allocated resources drop to 30 GB of memory and 15 CPUs. Actual resource consumption decreases to 21 GB of memory and 1 CPU, lowering emissions to about 139 kg of CO2 per week.\n\nUsing kube-green on this cluster for a week reduces CO2 emissions by 83 kg, a 38% decrease compared to normal operations. Over a year, this translates to a reduction of approximately 4000 kg of CO2. This not only benefits the environment but also results in significant cost savings. Most cloud providers charge based on the number of nodes used, so reducing node count during off-hours lowers operational costs.\n\n|                       | without kube-green | with kube-green | difference |\n| :-------------------- | :----------------- | :-------------: | ---------: |\n| CPU consumed [cpu]    | 4.5                |        1        |        3.5 |\n| CPU allocated [cpu]   | 40                 |       15        |         25 |\n| CO2eq/week [kg]       | 222                |       139       |         83 |\n\nImplementing kube-green offers a dual advantage: environmental sustainability and financial savings. By optimizing resource usage and reducing emissions, companies can achieve a more efficient and cost-effective cloud infrastructure.\n\n### Wrapping up\n\nAdopting FinOps practices alongside GreenOps allows us to use cost as a driver for GreenOps implementation, aligning business benefits with environmental advantages. With tools like kube-green, we can integrate our GreenOps strategy seamlessly into our existing workload management, achieving positive impacts on both infrastructure costs and sustainability.\n\nGreenOps and FinOps are closely intertwined concepts. Often, when we optimize resources from a sustainability standpoint, we also realize savings on our cloud expenses. To make well-informed decisions, it’s crucial to have visibility into both the costs of our clusters and their CO2eq emissions. Many cost monitoring tools, like [OpenCost](https://www.opencost.io/), now include carbon cost emissions tracking in addition to tracking cloud spending.\n\nBy leveraging FinOps and GreenOps together, we can enhance our operational efficiency and reduce our environmental footprint without significant changes to how we manage workloads. Tools such as [kube-green](https://kube-green.dev/) enable automatic scaling down of resources during off-peak hours, ensuring we only use what is necessary. This not only minimizes energy consumption and emissions but also translates into substantial cost savings, as most cloud service providers charge based on resource usage.\n\nHaving insight into both financial and environmental metrics empowers organizations to make data-driven decisions that benefit both the bottom line and the planet. Monitoring tools that provide comprehensive views of both cost and carbon emissions help businesses balance efficiency with sustainability, reinforcing the symbiotic relationship between FinOps and GreenOps.\n\nIn essence, combining FinOps with GreenOps helps us create a more sustainable and cost-effective cloud infrastructure, driving business success while promoting environmental responsibility.\n\n","metadata":{"title":"GreenOps and FinOps: The Perfect Pitch Toward Business and Environmental Goals","excerpt":"Learn how integrating GreenOps and FinOps methodologies helps organizations achieve both business objectives and environmental sustainability goals while optimizing IT costs and reducing carbon footprint.","date":"2024-07-25","author":"Graziano Casto"}},{"slug":"greenops-kube-green","content":"\n[Published by InfoQ](https://www.infoq.com/articles/greenops-operational-efficiency/)\n\nParaphrasing the words of a wise Uncle Ben (from the 2002 movie Spiderman): \"With great infrastructures come great responsibilities\" and these responsibilities are not only towards the end users of our systems but also towards the environment surrounding us.\n\nThe information and communication technology sector alone produces around [1.4% of overall global emissions](https://www.ericsson.com/en/reports-and-papers/industrylab/reports/a-quick-guide-to-your-digital-carbon-footprint). This equates to an estimated [1.6 billion tons of greenhouse gas emissions](https://www.bbc.com/future/article/20200305-why-your-internet-habits-are-not-as-clean-as-you-think#:~:text=If%20we%20were%20to%20rather,of%20carbon%20dioxide%20a%20year.), making each one of us internet users responsible for about [400kg of carbon dioxide a year](https://www.bbc.com/future/article/20200305-why-your-internet-habits-are-not-as-clean-as-you-think#:~:text=If%20we%20were%20to%20rather,of%20carbon%20dioxide%20a%20year.).\n\n### Understand the environmental impact of our systems\n\nCreating environmentally friendly software involves using data to design programs that produce as little carbon emissions as possible. This requires knowledge from different fields like climate science, computer programming, and how electricity works.\n\nWhen we talk about \"carbon,\" we mean all the stuff that contributes to global warming. To measure this, we use terms like CO2eq, which tells us how much carbon dioxide is released and carbon intensity measures how much carbon is released for each unit of electricity used.\n\nTo figure out how much carbon a software program produces, we need to gather data on how much energy it uses, how dirty that energy is, and what kind of computer it runs on. This can be tricky, even for big companies tracking their own software usage.\n\nTo make software greener, we can focus on three things: making it use less energy, raising awareness about carbon emissions, and using more efficient hardware.\n\nTo reduce carbon emissions without reducing the actual energy usage of our infrastructure, accessing data on the carbon intensity of data centers is necessary. This allows us to set up a [spatial](https://learn.greensoftware.foundation/carbon-awareness/#spatial-shifting) or [temporal](https://learn.greensoftware.foundation/carbon-awareness/#temporal-shifting) shifting strategy, thereby prioritizing the use of renewable energy sources where available.\n\nSince putting these strategies into action involves optimizing the workloads that use our infrastructure, and considering that upgrading to more efficient hardware can be expensive and challenging, the easiest and fastest way to enhance our environmental impact is by reducing energy consumption. But how can we achieve this without compromising the availability of our systems?\n\n### Zombies are real, and they are just around the corner\n\nIn simplest terms, the software lifecycle typically involves at least 4 phases: a design phase where the system is conceptualized and experiments may be conducted, a development phase where all features to be implemented are built, and a release phase to put the developed software into production, but not before testing everything in a secure environment.\n\nTo implement a workflow as described, we'll need at least three different environments: the first being a development environment that will be useful for both the design and implementation phases, a staging or pre-production environment to test our software before delivering it to end users, and finally the production environment.\n\n![](/blog//green-ops-kube-green/green-ops-kube-green-1.webp)\n\nThese three environments have different availability requirements: the development and pre-production environments will be useful to engineers only during working hours, while the production environment needs to be constantly up and running. Additionally, transitioning from one environment to another often leaves behind remnants that linger in our infrastructure like zombies, consuming resources without serving a useful purpose in the overall software lifecycle.\n\n### Kill them all and save the planet\n\nSuppose we hypothetically set an ideal case where all environments have the same number of pods running on the same infrastructure with a consistent and overlapping computational demand. We can assert that by keeping the development and staging environments constantly active even outside working hours, we're using more resources than necessary for 128 out of the total 168 hours in a week, which equates to approximately 103% more usage than what is actually required. In other words, by keeping the development and staging environments always active, engineers would effectively utilize them for only about 23% of their actual consumption.\n\nWith this simple example, it is evident how significant the amount of resources that engineering companies waste every day is. Fortunately, remedying this requires nothing more than a handful of configuration lines and the installation of a Kubernetes operator on our clusters. [kube-green](https://github.com/kube-green/kube-green) is an add-on for Kubernetes that, once installed, allows you to automatically shut down Kubernetes resources when they are not needed and restart them when they are, all in an automated manner simply by configuring the CRD SleepInfo.\n\n```yaml\napiVersion: kube-green.com/v1alpha1\nkind: SleepInfo\nmetadata:\n\tname: working-hours\nspec:\n\tweekdays: \"1-5\"\n\tsleepAt: \"20:00\"\n\twakeUpAt: \"08:00\"\n\ttimeZone: \"Europe/Rome\"\n\tsuspendCronJobs: true\n\texcludeRef:\n          - apiVersion: \"apps/v1\"\n            kind:       Deployment\n            name:       my-deployment\n```\n\nWith just a few lines of configuration like those in the previous image, you can define that all namespaces on the cluster with Kube-green are available only from Monday to Friday and only during the hours from 8 AM to 8 PM in the Rome timezone.\n\nWith the current Kube-green version, both Deployments and CronJobs can be managed, but with the [next version](https://github.com/kube-green/kube-green/releases/tag/v0.6.0-rc.1) all the Kubernetes resources will be supported.\n\nIn the case of Deployments, to stop the resource, the ReplicaSet is set to zero and then reset to its previous value to restart it. Conversely, to suspend CronJobs, they are marked as suspended.\n\nIn the SleepInfo configurations, you can also specify resources that should not be suspended, such as the \"my-deployment\" in the example configuration provided above.\n\n### Data not just words: a real-world kube-green usage report\n\nThe following configuration of kube-green was used to suspend 48 out of 75 namespaces outside of working hours while keeping all deployments with the name api-gateway active.\n\n```yaml\napiVersion: kube-green.com/v1alpha1\nkind: SleepInfo\nmetadata:\n\tname: working-hours\nspec: \n\tweekdays: \"1-5\"\n\tsleepAt: \"20:00\"\n\twakeUpAt: \"08:00\"\n\ttimeZone: \"Europe/Rome\"\n\tsuspendCronJobs: true\n\texcludeRef:\n         - apiVersion: \"apps/v1\"\n           kind:       Deployment\n           name:       api-gateway\n```\n\nA cluster of this size, operating without kube-green, manages an average of 1050 pods with an allocated amount of resources equal to 75 GB of memory and 45 CPUs. In terms of actual resource consumption, the cluster records usage of 45 GB of memory and 4.5 CPUs, emitting approximately 222 kg of CO2 into the environment per week.\n\nBy configuring SleepInfo on this cluster, we can reduce the total number of pods by 600, bringing it down to 450. The allocated memory decreases to 30 GB, and the CPUs decrease to only 15. With kube-green, there is also a decrease in terms of resource consumption, with memory usage reduced to 21 GB and CPUs reduced to 1, resulting in emissions of approximately 139 kg of CO2.\n\n|                       | without kube-green | with kube-green | difference |\n| :-------------------- | :----------------- | :-------------: | ---------: |\n| number of pods        | 1050               |       450       |        600 |\n| memory consumed [Gb]  | 54                 |       21        |         33 |\n| CPU consumed [cpu]    | 4.5                |        1        |        3.5 |\n| memory allocated [Gb] | 75                 |       30        |         45 |\n| CPU allocated [cpu]   | 40                 |       15        |         25 |\n| CO2eq/week [kg]       | 222                |       139       |         83 |\n\nThe use of kube-green on this cluster during the study week resulted in emitting approximately 83 kg of CO2eq into the environment, which is 38% less than what we would have emitted under normal circumstances. These numbers become even more impressive when projected over the entire year, resulting in approximately 4000 kg less CO2eq emitted thanks to the simple use of kube-green.\n\nTo put it into perspective, this reduction is [equivalent to the carbon emissions from an average car](https://www.epa.gov/greenvehicles/greenhouse-gas-emissions-typical-passenger-vehicle) traveling approximately 16,000 km.\n\n![](/blog//green-ops-kube-green/green-ops-kube-green-2.webp)\n\n![](/blog//green-ops-kube-green/green-ops-kube-green-3.webp)\n\n![](/blog//green-ops-kube-green/green-ops-kube-green-4.webp)\n\n![](/blog//green-ops-kube-green/green-ops-kube-green-5.webp)\n\nIn this specific use case, as evidenced by the snapshots of resource usage metrics, the use of kube-green has allowed for a significant decrease in resource utilization, enabling the downscaling of the number of nodes in the cluster. On average, 4 nodes are stopped during the night and on weekends, each of which has a size of 2 CPU cores and 8 GB of memory.\n\nThis brings several benefits, not only reducing CO2 emissions but also directly proportional cost savings for our cluster. This is because the pricing structure of clusters in major cloud providers is typically based on the number of nodes used. By scaling down the number of nodes when they are not needed, we effectively reduce the cost of operating the cluster.\n\n### The future of kube-green\n\nThe kube-green project is constantly evolving, and as mentioned earlier, we are already working on improvements that will make kube-green a better tool, hopefully making the world a better place as well.\n\nIn the next release of kube-green, all resources available in Kubernetes will be supported. Instead of directly acting on specific resources, it will be possible to define rules to schedule the shutdown and possibly the startup of any resource at defined times.\n\nWith these patches, kube-green's support won't be limited to just Deployments and CronJobs but will also extend to ReplicaSets, Pods, StatefulSets, and any other custom CRDs.\n\n```yml\napiVersion: kube-green.com/v1alpha1\nkind: SleepInfo\nmetadata:\n\tname: working-hours\nspec:\n\tweekdays: \"*\"\n\tsleepAt: \"20:00\"\n\twakeUpAt: \"08:00\"\n\tsuspendCronJobs: true\n\tpatches:\n        - target:\n             group: apps\n             kind: StatefulSet\n          patch:  \n            - path: /spec/replicas\n              op: add\n              value: 0\n       - target:\n            group: apps\n            kind: ReplicaSet\n         patch:\n           - path: /spec/replicas\n             op: add\n             value: 0\n\texcludeRef:\n        - apiVersion: \"apps/v1\"\n          kind:       StatefulSet\n          name:       do-not-sleep\n        - matchLabels:\n            kube-green.dev/exclude: \"true\"\n```\n\nWe are experimenting with solutions that would allow kube-green to interact with other operators like [Crossplane](https://www.crossplane.io/). This would enable scheduling the shutdown and restart of entire pieces of infrastructure managed through this tool.\n\nAnyone who wants to share their thoughts or contribute directly to the project with ideas, code, or content can take a look at the [repository on GitHub](https://github.com/kube-green/kube-green) or contact us directly.\n\n### Use your resources consciously: keep what you need when you need it\n\nThese days, our easy access to resources has made us a bit less mindful of how we use them. An unused pod isn't just an unused pod; in the long run, it has a significant impact on our overall carbon footprint.\n\nGreenOps allows us to have a significant positive impact on the environment with a negligible impact on our architectures. With minimal additional effort, we can ensure that our clusters use resources much more efficiently and without waste.\n\nWith modern approaches, we can implement our GreenOps strategy without drastically altering the management of our workloads, while also achieving positive impacts on our infrastructure costs. GreenOps and FinOps are extremely related concepts, and often, rationalizing our resources from a sustainability perspective leads to savings on our cloud bills.\n\nTo make informed decisions, it is essential to have visibility into both the costs of our clusters and their CO2eq emissions into the environment. For this reason, many cost monitoring tools, such as [OpenCost](https://www.opencost.io/), have introduced carbon cost emissions tracking across Kubernetes and cloud spending.\n\nIn conclusion, GreenOps can be the first step in aligning our operational practices with environmental sustainability before implementing more complex strategies aimed at using more efficient hardware or preferring renewable energy sources. With GreenOps techniques, you can improve your environmental impact from day one and see immediate results. This allows you to collect data and move more consciously in the vast world of eco-friendly software.\n","metadata":{"title":"Using GreenOps to Improve Your Operational Efficiency and Save the Planet","excerpt":"Discover how GreenOps practices can help reduce your infrastructure's environmental impact while improving operational efficiency, featuring practical solutions like kube-green for sustainable Kubernetes operations.","date":"2024-06-26","author":"Graziano Casto"}},{"slug":"what-cloud-native-mean","content":"\nGive it a shot on Google with the term \"Cloud Native\".\nThe top result you'll find is the definition according to [Amazon AWS](https://aws.amazon.com/what-is/cloud-native/), which goes like this:\n\n\u003e [!note]\n\u003e Cloud native is the software approach of building, deploying, and managing modern applications in cloud computing environments. Modern companies want to build highly scalable, flexible, and resilient applications that they can update quickly to meet customer demands.\n\nThis definition is widely agreeable since it touches upon the _software approach_, _modern application_, and _cloud environment_ aspects — all fundamental elements associated with the term _cloud native_. However, the issue lies in the underlying implication that \"being in the cloud\" is a necessary condition to be considered cloud native. What's even more problematic is the tendency for some to interpret \"run the apps in the cloud\" as a sufficient criterion for labeling their applications as cloud native, as if merely hosting infrastructure on any cloud provider automatically qualifies it as such.\n\n### Would I be considered cloud native if I wash dirty laundry at home?\n\nThe question naturally arises: can applications running in on-premise environments be considered modern? Does the development of _resilient_, _scalable_, and _flexible_ applications remain exclusive to those managing their infrastructure on any cloud provider? Conversely, does any application deployed in the cloud automatically embody the characteristics of a modern application?\n\nThe short answer is no. Not all donuts have holes, and not all applications hosted in the cloud can be considered modern. Staying within the realm of AWS, setting up an EC2 instance, installing a web server, a database, and running a web application on a single VM doesn't align with the principles of scalability, flexibility, and resilience required to deem an application as modern.\n\nAt the same time, cloud services like EKS or GKE are nothing but a fully managed service provided by the respective cloud provider, which eliminates the complexity of installing, managing, and maintaining full control, in this case, over Kubernetes. Behind almost all managed services of a cloud provider, there's often an open-source service that can be independently installed and managed on-premise as well. Behind EKS and GKE, there's Kubernetes; behind MSK, there's Kafka, and behind RDS, there's any of the many relational databases available like PostgreSQL, SQL Server, or MySQL.\nIndeed, there are even alternatives to the serverless functions offered by major cloud providers, which are open and deployable in on-premise environments, such as [KNative](https://knative.dev/docs/) and [OpenFaaS](https://www.openfaas.com/).\n\n### Is the cloud useless then?\n\nSo if the tools available on the cloud can also be used on-premise to create application architectures, what exactly does the term \"cloud\" mean in \"cloud-native\"? I know what you're thinking right now: this guy is crazy, trying to push the theory that the cloud is useless and everything I can do in the cloud can be replicated just as easily on self-managed infrastructure.\n\nThe answer to this question is quite simple: the cloud is a technological enabler. It's not the only way to do things, but it's the most efficient way. The definition of cloud-native refers to modern applications characterized by scalability, resilience, and flexibility, which can indeed be achieved on-premise. However, it's undeniable that the cloud adds virtually infinite firepower to these capabilities.\n\nOn self-managed infrastructure, we're limited by bare metal; the scalability and flexibility of our architecture are closely tied to the physical resources we have available. In the cloud, the scalability and flexibility of our architectures are directly proportional to our spending capacity (and our abilities to design efficient and truly scalable architectures, but that's another story).\n\n### Cloud Native is a metter of how, not where\n\nTo conclude, what I'm trying to say is that in my view, the true essence of the term \"cloud native\" lies not in what is used to build applications, but in how they are built. **Cloud Native isn't about the cloud; it's about technological excellence.** Distributed architectures, scalability, flexibility, service decoupling, sustainability, and observability – all of these shape cloud native.\n\nIn this regard, I'd like to reintroduce another definition of cloud native, provided by the [CNCF](https://github.com/cncf/toc/blob/main/DEFINITION.md#definition), which states:\n\n\u003e [!important] \n\u003e Cloud native practices empower organizations to develop, build, and deploy workloads in computing environments (public, private, hybrid cloud) to meet their organizational needs at scale in a programmatic and repeatable manner. It is characterized by loosely coupled systems that interoperate in a manner that is secure, resilient, manageable, sustainable, and observable.\n\u003e Cloud native technologies and architectures typically consist of some combination of containers, service meshes, multi-tenancy, microservices, immutable infrastructure, serverless, and declarative APIs — this list is non-exhaustive.\n\nThis definition captures the core principles of cloud native, highlighting adaptability, security, resilience, manageability, and observability in deploying workloads across diverse computing environments.\n","metadata":{"title":"What the Fu** Does Cloud Native Mean?","excerpt":"A critical examination of what 'Cloud Native' truly means, challenging the misconception that simply running applications in the cloud automatically qualifies them as modern, scalable, or resilient.","date":"2024-04-30","author":"Graziano Casto"}},{"slug":"green-cloud-native-apps","content":"\n[Published by TheNewStack](https://thenewstack.io/an-open-source-journey-to-greener-cloud-native-environments/)\n\nOver a year, the energy consumed by an average email inbox, which typically holds around 10,000 emails, is equivalent to driving a car for 212 meters. When this energy consumption is multiplied by global email usage, it generates carbon dioxide emissions equivalent to adding 7 million cars to the roads.\n\nThe information and communication technology sector produces around 1.4% of overall global emissions. This equates to an estimated 1.6 billion tons of greenhouse gas emissions, making each one of us internet users responsible for about 400kg of carbon dioxide a year.\n\nThe global digital carbon footprint could be reduced by 80% if the electricity powering it shifted from fossil fuels to renewable sources. Tech giants like Google and Meta are taking decisive steps in this direction, committing substantial financial resources toward carbon removal as part of a broader industry collaboration.\n\nHowever, environmental responsibility in our use of resources can’t be solely delegated to big tech companies. It should be a shared responsibility. Every company, regardless of size, should be aware of its carbon footprint and make necessary adjustments to improve environmental sustainability.\n\nHere are some approaches to enhance the environmental sustainability of cloud native applications by implementing optimizations in Kubernetes runtimes. We will explore methods to shift resource usage toward renewable energy sources and minimize the so-called cloud zombies.\n\n### A first step to a greener cloud native experience\n\nOne of the most challenging aspects of green software is understanding where to start. Before delving into more complex things, a good starting point is to raise awareness of the hidden “zombies” within our infrastructure. By zombies, I refer to workloads that do not perform any useful tasks, resulting in a waste of both environmental and financial resources.\n\nThose used for testing and development purposes — typically only during working hours — are a common example. Considering that the average ratio between working and nonworking hours in a week is approximately 4-to-1 in favor of nonworking hours, it’s evident that these workloads need to be addressed.\n\nOne effective solution is kube-green, an open source tool developed by Davide Bianchi, senior technical leader at Mia-Platform. Kube-green manages Kubernetes cluster resizing to optimize IT infrastructure energy consumption, reducing CO2 emissions by around 30% on average. This tool acts as a Kubernetes controller that defines a custom resource definition (CRD) named SleepInfo, which allows you to pause and restart pods within a specific namespace. With kube-green, you can scale down the number of deployments to zero and limit cron jobs to run only during working hours.\n\nKube-green has been part of the scheduling and orchestration section of the Cloud Native Computing Foundation (CNCF) landscape since 2022. The number of kube-green adopters is continuously growing, with users reporting significant benefits — not only in environmental impact but also in cloud cost management, with an average savings of about 30%.\n\n### Carbon Intensity\n\nBeing aware of carbon intensity, which refers to the amount of carbon dioxide emitted per kilowatt-hour of energy consumed in the regions where our cloud service provider makes resources available to us, allows us to adjust the execution of our workloads so that they primarily use energy from renewable sources. The basic concept is simple: When energy production is skewed toward renewable sources, we run more workloads; when it is not, we run fewer.\n\nTo balance workloads so they align with the use of energy sources by the cloud service provider, we have two strategies: temporal shifting, which involves shifting major workloads to times when renewable energy usage is lower in the region where our cluster is located, and spatial shifting, which involves multiregion federation mechanisms to move workloads to physical locations where the grid carbon intensity is lower.\n\nThe choice between the two approaches depends on the type of workload being considered. For extremely time-sensitive computations, such as digital payments, it’s best to adopt the temporal shifting approach. On the other hand, for computations that are not particularly time-sensitive, such as machine learning model training, the spatial approach can also be adopted. For instance, Google has implemented spatial shifting for managing multimedia files on platforms like Google Drive and YouTube. Another important consideration is the presence of policies that require the use of a specific region: In this case, temporal shifting is the only available option.\n\nRegardless of the choice between temporal shifting and spatial shifting, a fundamental requirement is access to data on the carbon intensity of the grid in our regions. To obtain such data, there are two available services: WattTime and Electricity Maps. The difference between the two is that WattTime provides marginal carbon intensity, while Electricity Maps provides average carbon intensity. Consequently, WattTime is preferred for optimizing immediate impacts, while Electricity Maps is more suitable for long-term optimizations.\n\nThe Green Web Foundation has simplified access to this type of data by providing grid-intensity-go, a Go library designed to be integrated into Kubernetes and other schedulers. This allows the use of carbon intensity values in decisions on where and when to run jobs.\n\n### Conclusion\n\nAs we continue to advance with innovative technologies, ensuring the environmental sustainability of cloud computing is essential for long-term success and progress at the forefront of the industry. As the demand for digital services continues to grow exponentially, so does the energy consumption and carbon footprint associated with powering data centers. Therefore, adopting green practices and prioritizing renewable energy sources in cloud infrastructure is mandatory to mitigate the environmental impact and pave the way for a sustainable digital future.\n\nWith this approach, not only do we address ecological concerns, but we also gain significant enhancements in FinOps management. By optimizing resource allocation, we can drive efficiency and reduce waste, contributing to a healthier planet while still pushing the boundaries of technological innovation.\n","metadata":{"title":"Green Cloud Native Apps: Optimize Kubernetes for Renewable Energy","excerpt":"Learn how to enhance the environmental sustainability of cloud-native applications using open-source tools and Kubernetes optimizations to reduce carbon footprint and align with renewable energy sources.","date":"2024-03-20","author":"Graziano Casto"}},{"slug":"being-vs-doing-agile","content":"\nAgile and Scrum are two commonly used terms in the software development industry, but they are often wrongly used interchangeably.\nWhit this article we will explore the key differences between these two terms and understand how they relate to each other.\n\nAgile is a mindset, a true “Way of Life”, a set of values and principles that guide the way teams work together to deliver software (or any other valuable product in other industries, not belonging only to software development) incrementally and iteratively.\nIt was first introduced in 2001 through the Agile Manifesto, which outlines four core values:\n\n1. Individuals and Interactions over processes and tools\n2. Working software over comprehensive documentation\n3. Customer collaboration over contract negotiation\n4. Responding to change over following a plan\n\nAgile is not a specific methodology, but rather a general philosophy that can be applied in various ways. One of the most important aspects of Agile mindset is the ability to adapt to change. As Jeff Sutherland, the co-creator of Scrum, stated:\n\n\u003e [!TIP] Agile is about creating an environment where teams can respond quickly to change. It's about embracing uncertainty and being able to pivot when needed.\n\nIt allows teams to respond quickly to changing requirements, and deliver software that meets the customer's needs quickly.\nIn Agile development, the emphasis is placed on the production of working software, enabling teams to evaluate and verify their outcomes at an early stage of the development cycle.\n\nScrum, on the other hand, is one of the many agile methodologies available. It is a framework for managing and completing complex projects. It is based on the Agile principles, but it provides a specific set of roles, ceremonies, and artifacts to help teams work in a complex environment and accomplish their goals.\nScrum is commonly used in software development, but it can also be applied to other industries such as marketing, finance, and healthcare.\n\nThe benefits of adopting an Agile mindset in the software development industry are numerous.\nAgile development has been shown to improve team collaboration, increase customer satisfaction, and deliver software quickly reducing risk at the same time through the inner iterative approach proposed by Scrum.\n\nAn example of a company that has successfully established a working Agile culture in its employees is Spotify.\nSpotify's engineering culture is heavily influenced by Agile principles, and they use Scrum as their primary methodology. they have achieved significant success by embracing the Agile mindset, and they continue to innovate and improve their development process.\n\n\u003e [!TIP] Agile is not just a process, it's a way of life. It's about being open to change and continuously improving.\n\nIn contrast, companies that do not adopt an Agile mindset may struggle with communication and collaboration among teams, lack of transparency, and difficulty in adapting to changing requirements resulting in not truly valuable and slow releases.\n\n\u003e [!TIP] If you're not continuously improving, you're falling behind.\n\nSummarizing the concept, Agile is a mindset that allows teams to adapt to change and continuously improve, while Scrum is a specific methodology that provides a framework for managing and completing complex projects. The combination of them can lead to significant benefits in software development. It is important to note that while Scrum is a popular methodology, there are other Agile methodologies available such as Kanban, Lean, and XP.\n\nIt's worth mentioning that Agile mindset and Scrum methodology are not only confined to software development, but also it can be applied to other industries such as marketing, finance, and healthcare.\n\nWhen a team or organization is \"doing Agile\" they may be using Agile practices such as Scrum or Kanban, but they may not fully understand the Agile values and principles. They may be going through the motions of ceremonies proposed in Agile Methodologies (such as SCRUM) and using Agile terminology, but they may not be fully embracing the Agile mindset.\n\nOn the other hand, \"being Agile\" means that a team or organization has fully embraced the Agile culture. They understand its values and principles and strive to embody them in everything they do. Their primary goal is to continuously improve by adapting themself to the continuous changes, understanding that Agile adaptation is not a destination, but a lifetime journey.\nBeing Agile through the adoption of one of the many Agile Methodologies available allows teams to be more collaborative, communicative, and flexible, and as a result, they can deliver value to the customer faster and with higher quality.\n\n\"Doing Agile\" is the surface-level implementation of Agile methodologies, while \"being Agile\" is the deeper understanding of Agile and culture, which leads to a truly Agile way of working.\n\nLast but not least, Scrum and the other Agile Methodologies are powerful tools for software development, but they must be understood and applied correctly to achieve the best results. It doesn't mean that companies must choose a specific methodology following it blindly, they can choose some aspects of a specific methodology mixing them with others from other methodologies, creating truly new and completely customized methodologies by the team which will apply them. Some of these custom methodologies could become widely recognized as happened with the Scrumban Framework resulting from some SCRUM aspects mixed with some from Kanban Framework. The key is to find the approach that works best for your team and organization.\n\nConcluding, Agile is a mindset that allows teams to adapt to change, while Scrum is a methodology that provides a framework for managing and completing complex projects. By embracing the Agile mindset and applying Scrum or other Agile methodologies (or a mix of them), teams can build a set of processes that allow them to work together more effectively, deliver value to customers, and continuously improve themselves.","metadata":{"title":"Are You Truly Agile or Just a Monkey Scrum Practitioner?","excerpt":"Explore the fundamental differences between Agile and Scrum, understanding how Agile represents a mindset and way of life while Scrum provides specific project management practices.","date":"2024-03-15","author":"Graziano Casto"}},{"slug":"my-digital-corner","content":"\nOne of the things that has always defined my way of working is the importance I place on communication—not just as a tool for shqaring but also as a driver for learning. Every time I tackle something new, knowing that I’ll later need to explain it to others makes the learning process much easier. I also really enjoy simplifying naturally complex topics by using the powerful tool of storytelling. Over time, this has become a core part of who I am as a professional and how I think. That’s why I decided to create this digital space—for myself and for anyone who wants to step into my daily life, learn about my professional journey, and see what I do and share with the global tech community. If you’re interested in the world of cloud-native technologies, follow me here, on my social channels, and don’t forget to bookmark this blog!\n\n## A bit of myself\n\nI am a child of the new millennium, and for as long as I can remember, my greatest passion has been the world of technology. While other kids spent their weekends at the stationery shop buying the latest issue of their favorite comic book, I was there picking up the latest edition of WinMagazine.\n\nI turned this passion into a profession almost immediately. After graduating from high school, I started working as a software engineer at a small software house in the metropolitan area of Brescia. I began with small tasks: some Java development, a bit of testing support, and occasionally getting my hands dirty with the servers when needed.\n\nNeedless to say, it was love at first sight with programming. The idea of solving complex problems just by pressing a few keys on the keyboard (and occasionally swearing, of course) was nothing short of exhilarating.\n\nFor two years, I kept developing, learning, and diving deeper into product dynamics, until my career took a significant turn. I transitioned from being a software engineer to becoming the product manager for the very product I had been working on. From one day to the next, I went from being the one getting hands-on to the one responsible for maximizing the value created by those who now got their hands dirty in my place.\n\nIt was a whole new world, one that opened doors to unfamiliar horizons: people and effective communication. I realized that the true strength of this industry lies in its people. One of the most crucial qualities for any \"software craftsman\" (and I’m not just talking about developers) is the ability to share knowledge and absorb what others share.\n\nAnd here we are. Four years later, my role now has a strong focus on just that—conferences, social media, publications, and of course, this blog!\n\nTo know more about me read [this page](/about), enjoy it!\n\n## What this blog is intended to be\n\nThis blog is created with the intention of being an open window into my content.\n\nFrom here, anyone eager to dive into some good, solid cloud-native development will find plenty to sink their teeth into through articles, reflections, talks, and much more.","metadata":{"title":"Welcome to My Digital Corner!","excerpt":"A personal introduction to my digital space where I share insights about cloud-native technologies, my professional journey, and the importance of communication and storytelling in tech.","date":"2024-02-01","author":"Graziano Casto"}}]},"schema":null},"__N_SSG":true},"page":"/articles","query":{},"buildId":"SYKRgUHRNYLLx0g7j2Qzh","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>