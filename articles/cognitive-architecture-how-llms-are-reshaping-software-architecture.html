<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><link rel="preload" href="/_next/static/media/graziano-casto.17cc67b9.jpg" as="image" fetchpriority="high"/><title>Cognitive architecture: how LLMs are changing the way we build software - by Graziano Casto</title><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","headline":"Cognitive architecture: how LLMs are changing the way we build software","datePublished":"2025-06-11T00:00:00.000Z","dateModified":"2025-06-11T00:00:00.000Z","author":[{"@type":"Person","name":"Graziano Casto","url":"https://castograziano.com/about-me"}]}</script><meta name="description" content="Cognitive architecture: how LLMs are changing the way we build software - by Graziano Casto"/><link rel="canonical" href="https://www.castograziano.comhttps://castograziano.com/articles/cognitive-architecture-how-llms-are-reshaping-software-architecture"/><meta property="og:title" content="Cognitive architecture: how LLMs are changing the way we build software - by Graziano Casto"/><meta property="og:description" content="Cognitive architecture: how LLMs are changing the way we build software - by Graziano Casto"/><meta property="og:url" content="https://www.castograziano.comhttps://castograziano.com/articles/cognitive-architecture-how-llms-are-reshaping-software-architecture"/><meta property="og:image" content="https://castograziano.com/casto_graziano_personal_website.png"/><meta property="og:type" content="article"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:title" content="Cognitive architecture: how LLMs are changing the way we build software - by Graziano Casto"/><meta name="twitter:description" content="Cognitive architecture: how LLMs are changing the way we build software - by Graziano Casto"/><meta name="twitter:image" content="https://castograziano.com/casto_graziano_personal_website.png"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="icon" href="/images/favicon.ico"/><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png"/><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png"/><meta name="next-head-count" content="20"/><link rel="preload" href="/_next/static/css/04abda8bc6794ff5.css" as="style"/><link rel="stylesheet" href="/_next/static/css/04abda8bc6794ff5.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-78c92fac7aa8fdd8.js"></script><script src="/_next/static/chunks/webpack-0b5d8249fb15f5f3.js" defer=""></script><script src="/_next/static/chunks/framework-2c16ac744b6cdea6.js" defer=""></script><script src="/_next/static/chunks/main-464365b1f754581b.js" defer=""></script><script src="/_next/static/chunks/pages/_app-1c667cc873da825a.js" defer=""></script><script src="/_next/static/chunks/pages/articles/%5Bslug%5D-0c229d73ad55d035.js" defer=""></script><script src="/_next/static/8mb-ixhYuS1a6kfd4PFug/_buildManifest.js" defer=""></script><script src="/_next/static/8mb-ixhYuS1a6kfd4PFug/_ssgManifest.js" defer=""></script></head><body><div id="__next"><script>!function(){try{var d=document.documentElement,c=d.classList;c.remove('light','dark');var e=localStorage.getItem('theme');if('system'===e||(!e&&true)){var t='(prefers-color-scheme: dark)',m=window.matchMedia(t);if(m.media!==t||m.matches){d.style.colorScheme = 'dark';c.add('dark')}else{d.style.colorScheme = 'light';c.add('light')}}else if(e){c.add(e|| '')}if(e==='light'||e==='dark')d.style.colorScheme=e}catch(e){}}()</script><div class="flex w-full"><div class="fixed inset-0 flex justify-center sm:px-8"><div class="flex w-full max-w-7xl lg:px-8"><div class="w-full bg-white ring-1 ring-zinc-100 dark:bg-zinc-900 dark:ring-zinc-300/20"></div></div></div><div class="relative flex w-full flex-col"><header class="pointer-events-none relative z-50 flex flex-none flex-col" style="height:var(--header-height);margin-bottom:var(--header-mb)"><div class="top-0 z-10 h-16 pt-6" style="position:var(--header-position)"><div class="sm:px-8 top-[var(--header-top,theme(spacing.6))] w-full" style="position:var(--header-inner-position)"><div class="mx-auto w-full max-w-7xl lg:px-8"><div class="relative px-4 sm:px-8 lg:px-12"><div class="mx-auto max-w-2xl lg:max-w-5xl"><div class="relative flex gap-4"><div class="flex flex-1"><div class="h-10 w-10 rounded-full bg-white/90 p-0.5 shadow-lg shadow-zinc-800/5 ring-1 ring-zinc-900/5 backdrop-blur dark:bg-zinc-800/90 dark:ring-white/10"><a aria-label="Home" class="pointer-events-auto" href="/"><img alt="" fetchpriority="high" width="1872" height="1914" decoding="async" data-nimg="1" class="rounded-full bg-zinc-100 object-cover dark:bg-zinc-800 h-9 w-9" style="color:transparent" src="/_next/static/media/graziano-casto.17cc67b9.jpg"/></a></div></div><div class="flex flex-1 justify-end md:justify-center"><div class="pointer-events-auto md:hidden" data-headlessui-state=""><button class="group flex items-center rounded-full bg-white/90 px-4 py-2 text-sm font-medium text-zinc-800 shadow-lg shadow-zinc-800/5 ring-1 ring-zinc-900/5 backdrop-blur dark:bg-zinc-800/90 dark:text-zinc-200 dark:ring-white/10 dark:hover:ring-white/20" type="button" aria-expanded="false" data-headlessui-state="">Menu<svg viewBox="0 0 8 6" aria-hidden="true" class="ml-3 h-auto w-2 stroke-zinc-500 group-hover:stroke-zinc-700 dark:group-hover:stroke-zinc-400"><path d="M1.75 1.75 4 4.25l2.25-2.5" fill="none" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></button></div><div hidden="" style="position:fixed;top:1px;left:1px;width:1px;height:0;padding:0;margin:-1px;overflow:hidden;clip:rect(0, 0, 0, 0);white-space:nowrap;border-width:0;display:none"></div><nav class="pointer-events-auto hidden md:block"><ul class="flex rounded-full bg-white/90 px-3 text-sm font-medium text-zinc-800 shadow-lg shadow-zinc-800/5 ring-1 ring-zinc-900/5 backdrop-blur dark:bg-zinc-800/90 dark:text-zinc-200 dark:ring-white/10"><li><a class="relative block px-3 py-2 transition hover:text-teal-500 dark:hover:text-teal-400" href="/about-me">About</a></li><li><a class="relative block px-3 py-2 transition hover:text-teal-500 dark:hover:text-teal-400" href="/articles">Articles</a></li><li><a class="relative block px-3 py-2 transition hover:text-teal-500 dark:hover:text-teal-400" href="/speaking">Speaking</a></li><li><a class="relative block px-3 py-2 transition hover:text-teal-500 dark:hover:text-teal-400" href="/projects">Projects</a></li></ul></nav></div><div class="flex justify-end md:flex-1"><div class="pointer-events-auto"><button type="button" aria-label="Toggle theme" class="group rounded-full bg-white/90 px-3 py-2 shadow-lg shadow-zinc-800/5 ring-1 ring-zinc-900/5 backdrop-blur transition dark:bg-zinc-800/90 dark:ring-white/10 dark:hover:ring-white/20"><svg viewBox="0 0 24 24" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" aria-hidden="true" class="h-6 w-6 fill-zinc-100 stroke-zinc-500 transition group-hover:fill-zinc-200 group-hover:stroke-zinc-700 dark:hidden [@media(prefers-color-scheme:dark)]:fill-teal-50 [@media(prefers-color-scheme:dark)]:stroke-teal-500 [@media(prefers-color-scheme:dark)]:group-hover:fill-teal-50 [@media(prefers-color-scheme:dark)]:group-hover:stroke-teal-600"><path d="M8 12.25A4.25 4.25 0 0 1 12.25 8v0a4.25 4.25 0 0 1 4.25 4.25v0a4.25 4.25 0 0 1-4.25 4.25v0A4.25 4.25 0 0 1 8 12.25v0Z"></path><path d="M12.25 3v1.5M21.5 12.25H20M18.791 18.791l-1.06-1.06M18.791 5.709l-1.06 1.06M12.25 20v1.5M4.5 12.25H3M6.77 6.77 5.709 5.709M6.77 17.73l-1.061 1.061" fill="none"></path></svg><svg viewBox="0 0 24 24" aria-hidden="true" class="hidden h-6 w-6 fill-zinc-700 stroke-zinc-500 transition dark:block [@media(prefers-color-scheme:dark)]:group-hover:stroke-zinc-400 [@media_not_(prefers-color-scheme:dark)]:fill-teal-400/10 [@media_not_(prefers-color-scheme:dark)]:stroke-teal-500"><path d="M17.25 16.22a6.937 6.937 0 0 1-9.47-9.47 7.451 7.451 0 1 0 9.47 9.47ZM12.75 7C17 7 17 2.75 17 2.75S17 7 21.25 7C17 7 17 11.25 17 11.25S17 7 12.75 7Z" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></button></div></div></div></div></div></div></div></div></header><main class="flex-auto"><div class="sm:px-8 mt-16 lg:mt-32"><div class="mx-auto w-full max-w-7xl lg:px-8"><div class="relative px-4 sm:px-8 lg:px-12"><div class="mx-auto max-w-2xl lg:max-w-5xl"><div class="xl:relative"><div class="mx-auto max-w-2xl"><article><header class="flex flex-col"><h1 class="mt-6 text-4xl font-bold tracking-tight text-zinc-800 sm:text-5xl dark:text-zinc-100">Cognitive architecture: how LLMs are changing the way we build software</h1><time dateTime="2025-06-11" class="order-first flex items-center text-base text-zinc-500 dark:text-zinc-400"><span class="h-4 w-0.5 rounded-full bg-zinc-200 dark:bg-zinc-500"></span><span class="ml-3">June 11, 2025</span></time></header><div class="mt-8 prose dark:prose-invert" data-mdx-content="true"><div><p><a href="https://dzone.com/articles/cognitive-architecture-llms-changing-software-development">Published by DZONE</a></p>
<p>Software architecture has long been rooted in object-oriented and, later, service-oriented paradigms. These models have helped teams build modular systems, isolating behavior into manageable services that communicate over well-defined APIs. As systems grew, microservices brought benefits like scalability and decoupling, but also introduced significant complexity in orchestration.</p>
<p>Today, we're witnessing a fundamental shift. The growing influence of foundation models, particularly large language models (LLMs), is changing how we approach software design. These models aren't just code libraries; they can understand context, reason about goals, and generate human-like responses. This has led to the rise of agent-oriented programming, where autonomous agents, not statically programmed services, drive system behavior. In this new paradigm, agents are constructed from language models, structured prompts, memory layers, and external tools.</p>
<p>What drives them is the <strong>cognitive loop</strong>: a cycle where an agent processes input, reasons over its state, takes actions using tools, and updates its memory. As small language models (SLMs) become more capable, this model is evolving to balance performance with flexibility and cost-efficiency.</p>
<h3>The cognitive architecture</h3>
<p>At the core of a cognitive architecture is a language model, effectively the brain of the system. This model is responsible for interpreting input, reasoning about goals, and planning actions. But reasoning alone is not enough. Just like the human brain depends on sensory organs and muscles to perceive and act on the world, an intelligent agent must be able to access and manipulate external systems in a structured way. This is the essence of agentic AI: giving models the ability to act, not just think.</p>
<p>One emerging approach to enable this interaction is the <a href="https://www.anthropic.com/news/model-context-protocol">Model Context Protocol (MCP)</a>, an open standard developed by Anthropic. MCP aims to provide a standardized interface through which models can retrieve contextual information and invoke tools in their environment. However, it’s important to note that MCP is still an early attempt, promising but not yet an established standard. It represents a broader effort across the AI community to define patterns and protocols that allow agents to interface safely and reliably with external components such as APIs, databases, and services.</p>
<p>In systems that use MCP or similar abstractions, the architecture separates reasoning from execution: the model focuses on understanding, planning, and decision-making, while dedicated tooling (like an MCP server) handles the actual execution of external operations. This creates a cognitive loop: the model observes inputs (from the user, sensors, or past interactions), interprets them using memory and reasoning, then takes action through tools that generate new inputs and continue the cycle.</p>
<p>The choice of model driving this architecture is essential. Large language models (LLMs) and small language models (SLMs) offer distinct trade-offs depending on the complexity of the task, the resource constraints, and the required level of reasoning. LLMs such as GPT-4, Claude, and Gemini are trained on massive corpora and exhibit broad generalization, abstraction, and conversational capabilities. They can manage multi-turn dialogues, resolve ambiguity, and reason across diverse domains.</p>
<p>However, they come at a high computational cost and typically require substantial infrastructure to operate efficiently. On the other hand, SLMs like DistilBERT, TinyLLaMA, and Phi-2 are optimized for speed and efficiency. They are lightweight, often open-source, and can be deployed on edge devices or environments with limited resources. While their reasoning capabilities are more narrow and their context windows smaller, they are highly effective for specialized, domain-specific tasks where determinism and performance are prioritized over generalization. This naturally leads to hybrid system designs, where LLMs are responsible for global coordination and strategy, while SLMs handle routine or narrowly scoped operations.</p>
<p>Below is a comparison highlighting the core differences between the two:</p>
<p>| Feature | Large Language Models (LLMs) | Small Language Models (SLMs) |
| ------- | ---------------------------- | ---------------------------- |
| <strong>Examples</strong> | GPT-4, Claude, Gemini | DistilBERT, TinyLLaMA, Phi-2 |
| <strong>Model Size</strong> | Billions to trillions of parameters | Tens to hundreds of millions of parameters |
| <strong>Reasoning Ability</strong> | High, can handle abstract, multi-step tasks | Limited to focused, well-defined tasks |
| <strong>Context Window</strong> | Large (32k–128k tokens) | Small to medium (512–8k tokens) |
| <strong>Inference Cost</strong> | High | Low |
| <strong>Deployment</strong> | Cloud, high-performance infrastructure | Edge, browser, lightweight servers |
| <strong>Use cases</strong> | Complex workflows, multi-agent coordination | Classification, log parsing, quick lookups |</p>
<p>Most cognitive systems benefit from hybrid designs, where an LLM oversees high-level reasoning and coordination, while SLMs handle specialized, well-scoped operations combining performance, adaptability, and cost-efficiency.</p>
<h3>From multi-service to multi-agent architectures: patterns for making agents work together</h3>
<p>As cognitive architectures mature, they evolve from handling isolated use cases to coordinating distributed tasks across multiple agents. This mirrors the shift from monolithic applications to microservice-based designs — only here, the components are intelligent agents that understand goals, reason about actions, and collaborate toward shared outcomes.</p>
<p>In multi-agent architectures, each agent can be powered by the same or different language models, and they may have overlapping or distinct toolsets. Often, agents are also assigned specific personas or domains of expertise, allowing them to handle different parts of a broader workflow. The structure of multi-agent systems generally falls along a spectrum between two extremes: <strong>vertical</strong> and <strong>horizontal</strong> coordination.</p>
<p>In vertical architectures, one agent plays the role of leader, orchestrating others and delegating responsibilities in a top-down manner. Communication typically flows through this central agent, though in some cases, all agents may share a joint conversational thread overseen by the leader. These systems work well for hierarchical workflows that benefit from clear task separation and control. In contrast, horizontal architectures treat all agents as peers. Each agent can see the shared context and respond accordingly, contributing ideas, solving tasks, or calling tools independently. These systems are better suited for collaborative environments where feedback, shared reasoning, and open discussion improve task outcomes.</p>
<p>Whether organized hierarchically or as peers, these agents can exchange information through memory structures, direct messaging, or via orchestration protocols such as <a href="https://github.com/google-a2a/A2A">A2A (Agent-to-Agent)</a>. As a result, systems gain modularity and resilience: agents can be updated or swapped without affecting the overall design, and capabilities can grow organically by expanding the agent set.</p>
<h3>Agent-Oriented systems: an interactive shopping assistant example</h3>
<p>To illustrate the capabilities of cognitive architectures, let’s consider an <strong>interactive shopping assistant</strong> for an e-commerce platform. Unlike a traditional product recommendation system focused on speed and structured queries, this assistant prioritizes a flexible, conversational user experience, allowing users to describe their preferences naturally and refine their choices through dialogue.</p>
<p>Imagine a user looking for a new outfit who might start with a free-form request like: “I’m looking for a red floral summer dress in medium size.” Instead of routing this through predefined APIs and checkboxes, we create an agent powered by an LLM that can understand such nuanced requests, extract product attributes, query the catalog, filter results, and engage in a dynamic conversation to refine the search.</p>
<p>Here’s how this could work with an agent-oriented approach, leveraging frameworks like <a href="https://google.github.io/adk-docs/">Google's Agent Development Kit (ADK)</a>, LangChain, or AutoGen. Let's assume we use ADK and have several tools available to the agent:</p>
<ul>
<li><code>product_search(query_parameters: dict)</code>: This tool interacts with the product catalog. Instead of fetching the entire catalog, it takes structured parameters (e.g., <code>{'color': 'red', 'pattern': 'floral', 'category': 'dress', 'size': 'medium'}</code>) and returns a filtered list of products. This addresses the context window limitation by allowing the agent to perform targeted searches.</li>
<li><code>image_recognition(image_url: str)</code>: This tool processes an uploaded image to identify attributes like color, style, and patterns.</li>
<li><code>refine_search(product_id: str, new_parameters: dict)</code>: Allows the agent to modify an existing search or product selection based on user feedback.</li>
<li><code>user_profile_update(preferences: dict)</code>: Stores user preferences in a vector memory or database for personalized recommendations in future interactions.</li>
</ul>
<p>The agent’s workflow would incorporate a <strong>planning phase</strong> and the ability to handle <strong>multi-turn interactions</strong>:</p>
<ol>
<li><strong>Initial request and intent understanding (LLM)</strong>: The user says, “I’m looking for a red floral summer dress in medium size.” The LLM agent, acting as the brain, processes this free-form text. It identifies the user’s intent (find a dress) and extracts key attributes: <code>color: red, pattern: floral, category: dress, size: medium</code>.
<ul>
<li><strong>Planning</strong>: The agent determines the best course of action. It decides to use the product_search tool first.</li>
</ul>
</li>
<li><strong>Tool invocation and execution</strong>: The agent constructs a structured query based on the extracted attributes and calls the product_search tool: <code>product_search({'color': 'red', 'pattern': 'floral', 'category': 'dress', 'size': 'medium'})</code>.</li>
<li><strong>Tool output and response generation (LLM)</strong>: The <code>product_search</code> tool returns a list of matching dresses. The LLM then synthesizes these results into a human-readable response, perhaps showing a few top recommendations with product names and prices. For example: "I found a few red floral summer dresses for you! How about the 'Crimson Bloom Maxi Dress' or the 'Garden Party Midi Dress'?"</li>
<li><strong>Refinement and dialogue (LLM and tools)</strong>: The user responds, “I like the Garden Party Midi Dress, but do you have it in blue instead of red?”
<ul>
<li><strong>Reasoning and planning</strong>: The LLM understands this is a refinement request. It recognizes the <code>product_id</code> (Garden Party Midi Dress) and the new <code>color: blue</code>. It plans to use the <code>refine_search</code> tool.</li>
<li><strong>Tool invocation</strong>: The agent calls <code>refine_search({'product_id': 'Garden Party Midi Dress', 'color': 'blue'})</code>.</li>
<li><strong>Guardrails/validation</strong>: If the <code>refine_search</code> tool returns no results, the agent is programmed with a fallback: "Unfortunately, the 'Garden Party Midi Dress' isn't available in blue. Would you like to see other blue floral dresses?" This demonstrates a guardrail to ensure a graceful fallback rather than a generic error.</li>
</ul>
</li>
<li><strong>Image-based search (optional)</strong>: If the user uploads a picture and says, “Find me something like this,” the agent could leverage the <code>image_recognition</code> tool to extract visual attributes, then use <code>product_search</code> with those attributes.</li>
</ol>
<p>Here’s a simplified Python example demonstrating the ADK agent with multiple tools:</p>
<pre><code class="language-python">import os
import requests
from google.adk.agents import Agent

def product_search(query_parameters: dict) -> dict:
    """
    Searches the product catalog based on structured query parameters.

    Args:
        query_parameters (dict): A dictionary of parameters like {'color': 'red', 'category': 'dress'}.

    Returns:
        dict: The search response or an error message.
    """
    try:
        products_api_url = os.getenv("PRODUCTS_SEARCH_API_PATH")
        if not products_api_url:
            raise ValueError("PRODUCTS_SEARCH_API_PATH not defined.")
        
        response = requests.get(products_api_url, params=query_parameters)
        response.raise_for_status()
        return {"status": "success", "report": {"data": response.json()}}
    except Exception as e:
        return {"status": "error", "error_message": f"Error searching products: {str(e)}"}

def refine_search(product_id: str, new_parameters: dict) -> dict:
    """
    Refines an existing product search or modifies parameters for a specific product.
    
    Args:
        product_id (str): The ID of the product to refine.
        new_parameters (dict): New parameters to apply (e.g., {'color': 'blue'}).

    Returns:
        dict: The updated product information or an error.
    """
    # This would typically interact with a product details API or an update mechanism
    print(f"Refining product {product_id} with parameters: {new_parameters}")
    # Simulate a successful refinement for demonstration
    return {"status": "success", "report": {"message": f"Refined search for {product_id} with new parameters."}}


root_agent = Agent(
    name="interactive_shopping_assistant",
    model="gemini-2.0-flash", # Or a more capable LLM like Gemini 1.5 Pro for complex reasoning
    description=(
        "An agent that provides a conversational interface for product discovery and recommendations."
    ),
    instruction=(
        "You are a helpful interactive shopping assistant. Understand user preferences from freeform text or images, "
        "use available tools to find products, and engage in multi-turn dialogues to refine results. "
        "If a search yields no results, suggest alternative options gracefully."
    ),
    tools=[product_search, refine_search], # Add other tools like image_recognition as needed
)

# Example of agent processing a request (conceptual)
# user_input_1 = "I'm looking for a red floral summer dress in medium size."
# agent_response_1 = root_agent.process_input(user_input_1) 
# print(agent_response_1)
# 
# user_input_2 = "I like the Garden Party Midi Dress, but do you have it in blue instead of red?"
# agent_response_2 = root_agent.process_input(user_input_2)
# print(agent_response_2)
</code></pre>
<p>The agentic approach simplifies orchestration logic by allowing the LLM to interpret intent, sequence tool usage, and manage context within a single reasoning loop. This makes the architecture highly adaptive to shifting user expectations and business needs, especially when new product attributes or complex search patterns emerge.</p>
<p>The ability to integrate user feedback in real-time, refine searches conversationally, and handle diverse input modalities (like text or images) demonstrates why cognitive architectures excel in scenarios requiring flexibility and natural interaction.</p>
<h3>Benefits and challenges of cognitive architectures</h3>
<p>One of the most significant advantages of cognitive architectures is the <strong>natural interface</strong> they provide. Users and developers can interact with systems through plain language rather than structured APIs or formal input schemas. This allows faster iteration and reduces the complexity typically associated with tightly coupled service orchestration. But the real shift comes from the agent’s ability to <strong>reason</strong>.</p>
<p>Reasoning is a core part of human intelligence: it allows us to make informed decisions, adapt to unexpected situations, and learn from new information. The same capabilities are essential for agents. Without reasoning, an agent might take user input too literally, fail to account for multi-step implications, or ignore relevant context. With reasoning, agents can plan, reflect, revise, and make decisions autonomously.</p>
<p>In practice, most agent architectures include a dedicated <strong>planning phase</strong>, where the model chooses how to act before executing any specific steps. This planning can follow various strategies, such as task decomposition, multi-option evaluation, retrieval-augmented guidance, or plan refinement. More advanced techniques, like representing plans as graphs (e.g., in Plan Like a Graph or PLaG), allow agents to execute steps in parallel, improving performance for workflows with many independent subtasks. The ability to <strong>adapt</strong> is another key benefit. Agents don’t require redeployment to change behavior: often, changing a prompt or swapping a tool is enough. They can integrate feedback, adjust strategies in real time, and operate in environments where the full task definition is not known upfront.</p>
<p>Of course, there are tradeoffs. Agents must manage <strong>limited context windows</strong>, which can impact long-running or multi-step tasks. Reasoning itself requires larger models, which increases the cost. Using SLMs can reduce this overhead, but it comes with limited planning and abstraction capabilities. There’s also the challenge of <strong>unpredictability</strong>. Traditional systems are deterministic and easy to debug. In contrast, agents reason probabilistically, and tracing their decisions isn’t straightforward. Ensuring consistent outputs often means combining language model-based reasoning with <strong>guardrails, fallback logic, or rules-based validators</strong>, as seen in our shopping assistant example, where the agent gracefully handles unavailable product variations.</p>
<p>Finally, <strong>observability</strong> remains a critical frontier. As reasoning becomes a central part of system behavior, we need better tools to trace decisions, evaluate alternatives, and debug unexpected outputs. This will be key to deploying robust, production-grade agentic systems at scale.</p>
<h3>Conclusion</h3>
<p>The movement toward agentic architectures signals a deeper change in how we think about software. Instead of writing detailed instructions and managing services manually, we are increasingly enabling intelligent agents to reason, act, and learn on our behalf. By carefully combining LLMs and SLMs, developers can design systems that are not only more powerful but also more adaptable. The cognitive capabilities of modern models allow us to abstract complexity and work closer to natural human thinking.</p>
<p>Yet this power comes with new responsibilities. As we step into a world of cognitive software, we must rethink reliability, cost management, and transparency. The future of software may not be written in code alone: it may be prompted, reasoned, and evolved through agents that think alongside us.</p>
</div></div></article></div></div></div></div></div></div></main><footer class="mt-32 flex-none"><div class="sm:px-8"><div class="mx-auto w-full max-w-7xl lg:px-8"><div class="border-t border-zinc-100 pb-16 pt-10 dark:border-zinc-700/40"><div class="relative px-4 sm:px-8 lg:px-12"><div class="mx-auto max-w-2xl lg:max-w-5xl"><div class="flex flex-col items-center justify-between gap-6 sm:flex-row"><div class="flex flex-wrap justify-center gap-x-6 gap-y-1 text-sm font-medium text-zinc-800 dark:text-zinc-200"><a class="transition hover:text-teal-500 dark:hover:text-teal-400" href="/about-me">About</a><a class="transition hover:text-teal-500 dark:hover:text-teal-400" href="/projects">Projects</a><a class="transition hover:text-teal-500 dark:hover:text-teal-400" href="/speaking">Speaking</a></div><p class="text-sm text-zinc-500 dark:text-zinc-400">© <!-- -->2025<!-- --> Graziano Casto. All rights reserved.</p></div></div></div></div></div></div></footer></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"data":{"article":{"title":"Cognitive architecture: how LLMs are changing the way we build software","date":"2025-06-11"},"children":"\u003cp\u003e\u003ca href=\"https://dzone.com/articles/cognitive-architecture-llms-changing-software-development\"\u003ePublished by DZONE\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSoftware architecture has long been rooted in object-oriented and, later, service-oriented paradigms. These models have helped teams build modular systems, isolating behavior into manageable services that communicate over well-defined APIs. As systems grew, microservices brought benefits like scalability and decoupling, but also introduced significant complexity in orchestration.\u003c/p\u003e\n\u003cp\u003eToday, we're witnessing a fundamental shift. The growing influence of foundation models, particularly large language models (LLMs), is changing how we approach software design. These models aren't just code libraries; they can understand context, reason about goals, and generate human-like responses. This has led to the rise of agent-oriented programming, where autonomous agents, not statically programmed services, drive system behavior. In this new paradigm, agents are constructed from language models, structured prompts, memory layers, and external tools.\u003c/p\u003e\n\u003cp\u003eWhat drives them is the \u003cstrong\u003ecognitive loop\u003c/strong\u003e: a cycle where an agent processes input, reasons over its state, takes actions using tools, and updates its memory. As small language models (SLMs) become more capable, this model is evolving to balance performance with flexibility and cost-efficiency.\u003c/p\u003e\n\u003ch3\u003eThe cognitive architecture\u003c/h3\u003e\n\u003cp\u003eAt the core of a cognitive architecture is a language model, effectively the brain of the system. This model is responsible for interpreting input, reasoning about goals, and planning actions. But reasoning alone is not enough. Just like the human brain depends on sensory organs and muscles to perceive and act on the world, an intelligent agent must be able to access and manipulate external systems in a structured way. This is the essence of agentic AI: giving models the ability to act, not just think.\u003c/p\u003e\n\u003cp\u003eOne emerging approach to enable this interaction is the \u003ca href=\"https://www.anthropic.com/news/model-context-protocol\"\u003eModel Context Protocol (MCP)\u003c/a\u003e, an open standard developed by Anthropic. MCP aims to provide a standardized interface through which models can retrieve contextual information and invoke tools in their environment. However, it’s important to note that MCP is still an early attempt, promising but not yet an established standard. It represents a broader effort across the AI community to define patterns and protocols that allow agents to interface safely and reliably with external components such as APIs, databases, and services.\u003c/p\u003e\n\u003cp\u003eIn systems that use MCP or similar abstractions, the architecture separates reasoning from execution: the model focuses on understanding, planning, and decision-making, while dedicated tooling (like an MCP server) handles the actual execution of external operations. This creates a cognitive loop: the model observes inputs (from the user, sensors, or past interactions), interprets them using memory and reasoning, then takes action through tools that generate new inputs and continue the cycle.\u003c/p\u003e\n\u003cp\u003eThe choice of model driving this architecture is essential. Large language models (LLMs) and small language models (SLMs) offer distinct trade-offs depending on the complexity of the task, the resource constraints, and the required level of reasoning. LLMs such as GPT-4, Claude, and Gemini are trained on massive corpora and exhibit broad generalization, abstraction, and conversational capabilities. They can manage multi-turn dialogues, resolve ambiguity, and reason across diverse domains.\u003c/p\u003e\n\u003cp\u003eHowever, they come at a high computational cost and typically require substantial infrastructure to operate efficiently. On the other hand, SLMs like DistilBERT, TinyLLaMA, and Phi-2 are optimized for speed and efficiency. They are lightweight, often open-source, and can be deployed on edge devices or environments with limited resources. While their reasoning capabilities are more narrow and their context windows smaller, they are highly effective for specialized, domain-specific tasks where determinism and performance are prioritized over generalization. This naturally leads to hybrid system designs, where LLMs are responsible for global coordination and strategy, while SLMs handle routine or narrowly scoped operations.\u003c/p\u003e\n\u003cp\u003eBelow is a comparison highlighting the core differences between the two:\u003c/p\u003e\n\u003cp\u003e| Feature | Large Language Models (LLMs) | Small Language Models (SLMs) |\n| ------- | ---------------------------- | ---------------------------- |\n| \u003cstrong\u003eExamples\u003c/strong\u003e | GPT-4, Claude, Gemini | DistilBERT, TinyLLaMA, Phi-2 |\n| \u003cstrong\u003eModel Size\u003c/strong\u003e | Billions to trillions of parameters | Tens to hundreds of millions of parameters |\n| \u003cstrong\u003eReasoning Ability\u003c/strong\u003e | High, can handle abstract, multi-step tasks | Limited to focused, well-defined tasks |\n| \u003cstrong\u003eContext Window\u003c/strong\u003e | Large (32k–128k tokens) | Small to medium (512–8k tokens) |\n| \u003cstrong\u003eInference Cost\u003c/strong\u003e | High | Low |\n| \u003cstrong\u003eDeployment\u003c/strong\u003e | Cloud, high-performance infrastructure | Edge, browser, lightweight servers |\n| \u003cstrong\u003eUse cases\u003c/strong\u003e | Complex workflows, multi-agent coordination | Classification, log parsing, quick lookups |\u003c/p\u003e\n\u003cp\u003eMost cognitive systems benefit from hybrid designs, where an LLM oversees high-level reasoning and coordination, while SLMs handle specialized, well-scoped operations combining performance, adaptability, and cost-efficiency.\u003c/p\u003e\n\u003ch3\u003eFrom multi-service to multi-agent architectures: patterns for making agents work together\u003c/h3\u003e\n\u003cp\u003eAs cognitive architectures mature, they evolve from handling isolated use cases to coordinating distributed tasks across multiple agents. This mirrors the shift from monolithic applications to microservice-based designs — only here, the components are intelligent agents that understand goals, reason about actions, and collaborate toward shared outcomes.\u003c/p\u003e\n\u003cp\u003eIn multi-agent architectures, each agent can be powered by the same or different language models, and they may have overlapping or distinct toolsets. Often, agents are also assigned specific personas or domains of expertise, allowing them to handle different parts of a broader workflow. The structure of multi-agent systems generally falls along a spectrum between two extremes: \u003cstrong\u003evertical\u003c/strong\u003e and \u003cstrong\u003ehorizontal\u003c/strong\u003e coordination.\u003c/p\u003e\n\u003cp\u003eIn vertical architectures, one agent plays the role of leader, orchestrating others and delegating responsibilities in a top-down manner. Communication typically flows through this central agent, though in some cases, all agents may share a joint conversational thread overseen by the leader. These systems work well for hierarchical workflows that benefit from clear task separation and control. In contrast, horizontal architectures treat all agents as peers. Each agent can see the shared context and respond accordingly, contributing ideas, solving tasks, or calling tools independently. These systems are better suited for collaborative environments where feedback, shared reasoning, and open discussion improve task outcomes.\u003c/p\u003e\n\u003cp\u003eWhether organized hierarchically or as peers, these agents can exchange information through memory structures, direct messaging, or via orchestration protocols such as \u003ca href=\"https://github.com/google-a2a/A2A\"\u003eA2A (Agent-to-Agent)\u003c/a\u003e. As a result, systems gain modularity and resilience: agents can be updated or swapped without affecting the overall design, and capabilities can grow organically by expanding the agent set.\u003c/p\u003e\n\u003ch3\u003eAgent-Oriented systems: an interactive shopping assistant example\u003c/h3\u003e\n\u003cp\u003eTo illustrate the capabilities of cognitive architectures, let’s consider an \u003cstrong\u003einteractive shopping assistant\u003c/strong\u003e for an e-commerce platform. Unlike a traditional product recommendation system focused on speed and structured queries, this assistant prioritizes a flexible, conversational user experience, allowing users to describe their preferences naturally and refine their choices through dialogue.\u003c/p\u003e\n\u003cp\u003eImagine a user looking for a new outfit who might start with a free-form request like: “I’m looking for a red floral summer dress in medium size.” Instead of routing this through predefined APIs and checkboxes, we create an agent powered by an LLM that can understand such nuanced requests, extract product attributes, query the catalog, filter results, and engage in a dynamic conversation to refine the search.\u003c/p\u003e\n\u003cp\u003eHere’s how this could work with an agent-oriented approach, leveraging frameworks like \u003ca href=\"https://google.github.io/adk-docs/\"\u003eGoogle's Agent Development Kit (ADK)\u003c/a\u003e, LangChain, or AutoGen. Let's assume we use ADK and have several tools available to the agent:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003eproduct_search(query_parameters: dict)\u003c/code\u003e: This tool interacts with the product catalog. Instead of fetching the entire catalog, it takes structured parameters (e.g., \u003ccode\u003e{'color': 'red', 'pattern': 'floral', 'category': 'dress', 'size': 'medium'}\u003c/code\u003e) and returns a filtered list of products. This addresses the context window limitation by allowing the agent to perform targeted searches.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eimage_recognition(image_url: str)\u003c/code\u003e: This tool processes an uploaded image to identify attributes like color, style, and patterns.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003erefine_search(product_id: str, new_parameters: dict)\u003c/code\u003e: Allows the agent to modify an existing search or product selection based on user feedback.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003euser_profile_update(preferences: dict)\u003c/code\u003e: Stores user preferences in a vector memory or database for personalized recommendations in future interactions.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe agent’s workflow would incorporate a \u003cstrong\u003eplanning phase\u003c/strong\u003e and the ability to handle \u003cstrong\u003emulti-turn interactions\u003c/strong\u003e:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eInitial request and intent understanding (LLM)\u003c/strong\u003e: The user says, “I’m looking for a red floral summer dress in medium size.” The LLM agent, acting as the brain, processes this free-form text. It identifies the user’s intent (find a dress) and extracts key attributes: \u003ccode\u003ecolor: red, pattern: floral, category: dress, size: medium\u003c/code\u003e.\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003ePlanning\u003c/strong\u003e: The agent determines the best course of action. It decides to use the product_search tool first.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTool invocation and execution\u003c/strong\u003e: The agent constructs a structured query based on the extracted attributes and calls the product_search tool: \u003ccode\u003eproduct_search({'color': 'red', 'pattern': 'floral', 'category': 'dress', 'size': 'medium'})\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTool output and response generation (LLM)\u003c/strong\u003e: The \u003ccode\u003eproduct_search\u003c/code\u003e tool returns a list of matching dresses. The LLM then synthesizes these results into a human-readable response, perhaps showing a few top recommendations with product names and prices. For example: \"I found a few red floral summer dresses for you! How about the 'Crimson Bloom Maxi Dress' or the 'Garden Party Midi Dress'?\"\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRefinement and dialogue (LLM and tools)\u003c/strong\u003e: The user responds, “I like the Garden Party Midi Dress, but do you have it in blue instead of red?”\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eReasoning and planning\u003c/strong\u003e: The LLM understands this is a refinement request. It recognizes the \u003ccode\u003eproduct_id\u003c/code\u003e (Garden Party Midi Dress) and the new \u003ccode\u003ecolor: blue\u003c/code\u003e. It plans to use the \u003ccode\u003erefine_search\u003c/code\u003e tool.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTool invocation\u003c/strong\u003e: The agent calls \u003ccode\u003erefine_search({'product_id': 'Garden Party Midi Dress', 'color': 'blue'})\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eGuardrails/validation\u003c/strong\u003e: If the \u003ccode\u003erefine_search\u003c/code\u003e tool returns no results, the agent is programmed with a fallback: \"Unfortunately, the 'Garden Party Midi Dress' isn't available in blue. Would you like to see other blue floral dresses?\" This demonstrates a guardrail to ensure a graceful fallback rather than a generic error.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImage-based search (optional)\u003c/strong\u003e: If the user uploads a picture and says, “Find me something like this,” the agent could leverage the \u003ccode\u003eimage_recognition\u003c/code\u003e tool to extract visual attributes, then use \u003ccode\u003eproduct_search\u003c/code\u003e with those attributes.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eHere’s a simplified Python example demonstrating the ADK agent with multiple tools:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003eimport os\nimport requests\nfrom google.adk.agents import Agent\n\ndef product_search(query_parameters: dict) -\u003e dict:\n    \"\"\"\n    Searches the product catalog based on structured query parameters.\n\n    Args:\n        query_parameters (dict): A dictionary of parameters like {'color': 'red', 'category': 'dress'}.\n\n    Returns:\n        dict: The search response or an error message.\n    \"\"\"\n    try:\n        products_api_url = os.getenv(\"PRODUCTS_SEARCH_API_PATH\")\n        if not products_api_url:\n            raise ValueError(\"PRODUCTS_SEARCH_API_PATH not defined.\")\n        \n        response = requests.get(products_api_url, params=query_parameters)\n        response.raise_for_status()\n        return {\"status\": \"success\", \"report\": {\"data\": response.json()}}\n    except Exception as e:\n        return {\"status\": \"error\", \"error_message\": f\"Error searching products: {str(e)}\"}\n\ndef refine_search(product_id: str, new_parameters: dict) -\u003e dict:\n    \"\"\"\n    Refines an existing product search or modifies parameters for a specific product.\n    \n    Args:\n        product_id (str): The ID of the product to refine.\n        new_parameters (dict): New parameters to apply (e.g., {'color': 'blue'}).\n\n    Returns:\n        dict: The updated product information or an error.\n    \"\"\"\n    # This would typically interact with a product details API or an update mechanism\n    print(f\"Refining product {product_id} with parameters: {new_parameters}\")\n    # Simulate a successful refinement for demonstration\n    return {\"status\": \"success\", \"report\": {\"message\": f\"Refined search for {product_id} with new parameters.\"}}\n\n\nroot_agent = Agent(\n    name=\"interactive_shopping_assistant\",\n    model=\"gemini-2.0-flash\", # Or a more capable LLM like Gemini 1.5 Pro for complex reasoning\n    description=(\n        \"An agent that provides a conversational interface for product discovery and recommendations.\"\n    ),\n    instruction=(\n        \"You are a helpful interactive shopping assistant. Understand user preferences from freeform text or images, \"\n        \"use available tools to find products, and engage in multi-turn dialogues to refine results. \"\n        \"If a search yields no results, suggest alternative options gracefully.\"\n    ),\n    tools=[product_search, refine_search], # Add other tools like image_recognition as needed\n)\n\n# Example of agent processing a request (conceptual)\n# user_input_1 = \"I'm looking for a red floral summer dress in medium size.\"\n# agent_response_1 = root_agent.process_input(user_input_1) \n# print(agent_response_1)\n# \n# user_input_2 = \"I like the Garden Party Midi Dress, but do you have it in blue instead of red?\"\n# agent_response_2 = root_agent.process_input(user_input_2)\n# print(agent_response_2)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe agentic approach simplifies orchestration logic by allowing the LLM to interpret intent, sequence tool usage, and manage context within a single reasoning loop. This makes the architecture highly adaptive to shifting user expectations and business needs, especially when new product attributes or complex search patterns emerge.\u003c/p\u003e\n\u003cp\u003eThe ability to integrate user feedback in real-time, refine searches conversationally, and handle diverse input modalities (like text or images) demonstrates why cognitive architectures excel in scenarios requiring flexibility and natural interaction.\u003c/p\u003e\n\u003ch3\u003eBenefits and challenges of cognitive architectures\u003c/h3\u003e\n\u003cp\u003eOne of the most significant advantages of cognitive architectures is the \u003cstrong\u003enatural interface\u003c/strong\u003e they provide. Users and developers can interact with systems through plain language rather than structured APIs or formal input schemas. This allows faster iteration and reduces the complexity typically associated with tightly coupled service orchestration. But the real shift comes from the agent’s ability to \u003cstrong\u003ereason\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eReasoning is a core part of human intelligence: it allows us to make informed decisions, adapt to unexpected situations, and learn from new information. The same capabilities are essential for agents. Without reasoning, an agent might take user input too literally, fail to account for multi-step implications, or ignore relevant context. With reasoning, agents can plan, reflect, revise, and make decisions autonomously.\u003c/p\u003e\n\u003cp\u003eIn practice, most agent architectures include a dedicated \u003cstrong\u003eplanning phase\u003c/strong\u003e, where the model chooses how to act before executing any specific steps. This planning can follow various strategies, such as task decomposition, multi-option evaluation, retrieval-augmented guidance, or plan refinement. More advanced techniques, like representing plans as graphs (e.g., in Plan Like a Graph or PLaG), allow agents to execute steps in parallel, improving performance for workflows with many independent subtasks. The ability to \u003cstrong\u003eadapt\u003c/strong\u003e is another key benefit. Agents don’t require redeployment to change behavior: often, changing a prompt or swapping a tool is enough. They can integrate feedback, adjust strategies in real time, and operate in environments where the full task definition is not known upfront.\u003c/p\u003e\n\u003cp\u003eOf course, there are tradeoffs. Agents must manage \u003cstrong\u003elimited context windows\u003c/strong\u003e, which can impact long-running or multi-step tasks. Reasoning itself requires larger models, which increases the cost. Using SLMs can reduce this overhead, but it comes with limited planning and abstraction capabilities. There’s also the challenge of \u003cstrong\u003eunpredictability\u003c/strong\u003e. Traditional systems are deterministic and easy to debug. In contrast, agents reason probabilistically, and tracing their decisions isn’t straightforward. Ensuring consistent outputs often means combining language model-based reasoning with \u003cstrong\u003eguardrails, fallback logic, or rules-based validators\u003c/strong\u003e, as seen in our shopping assistant example, where the agent gracefully handles unavailable product variations.\u003c/p\u003e\n\u003cp\u003eFinally, \u003cstrong\u003eobservability\u003c/strong\u003e remains a critical frontier. As reasoning becomes a central part of system behavior, we need better tools to trace decisions, evaluate alternatives, and debug unexpected outputs. This will be key to deploying robust, production-grade agentic systems at scale.\u003c/p\u003e\n\u003ch3\u003eConclusion\u003c/h3\u003e\n\u003cp\u003eThe movement toward agentic architectures signals a deeper change in how we think about software. Instead of writing detailed instructions and managing services manually, we are increasingly enabling intelligent agents to reason, act, and learn on our behalf. By carefully combining LLMs and SLMs, developers can design systems that are not only more powerful but also more adaptable. The cognitive capabilities of modern models allow us to abstract complexity and work closer to natural human thinking.\u003c/p\u003e\n\u003cp\u003eYet this power comes with new responsibilities. As we step into a world of cognitive software, we must rethink reliability, cost management, and transparency. The future of software may not be written in code alone: it may be prompted, reasoned, and evolved through agents that think alongside us.\u003c/p\u003e\n","pageTitle":"Cognitive architecture: how LLMs are changing the way we build software - by Graziano Casto","pageDescription":"Cognitive architecture: how LLMs are changing the way we build software - by Graziano Casto","pageLink":"https://castograziano.com/articles/cognitive-architecture-how-llms-are-reshaping-software-architecture","pageImage":"https://castograziano.com/casto_graziano_personal_website.png"},"schema":{"@context":"https://schema.org","@type":"BlogPosting","headline":"Cognitive architecture: how LLMs are changing the way we build software","datePublished":"2025-06-11T00:00:00.000Z","dateModified":"2025-06-11T00:00:00.000Z","author":[{"@type":"Person","name":"Graziano Casto","url":"https://castograziano.com/about-me"}]}},"__N_SSG":true},"page":"/articles/[slug]","query":{"slug":"cognitive-architecture-how-llms-are-reshaping-software-architecture"},"buildId":"8mb-ixhYuS1a6kfd4PFug","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>