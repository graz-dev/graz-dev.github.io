<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><link rel="preload" href="/_next/static/media/graziano-casto.17cc67b9.jpg" as="image" fetchpriority="high"/><title>Kubernetes v1.34: Of Wind &amp; Will (O&#x27; WaW) - by Graziano Casto</title><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","headline":"Kubernetes v1.34: Of Wind & Will (O' WaW)","datePublished":"2025-08-27T00:00:00.000Z","dateModified":"2025-08-27T00:00:00.000Z","author":[{"@type":"Person","name":"Graziano Casto","url":"https://castograziano.com/about-me"}]}</script><meta name="description" content="Kubernetes v1.34: Of Wind &amp; Will (O&#x27; WaW) - by Graziano Casto"/><link rel="canonical" href="https://www.castograziano.comhttps://castograziano.com/articles/kubernetes-v134-announcement"/><meta property="og:title" content="Kubernetes v1.34: Of Wind &amp; Will (O&#x27; WaW) - by Graziano Casto"/><meta property="og:description" content="Kubernetes v1.34: Of Wind &amp; Will (O&#x27; WaW) - by Graziano Casto"/><meta property="og:url" content="https://www.castograziano.comhttps://castograziano.com/articles/kubernetes-v134-announcement"/><meta property="og:image" content="https://castograziano.com/casto_graziano_personal_website.png"/><meta property="og:type" content="article"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:title" content="Kubernetes v1.34: Of Wind &amp; Will (O&#x27; WaW) - by Graziano Casto"/><meta name="twitter:description" content="Kubernetes v1.34: Of Wind &amp; Will (O&#x27; WaW) - by Graziano Casto"/><meta name="twitter:image" content="https://castograziano.com/casto_graziano_personal_website.png"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="icon" href="/images/favicon.ico"/><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png"/><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png"/><meta name="next-head-count" content="20"/><link rel="preload" href="/_next/static/css/3619d3bb64ea3598.css" as="style"/><link rel="stylesheet" href="/_next/static/css/3619d3bb64ea3598.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-78c92fac7aa8fdd8.js"></script><script src="/_next/static/chunks/webpack-0b5d8249fb15f5f3.js" defer=""></script><script src="/_next/static/chunks/framework-2c16ac744b6cdea6.js" defer=""></script><script src="/_next/static/chunks/main-69b16c27ce463005.js" defer=""></script><script src="/_next/static/chunks/pages/_app-666f3c6bf21653ac.js" defer=""></script><script src="/_next/static/chunks/pages/articles/%5Bslug%5D-0c229d73ad55d035.js" defer=""></script><script src="/_next/static/91HY78FfS8Xc4CAJLov4-/_buildManifest.js" defer=""></script><script src="/_next/static/91HY78FfS8Xc4CAJLov4-/_ssgManifest.js" defer=""></script></head><body><div id="__next"><script>!function(){try{var d=document.documentElement,c=d.classList;c.remove('light','dark');var e=localStorage.getItem('theme');if('system'===e||(!e&&true)){var t='(prefers-color-scheme: dark)',m=window.matchMedia(t);if(m.media!==t||m.matches){d.style.colorScheme = 'dark';c.add('dark')}else{d.style.colorScheme = 'light';c.add('light')}}else if(e){c.add(e|| '')}if(e==='light'||e==='dark')d.style.colorScheme=e}catch(e){}}()</script><div class="flex w-full"><div class="fixed inset-0 flex justify-center sm:px-8"><div class="flex w-full max-w-7xl lg:px-8"><div class="w-full bg-white ring-1 ring-zinc-100 dark:bg-zinc-900 dark:ring-zinc-300/20"></div></div></div><div class="relative flex w-full flex-col"><header class="pointer-events-none relative z-50 flex flex-none flex-col" style="height:var(--header-height);margin-bottom:var(--header-mb)"><div class="top-0 z-10 h-16 pt-6" style="position:var(--header-position)"><div class="sm:px-8 top-[var(--header-top,theme(spacing.6))] w-full" style="position:var(--header-inner-position)"><div class="mx-auto w-full max-w-7xl lg:px-8"><div class="relative px-4 sm:px-8 lg:px-12"><div class="mx-auto max-w-2xl lg:max-w-5xl"><div class="relative flex gap-4"><div class="flex flex-1"><div class="h-10 w-10 rounded-full bg-white/90 p-0.5 shadow-lg shadow-zinc-800/5 ring-1 ring-zinc-900/5 backdrop-blur dark:bg-zinc-800/90 dark:ring-white/10"><a aria-label="Home" class="pointer-events-auto" href="/"><img alt="" fetchpriority="high" width="1872" height="1914" decoding="async" data-nimg="1" class="rounded-full bg-zinc-100 object-cover dark:bg-zinc-800 h-9 w-9" style="color:transparent" src="/_next/static/media/graziano-casto.17cc67b9.jpg"/></a></div></div><div class="flex flex-1 justify-end md:justify-center"><div class="pointer-events-auto md:hidden" data-headlessui-state=""><button class="group flex items-center rounded-full bg-white/90 px-4 py-2 text-sm font-medium text-zinc-800 shadow-lg shadow-zinc-800/5 ring-1 ring-zinc-900/5 backdrop-blur dark:bg-zinc-800/90 dark:text-zinc-200 dark:ring-white/10 dark:hover:ring-white/20" type="button" aria-expanded="false" data-headlessui-state="">Menu<svg viewBox="0 0 8 6" aria-hidden="true" class="ml-3 h-auto w-2 stroke-zinc-500 group-hover:stroke-zinc-700 dark:group-hover:stroke-zinc-400"><path d="M1.75 1.75 4 4.25l2.25-2.5" fill="none" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></button></div><div hidden="" style="position:fixed;top:1px;left:1px;width:1px;height:0;padding:0;margin:-1px;overflow:hidden;clip:rect(0, 0, 0, 0);white-space:nowrap;border-width:0;display:none"></div><nav class="pointer-events-auto hidden md:block"><ul class="flex rounded-full bg-white/90 px-3 text-sm font-medium text-zinc-800 shadow-lg shadow-zinc-800/5 ring-1 ring-zinc-900/5 backdrop-blur dark:bg-zinc-800/90 dark:text-zinc-200 dark:ring-white/10"><li><a class="relative block px-3 py-2 transition hover:text-teal-500 dark:hover:text-teal-400" href="/about-me/">About</a></li><li><a class="relative block px-3 py-2 transition hover:text-teal-500 dark:hover:text-teal-400" href="/articles/">Articles</a></li><li><a class="relative block px-3 py-2 transition hover:text-teal-500 dark:hover:text-teal-400" href="/speaking/">Speaking</a></li><li><a class="relative block px-3 py-2 transition hover:text-teal-500 dark:hover:text-teal-400" href="/projects/">Projects</a></li></ul></nav></div><div class="flex justify-end md:flex-1"><div class="pointer-events-auto"><button type="button" aria-label="Toggle theme" class="group rounded-full bg-white/90 px-3 py-2 shadow-lg shadow-zinc-800/5 ring-1 ring-zinc-900/5 backdrop-blur transition dark:bg-zinc-800/90 dark:ring-white/10 dark:hover:ring-white/20"><svg viewBox="0 0 24 24" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" aria-hidden="true" class="h-6 w-6 fill-zinc-100 stroke-zinc-500 transition group-hover:fill-zinc-200 group-hover:stroke-zinc-700 dark:hidden [@media(prefers-color-scheme:dark)]:fill-teal-50 [@media(prefers-color-scheme:dark)]:stroke-teal-500 [@media(prefers-color-scheme:dark)]:group-hover:fill-teal-50 [@media(prefers-color-scheme:dark)]:group-hover:stroke-teal-600"><path d="M8 12.25A4.25 4.25 0 0 1 12.25 8v0a4.25 4.25 0 0 1 4.25 4.25v0a4.25 4.25 0 0 1-4.25 4.25v0A4.25 4.25 0 0 1 8 12.25v0Z"></path><path d="M12.25 3v1.5M21.5 12.25H20M18.791 18.791l-1.06-1.06M18.791 5.709l-1.06 1.06M12.25 20v1.5M4.5 12.25H3M6.77 6.77 5.709 5.709M6.77 17.73l-1.061 1.061" fill="none"></path></svg><svg viewBox="0 0 24 24" aria-hidden="true" class="hidden h-6 w-6 fill-zinc-700 stroke-zinc-500 transition dark:block [@media(prefers-color-scheme:dark)]:group-hover:stroke-zinc-400 [@media_not_(prefers-color-scheme:dark)]:fill-teal-400/10 [@media_not_(prefers-color-scheme:dark)]:stroke-teal-500"><path d="M17.25 16.22a6.937 6.937 0 0 1-9.47-9.47 7.451 7.451 0 1 0 9.47 9.47ZM12.75 7C17 7 17 2.75 17 2.75S17 7 21.25 7C17 7 17 11.25 17 11.25S17 7 12.75 7Z" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></button></div></div></div></div></div></div></div></div></header><main class="flex-auto"><div class="sm:px-8 mt-16 lg:mt-32"><div class="mx-auto w-full max-w-7xl lg:px-8"><div class="relative px-4 sm:px-8 lg:px-12"><div class="mx-auto max-w-2xl lg:max-w-5xl"><div class="xl:relative"><div class="mx-auto max-w-2xl"><article><header class="flex flex-col"><h1 class="mt-6 text-4xl font-bold tracking-tight text-zinc-800 sm:text-5xl dark:text-zinc-100">Kubernetes v1.34: Of Wind &amp; Will (O&#x27; WaW)</h1><time dateTime="2025-08-27" class="order-first flex items-center text-base text-zinc-500 dark:text-zinc-400"><span class="h-4 w-0.5 rounded-full bg-zinc-200 dark:bg-zinc-500"></span><span class="ml-3">August 27, 2025</span></time></header><div class="mt-8 prose dark:prose-invert" data-mdx-content="true"><div><p><a href="https://kubernetes.io/blog/2025/08/27/kubernetes-v1-34-release/">Published by Kubernetes co-authored with Agustina Barbetta, Alejandro Josue Leon Bellido, Melony Qin, Dipesh Rawat</a></p>
<p>Similar to previous releases, the release of Kubernetes v1.34 introduces new stable, beta, and alpha features. The consistent delivery of high-quality releases underscores the strength of our development cycle and the vibrant support from our community.</p>
<p>This release consists of 58 enhancements. Of those enhancements, 23 have graduated to Stable, 22 have entered Beta, and 13 have entered Alpha.</p>
<p>There are also some <a href="https://kubernetes.io/blog/2025/08/27/kubernetes-v1-34-release/#deprecations-and-removals">deprecations and removals</a> in this release; make sure to read about those.</p>
<h2>Release theme and logo</h2>
<p><img src="https://kubernetes.io/blog/2025/08/27/kubernetes-v1-34-release/k8s-v1.34.png" alt="Kubernetes v1.34 logo: Three bears sail a wooden ship with a flag featuring a paw and a helm symbol on the sail, as wind blows across the ocean"></p>
<p>Kubernetes v1.34 logo: Three bears sail a wooden ship with a flag featuring a paw and a helm symbol on the sail, as wind blows across the ocean</p>
<p>A release powered by the wind around us — and the will within us.</p>
<p>Every release cycle, we inherit winds that we don't really control — the state of our tooling, documentation, and the historical quirks of our project. Sometimes these winds fill our sails, sometimes they push us sideways or die down.</p>
<p>What keeps Kubernetes moving isn't the perfect winds, but the will of our sailors who adjust the sails, man the helm, chart the courses and keep the ship steady. The release happens not because conditions are always ideal, but because of the people who build it, the people who release it, and the bears ^, cats, dogs, wizards, and curious minds who keep Kubernetes sailing strong — no matter which way the wind blows.</p>
<p>This release, <strong>Of Wind &#x26; Will (O' WaW)</strong>, honors the winds that have shaped us, and the will that propels us forward.</p>
<p>^ Oh, and you wonder why bears? Keep wondering!</p>
<h2>Spotlight on key updates</h2>
<p>Kubernetes v1.34 is packed with new features and improvements. Here are a few select updates the Release Team would like to highlight!</p>
<h3>Stable: The core of DRA is GA</h3>
<p><a href="https://kubernetes.io/docs/concepts/scheduling-eviction/dynamic-resource-allocation/">Dynamic Resource Allocation</a> (DRA) enables more powerful ways to select, allocate, share, and configure GPUs, TPUs, NICs and other devices.</p>
<p>Since the v1.30 release, DRA has been based around claiming devices using <em>structured parameters</em> that are opaque to the core of Kubernetes. This enhancement took inspiration from dynamic provisioning for storage volumes. DRA with structured parameters relies on a set of supporting API kinds: ResourceClaim, DeviceClass, ResourceClaimTemplate, and ResourceSlice API types under <code>resource.k8s.io</code>, while extending the <code>.spec</code> for Pods with a new <code>resourceClaims</code> field.<br>
The <code>resource.k8s.io/v1</code> APIs have graduated to stable and are now available by default.</p>
<p>This work was done as part of <a href="https://kep.k8s.io/4381">KEP #4381</a> led by WG Device Management.</p>
<h3>Beta: Projected ServiceAccount tokens for kubelet image credential providers</h3>
<p>The <code>kubelet</code> credential providers, used for pulling private container images, traditionally relied on long-lived Secrets stored on the node or in the cluster. This approach increased security risks and management overhead, as these credentials were not tied to the specific workload and did not rotate automatically.<br>
To solve this, the <code>kubelet</code> can now request short-lived, audience-bound ServiceAccount tokens for authenticating to container registries. This allows image pulls to be authorized based on the Pod's own identity rather than a node-level credential.<br>
The primary benefit is a significant security improvement. It eliminates the need for long-lived Secrets for image pulls, reducing the attack surface and simplifying credential management for both administrators and developers.</p>
<p>This work was done as part of <a href="https://kep.k8s.io/4412">KEP #4412</a> led by SIG Auth and SIG Node.</p>
<h3>Alpha: Support for KYAML, a Kubernetes dialect of YAML</h3>
<p>KYAML aims to be a safer and less ambiguous YAML subset, and was designed specifically for Kubernetes. Whatever version of Kubernetes you use, starting from Kubernetes v1.34 you are able to use KYAML as a new output format for kubectl.</p>
<p>KYAML addresses specific challenges with both YAML and JSON. YAML's significant whitespace requires careful attention to indentation and nesting, while its optional string-quoting can lead to unexpected type coercion (for example: <a href="https://hitchdev.com/strictyaml/why/implicit-typing-removed/">"The Norway Bug"</a>). Meanwhile, JSON lacks comment support and has strict requirements for trailing commas and quoted keys.</p>
<p>You can write KYAML and pass it as an input to any version of <code>kubectl</code>, because all KYAML files are also valid as YAML. With <code>kubectl</code> v1.34, you are also able to <a href="https://kubernetes.io/docs/reference/kubectl/#syntax-1">request KYAML output</a> (as in kubectl get -o kyaml …) by setting environment variable <code>KUBECTL_KYAML=true</code>. If you prefer, you can still request the output in JSON or YAML format.</p>
<p>This work was done as part of <a href="https://kep.k8s.io/5295">KEP #5295</a> led by SIG CLI.</p>
<h2>Features graduating to Stable</h2>
<p><em>This is a selection of some of the improvements that are now stable following the v1.34 release.</em></p>
<h3>Delayed creation of Job’s replacement Pods</h3>
<p>By default, Job controllers create replacement Pods immediately when a Pod starts terminating, causing both Pods to run simultaneously. This can cause resource contention in constrained clusters, where the replacement Pod may struggle to find available nodes until the original Pod fully terminates. The situation can also trigger unwanted cluster autoscaler scale-ups. Additionally, some machine learning frameworks like TensorFlow and <a href="https://jax.readthedocs.io/en/latest/">JAX</a> require only one Pod per index to run at a time, making simultaneous Pod execution problematic. This feature introduces <code>.spec.podReplacementPolicy</code> in Jobs. You may choose to create replacement Pods only when the Pod is fully terminated (has <code>.status.phase: Failed</code>). To do this, set <code>.spec.podReplacementPolicy: Failed</code>.<br>
Introduced as alpha in v1.28, this feature has graduated to stable in v1.34.</p>
<p>This work was done as part of <a href="https://kep.k8s.io/3939">KEP #3939</a> led by SIG Apps.</p>
<h3>Recovery from volume expansion failure</h3>
<p>This feature allows users to cancel volume expansions that are unsupported by the underlying storage provider, and retry volume expansion with smaller values that may succeed.<br>
Introduced as alpha in v1.23, this feature has graduated to stable in v1.34.</p>
<p>This work was done as part of <a href="https://kep.k8s.io/1790">KEP #1790</a> led by SIG Storage.</p>
<h3>VolumeAttributesClass for volume modification</h3>
<p><a href="https://kubernetes.io/docs/concepts/storage/volume-attributes-classes/">VolumeAttributesClass</a> has graduated to stable in v1.34. VolumeAttributesClass is a generic, Kubernetes-native API for modifying volume parameters like provisioned IO. It allows workloads to vertically scale their volumes on-line to balance cost and performance, if supported by their provider.<br>
Like all new volume features in Kubernetes, this API is implemented via the <a href="https://kubernetes-csi.github.io/docs/">container storage interface (CSI)</a>. Your provisioner-specific CSI driver must support the new ModifyVolume API which is the CSI side of this feature.</p>
<p>This work was done as part of <a href="https://kep.k8s.io/3751">KEP #3751</a> led by SIG Storage.</p>
<h3>Structured authentication configuration</h3>
<p>Kubernetes v1.29 introduced a configuration file format to manage API server client authentication, moving away from the previous reliance on a large set of command-line options. The <a href="https://kubernetes.io/docs/reference/access-authn-authz/authentication/#using-authentication-configuration">AuthenticationConfiguration</a> kind allows administrators to support multiple JWT authenticators, CEL expression validation, and dynamic reloading. This change significantly improves the manageability and auditability of the cluster's authentication settings - and has graduated to stable in v1.34.</p>
<p>This work was done as part of <a href="https://kep.k8s.io/3331">KEP #3331</a> led by SIG Auth.</p>
<h3>Finer-grained authorization based on selectors</h3>
<p>Kubernetes authorizers, including webhook authorizers and the built-in node authorizer, can now make authorization decisions based on field and label selectors in incoming requests. When you send <strong>list</strong>, <strong>watch</strong> or <strong>deletecollection</strong> requests with selectors, the authorization layer can now evaluate access with that additional context.</p>
<p>For example, you can write an authorization policy that only allows listing Pods bound to a specific <code>.spec.nodeName</code>. The client (perhaps the kubelet on a particular node) must specify the field selector that the policy requires, otherwise the request is forbidden. This change makes it feasible to set up least privilege rules, provided that the client knows how to conform to the restrictions you set. Kubernetes v1.34 now supports more granular control in environments like per-node isolation or custom multi-tenant setups.</p>
<p>This work was done as part of <a href="https://kep.k8s.io/4601">KEP #4601</a> led by SIG Auth.</p>
<h3>Restrict anonymous requests with fine-grained controls</h3>
<p>Instead of fully enabling or disabling anonymous access, you can now configure a strict list of endpoints where unauthenticated requests are allowed. This provides a safer alternative for clusters that rely on anonymous access to health or bootstrap endpoints like <code>/healthz</code>, <code>/readyz</code>, or <code>/livez</code>.</p>
<p>With this feature, accidental RBAC misconfigurations that grant broad access to anonymous users can be avoided without requiring changes to external probes or bootstrapping tools.</p>
<p>This work was done as part of <a href="https://kep.k8s.io/4633">KEP #4633</a> led by SIG Auth.</p>
<p>The <code>kube-scheduler</code> can now make more accurate decisions about when to retry scheduling Pods that were previously unschedulable. Each scheduling plugin can now register callback functions that tell the scheduler whether an incoming cluster event is likely to make a rejected Pod schedulable again.</p>
<p>This reduces unnecessary retries and improves overall scheduling throughput - especially in clusters using dynamic resource allocation. The feature also lets certain plugins skip the usual backoff delay when it is safe to do so, making scheduling faster in specific cases.</p>
<p>This work was done as part of <a href="https://kep.k8s.io/4247">KEP #4247</a> led by SIG Scheduling.</p>
<h3>Ordered Namespace deletion</h3>
<p>Semi-random resource deletion order can create security gaps or unintended behavior, such as Pods persisting after their associated NetworkPolicies are deleted.<br>
This improvement introduces a more structured deletion process for Kubernetes <a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/">namespaces</a> to ensure secure and deterministic resource removal. By enforcing a structured deletion sequence that respects logical and security dependencies, this approach ensures Pods are removed before other resources.<br>
This feature was introduced in Kubernetes v1.33 and graduated to stable in v1.34. The graduation improves security and reliability by mitigating risks from non-deterministic deletions, including the vulnerability described in <a href="https://github.com/advisories/GHSA-r56h-j38w-hrqq">CVE-2024-7598</a>.</p>
<p>This work was done as part of <a href="https://kep.k8s.io/5080">KEP #5080</a> led by SIG API Machinery.</p>
<h3>Streaming list responses</h3>
<p>Handling large <strong>list</strong> responses in Kubernetes previously posed a significant scalability challenge. When clients requested extensive resource lists, such as thousands of Pods or Custom Resources, the API server was required to serialize the entire collection of objects into a single, large memory buffer before sending it. This process created substantial memory pressure and could lead to performance degradation, impacting the overall stability of the cluster.<br>
To address this limitation, a streaming encoding mechanism for collections (list responses) has been introduced. For the JSON and Kubernetes Protobuf response formats, that streaming mechanism is automatically active and the associated feature gate is stable. The primary benefit of this approach is the avoidance of large memory allocations on the API server, resulting in a much smaller and more predictable memory footprint. Consequently, the cluster becomes more resilient and performant, especially in large-scale environments where frequent requests for extensive resource lists are common.</p>
<p>This work was done as part of <a href="https://kep.k8s.io/5116">KEP #5116</a> led by SIG API Machinery.</p>
<h3>Resilient watch cache initialization</h3>
<p>Watch cache is a caching layer inside <code>kube-apiserver</code> that maintains an eventually consistent cache of cluster state stored in etcd. In the past, issues could occur when the watch cache was not yet initialized during <code>kube-apiserver</code> startup or when it required re-initialization.</p>
<p>To address these issues, the watch cache initialization process has been made more resilient to failures, improving control plane robustness and ensuring controllers and clients can reliably establish watches. This improvement was introduced as beta in v1.31 and is now stable.</p>
<p>This work was done as part of <a href="https://kep.k8s.io/4568">KEP #4568</a> led by SIG API Machinery and SIG Scalability.</p>
<p>Previously, the strict validation of a Pod's DNS <code>search</code> path in Kubernetes often created integration challenges in complex or legacy network environments. This restrictiveness could block configurations that were necessary for an organization's infrastructure, forcing administrators to implement difficult workarounds.<br>
To address this, relaxed DNS validation was introduced as alpha in v1.32 and has now graduated to stable in v1.34. A common use case involves Pods that need to communicate with both internal Kubernetes services and external domains. By setting a single dot (<code>.</code>) as the first entry in the <code>searches</code> list of the Pod's <code>.spec.dnsConfig</code>, administrators can prevent the system's resolver from appending the cluster's internal search domains to external queries. This avoids generating unnecessary DNS requests to the internal DNS server for external hostnames, improving efficiency and preventing potential resolution errors.</p>
<p>This work was done as part of <a href="https://kep.k8s.io/4427">KEP #4427</a> led by SIG Network.</p>
<h3>Support for Direct Service Return (DSR) in Windows kube-proxy</h3>
<p>DSR provides performance optimizations by allowing return traffic routed through load balancers to bypass the load balancer and respond directly to the client, reducing load on the load balancer and improving overall latency. For information on DSR on Windows, read <a href="https://techcommunity.microsoft.com/blog/networkingblog/direct-server-return-dsr-in-a-nutshell/693710">Direct Server Return (DSR) in a nutshell</a>.<br>
Initially introduced in v1.14, this feature has graduated to stable in v1.34.</p>
<p>This work was done as part of <a href="https://kep.k8s.io/5100">KEP #5100</a> led by SIG Windows.</p>
<h3>Sleep action for Container lifecycle hooks</h3>
<p>A Sleep action for containers’ PreStop and PostStart lifecycle hooks was introduced to provide a straightforward way to manage graceful shutdowns and improve overall container lifecycle management.<br>
The Sleep action allows containers to pause for a specified duration after starting or before termination. Using a negative or zero sleep duration returns immediately, resulting in a no-op.<br>
The Sleep action was introduced in Kubernetes v1.29, with zero value support added in v1.32. Both features graduated to stable in v1.34.</p>
<p>This work was done as part of <a href="https://kep.k8s.io/3960">KEP #3960</a> and <a href="https://kep.k8s.io/4818">KEP #4818</a> led by SIG Node.</p>
<h3>Linux node swap support</h3>
<p>Historically, the lack of swap support in Kubernetes could lead to workload instability, as nodes under memory pressure often had to terminate processes abruptly. This particularly affected applications with large but infrequently accessed memory footprints and prevented more graceful resource management.</p>
<p>To address this, configurable per-node swap support was introduced in v1.22. It has progressed through alpha and beta stages and has graduated to stable in v1.34. The primary mode, <code>LimitedSwap</code>, allows Pods to use swap within their existing memory limits, providing a direct solution to the problem. By default, the <code>kubelet</code> is configured with <code>NoSwap</code> mode, which means Kubernetes workloads cannot use swap.</p>
<p>This feature improves workload stability and allows for more efficient resource utilization. It enables clusters to support a wider variety of applications, especially in resource-constrained environments, though administrators must consider the potential performance impact of swapping.</p>
<p>This work was done as part of <a href="https://kep.k8s.io/2400">KEP #2400</a> led by SIG Node.</p>
<h3>Allow special characters in environment variables</h3>
<p>The environment variable validation rules in Kubernetes have been relaxed to allow nearly all printable ASCII characters in variable names, excluding <code>=</code>. This change supports scenarios where workloads require nonstandard characters in variable names - for example, frameworks like.NET Core that use <code>:</code> to represent nested configuration keys.</p>
<p>The relaxed validation applies to environment variables defined directly in Pod spec, as well as those injected using <code>envFrom</code> references to ConfigMaps and Secrets.</p>
<p>This work was done as part of <a href="https://kep.k8s.io/4369">KEP #4369</a> led by SIG Node.</p>
<h3>Taint management is separated from Node lifecycle</h3>
<p>Historically, the <code>TaintManager</code> 's logic for applying NoSchedule and NoExecute taints to nodes based on their condition (NotReady, Unreachable, etc.) was tightly coupled with the node lifecycle controller. This tight coupling made the code harder to maintain and test, and it also limited the flexibility of the taint-based eviction mechanism. This KEP refactors the <code>TaintManager</code> into its own separate controller within the Kubernetes controller manager. It is an internal architectural improvement designed to increase code modularity and maintainability. This change allows the logic for taint-based evictions to be tested and evolved independently, but it has no direct user-facing impact on how taints are used.</p>
<p>This work was done as part of <a href="https://kep.k8s.io/3902">KEP #3902</a> led by SIG Scheduling and SIG Node.</p>
<h2>New features in Beta</h2>
<p><em>This is a selection of some of the improvements that are now beta following the v1.34 release.</em></p>
<h3>Pod-level resource requests and limits</h3>
<p>Defining resource needs for Pods with multiple containers has been challenging, as requests and limits could only be set on a per-container basis. This forced developers to either over-provision resources for each container or meticulously divide the total desired resources, making configuration complex and often leading to inefficient resource allocation. To simplify this, the ability to specify resource requests and limits at the Pod level was introduced. This allows developers to define an overall resource budget for a Pod, which is then shared among its constituent containers. This feature was introduced as alpha in v1.32 and has graduated to beta in v1.34, with HPA now supporting pod-level resource specifications.</p>
<p>The primary benefit is a more intuitive and straightforward way to manage resources for multi-container Pods. It ensures that the total resources used by all containers do not exceed the Pod's defined limits, leading to better resource planning, more accurate scheduling, and more efficient utilization of cluster resources.</p>
<p>This work was done as part of <a href="https://kep.k8s.io/2837">KEP #2837</a> led by SIG Scheduling and SIG Autoscaling.</p>
<h3>.kuberc file for kubectl user preferences</h3>
<p>A <code>.kuberc</code> configuration file allows you to define preferences for <code>kubectl</code>, such as default options and command aliases. Unlike the kubeconfig file, the <code>.kuberc</code> configuration file does not contain cluster details, usernames or passwords.<br>
This feature was introduced as alpha in v1.33, gated behind the environment variable <code>KUBECTL_KUBERC</code>. It has graduated to beta in v1.34 and is enabled by default.</p>
<p>This work was done as part of <a href="https://kep.k8s.io/3104">KEP #3104</a> led by SIG CLI.</p>
<h3>External ServiceAccount token signing</h3>
<p>Traditionally, Kubernetes manages ServiceAccount tokens using static signing keys that are loaded from disk at <code>kube-apiserver</code> startup. This feature introduces an <code>ExternalJWTSigner</code> gRPC service for out-of-process signing, enabling Kubernetes distributions to integrate with external key management solutions (for example, HSMs, cloud KMSes) for ServiceAccount token signing instead of static disk-based keys.</p>
<p>Introduced as alpha in v1.32, this external JWT signing capability advances to beta and is enabled by default in v1.34.</p>
<p>This work was done as part of <a href="https://kep.k8s.io/740">KEP #740</a> led by SIG Auth.</p>
<h3>DRA features in beta</h3>
<h4>Admin access for secure resource monitoring</h4>
<p>DRA supports controlled administrative access via the <code>adminAccess</code> field in ResourceClaims or ResourceClaimTemplates, allowing cluster operators to access devices already in use by others for monitoring or diagnostics. This privileged mode is limited to users authorized to create such objects in namespaces labeled <code>resource.k8s.io/admin-access: "true"</code>, ensuring regular workloads remain unaffected. Graduating to beta in v1.34, this feature provides secure introspection capabilities while preserving workload isolation through namespace-based authorization checks.</p>
<p>This work was done as part of <a href="https://kep.k8s.io/5018">KEP #5018</a> led by WG Device Management and SIG Auth.</p>
<h4>Prioritized alternatives in ResourceClaims and ResourceClaimTemplates</h4>
<p>While a workload might run best on a single high-performance GPU, it might also be able to run on two mid-level GPUs.<br>
With the feature gate <code>DRAPrioritizedList</code> (now enabled by default), ResourceClaims and ResourceClaimTemplates get a new field named <code>firstAvailable</code>. This field is an ordered list that allows users to specify that a request may be satisfied in different ways, including allocating nothing at all if specific hardware is not available. The scheduler will attempt to satisfy the alternatives in the list in order, so the workload will be allocated the best set of devices available in the cluster.</p>
<p>This work was done as part of <a href="https://kep.k8s.io/4816">KEP #4816</a> led by WG Device Management.</p>
<h4>The kubelet reports allocated DRA resources</h4>
<p>The <code>kubelet</code> 's API has been updated to report on Pod resources allocated through DRA. This allows node monitoring agents to discover the allocated DRA resources for Pods on a node. Additionally, it enables node components to use the PodResourcesAPI and leverage this DRA information when developing new features and integrations.<br>
Starting from Kubernetes v1.34, this feature is enabled by default.</p>
<p>This work was done as part of <a href="https://kep.k8s.io/3695">KEP #3695</a> led by WG Device Management.</p>
<h3>kube-scheduler non-blocking API calls</h3>
<p>The <code>kube-scheduler</code> makes blocking API calls during scheduling cycles, creating performance bottlenecks. This feature introduces asynchronous API handling through a prioritized queue system with request deduplication, allowing the scheduler to continue processing Pods while API operations complete in the background. Key benefits include reduced scheduling latency, prevention of scheduler thread starvation during API delays, and immediate retry capability for unschedulable Pods. The implementation maintains backward compatibility and adds metrics for monitoring pending API operations.</p>
<p>This work was done as part of <a href="https://kep.k8s.io/5229">KEP #5229</a> led by SIG Scheduling.</p>
<h3>Mutating admission policies</h3>
<p><a href="https://kubernetes.io/docs/reference/access-authn-authz/mutating-admission-policy/">MutatingAdmissionPolicies</a> offer a declarative, in-process alternative to mutating admission webhooks. This feature leverages CEL's object instantiation and JSON Patch strategies, combined with Server Side Apply’s merge algorithms.<br>
This significantly simplifies admission control by allowing administrators to define mutation rules directly in the API server.<br>
Introduced as alpha in v1.32, mutating admission policies has graduated to beta in v1.34.</p>
<p>This work was done as part of <a href="https://kep.k8s.io/3962">KEP #3962</a> led by SIG API Machinery.</p>
<h3>Snapshottable API server cache</h3>
<p>The <code>kube-apiserver</code> 's caching mechanism (watch cache) efficiently serves requests for the latest observed state. However, <strong>list</strong> requests for previous states (for example, via pagination or by specifying a <code>resourceVersion</code>) often bypass this cache and are served directly from etcd. This direct etcd access significantly increases performance costs and can lead to stability issues, particularly with large resources, due to memory pressure from transferring large data blobs.<br>
With the <code>ListFromCacheSnapshot</code> feature gate enabled by default, <code>kube-apiserver</code> will attempt to serve the response from snapshots if one is available with <code>resourceVersion</code> older than requested. The <code>kube-apiserver</code> starts with no snapshots, creates a new snapshot on every watch event, and keeps them until it detects etcd is compacted or if cache is full with events older than 75 seconds. If the provided <code>resourceVersion</code> is unavailable, the server will fallback to etcd.</p>
<p>This work was done as part of <a href="https://kep.k8s.io/4988">KEP #4988</a> led by SIG API Machinery.</p>
<h3>Tooling for declarative validation of Kubernetes-native types</h3>
<p>Prior to this release, validation rules for the APIs built into Kubernetes were written entirely by hand, which makes them difficult for maintainers to discover, understand, improve or test. There was no single way to find all the validation rules that might apply to an API.<em>Declarative validation</em> benefits Kubernetes maintainers by making API development, maintenance, and review easier while enabling programmatic inspection for better tooling and documentation. For people using Kubernetes libraries to write their own code (for example: a controller), the new approach streamlines adding new fields through IDL tags, rather than complex validation functions. This change helps speed up API creation by automating validation boilerplate, and provides more relevant error messages by performing validation on versioned types.<br>
This enhancement (which graduated to beta in v1.33 and continues as beta in v1.34) brings CEL-based validation rules to native Kubernetes types. It allows for more granular and declarative validation to be defined directly in the type definitions, improving API consistency and developer experience.</p>
<p>This work was done as part of <a href="https://kep.k8s.io/5073">KEP #5073</a> led by SIG API Machinery.</p>
<h3>Streaming informers for list requests</h3>
<p>The streaming informers feature, which has been in beta since v1.32, gains further beta refinements in v1.34. This capability allows <strong>list</strong> requests to return data as a continuous stream of objects from the API server’s watch cache, rather than assembling paged results directly from etcd. By reusing the same mechanics used for <strong>watch</strong> operations, the API server can serve large datasets while keeping memory usage steady and avoiding allocation spikes that can affect stability.</p>
<p>In this release, the <code>kube-apiserver</code> and <code>kube-controller-manager</code> both take advantage of the new <code>WatchList</code> mechanism by default. For the <code>kube-apiserver</code>, this means list requests are streamed more efficiently, while the <code>kube-controller-manager</code> benefits from a more memory-efficient and predictable way to work with informers. Together, these improvements reduce memory pressure during large list operations, and improve reliability under sustained load, making list streaming more predictable and efficient.</p>
<p>This work was done as part of <a href="https://kep.k8s.io/3157">KEP #3157</a> led by SIG API Machinery and SIG Scalability.</p>
<h3>Graceful node shutdown handling for Windows nodes</h3>
<p>The <code>kubelet</code> on Windows nodes can now detect system shutdown events and begin graceful termination of running Pods. This mirrors existing behavior on Linux and helps ensure workloads exit cleanly during planned shutdowns or restarts.<br>
When the system begins shutting down, the <code>kubelet</code> reacts by using standard termination logic. It respects the configured lifecycle hooks and grace periods, giving Pods time to stop before the node powers off. The feature relies on Windows pre-shutdown notifications to coordinate this process. This enhancement improves workload reliability during maintenance, restarts, or system updates. It is now in beta and enabled by default.</p>
<p>This work was done as part of <a href="https://kep.k8s.io/4802">KEP #4802</a> led by SIG Windows.</p>
<h3>In-place Pod resize improvements</h3>
<p>Graduated to beta and enabled by default in v1.33, in-place Pod resizing receives further improvements in v1.34. These include support for decreasing memory usage and integration with Pod-level resources.</p>
<p>This feature remains in beta in v1.34. For detailed usage instructions and examples, refer to the documentation: <a href="https://kubernetes.io/docs/tasks/configure-pod-container/resize-container-resources/">Resize CPU and Memory Resources assigned to Containers</a>.</p>
<p>This work was done as part of <a href="https://kep.k8s.io/1287">KEP #1287</a> led by SIG Node and SIG Autoscaling.</p>
<h2>New features in Alpha</h2>
<p><em>This is a selection of some of the improvements that are now alpha following the v1.34 release.</em></p>
<h3>Pod certificates for mTLS authentication</h3>
<p>Authenticating workloads within a cluster, especially for communication with the API server, has primarily relied on ServiceAccount tokens. While effective, these tokens aren't always ideal for establishing a strong, verifiable identity for mutual TLS (mTLS) and can present challenges when integrating with external systems that expect certificate-based authentication.<br>
Kubernetes v1.34 introduces a built-in mechanism for Pods to obtain X.509 certificates via <a href="https://kubernetes.io/docs/reference/access-authn-authz/certificate-signing-requests/#pod-certificate-requests">PodCertificateRequests</a>. The <code>kubelet</code> can request and manage certificates for Pods, which can then be used to authenticate to the Kubernetes API server and other services using mTLS. The primary benefit is a more robust and flexible identity mechanism for Pods. It provides a native way to implement strong mTLS authentication without relying solely on bearer tokens, aligning Kubernetes with standard security practices and simplifying integrations with certificate-aware observability and security tooling.</p>
<p>This work was done as part of <a href="https://kep.k8s.io/4317">KEP #4317</a> led by SIG Auth.</p>
<h3>"Restricted" Pod security standard now forbids remote probes</h3>
<p>The <code>host</code> field within probes and lifecycle handlers allows users to specify an entity other than the <code>podIP</code> for the <code>kubelet</code> to probe. However, this opens up a route for misuse and for attacks that bypass security controls, since the <code>host</code> field could be set to <strong>any</strong> value, including security sensitive external hosts, or localhost on the node. In Kubernetes v1.34, Pods only meet the <a href="https://kubernetes.io/docs/concepts/security/pod-security-standards/#restricted">Restricted</a> Pod security standard if they either leave the <code>host</code> field unset, or if they don't even use this kind of probe. You can use <em>Pod security admission</em>, or a third party solution, to enforce that Pods meet this standard. Because these are security controls, check the documentation to understand the limitations and behavior of the enforcement mechanism you choose.</p>
<p>This work was done as part of <a href="https://kep.k8s.io/4940">KEP #4940</a> led by SIG Auth.</p>
<h3>Use.status.nominatedNodeName to express Pod placement</h3>
<p>When the <code>kube-scheduler</code> takes time to bind Pods to Nodes, cluster autoscalers may not understand that a Pod will be bound to a specific Node. Consequently, they may mistakenly consider the Node as underutilized and delete it.<br>
To address this issue, the <code>kube-scheduler</code> can use <code>.status.nominatedNodeName</code> not only to indicate ongoing preemption but also to express Pod placement intentions. By enabling the <code>NominatedNodeNameForExpectation</code> feature gate, the scheduler uses this field to indicate where a Pod will be bound. This exposes internal reservations to help external components make informed decisions.</p>
<p>This work was done as part of <a href="https://kep.k8s.io/5278">KEP #5278</a> led by SIG Scheduling.</p>
<h3>DRA features in alpha</h3>
<h4>Resource health status for DRA</h4>
<p>It can be difficult to know when a Pod is using a device that has failed or is temporarily unhealthy, which makes troubleshooting Pod crashes challenging or impossible.<br>
Resource Health Status for DRA improves observability by exposing the health status of devices allocated to a Pod in the Pod’s status. This makes it easier to identify the cause of Pod issues related to unhealthy devices and respond appropriately.<br>
To enable this functionality, the <code>ResourceHealthStatus</code> feature gate must be enabled, and the DRA driver must implement the <code>DRAResourceHealth</code> gRPC service.</p>
<p>This work was done as part of <a href="https://kep.k8s.io/4680">KEP #4680</a> led by WG Device Management.</p>
<h4>Extended resource mapping</h4>
<p>Extended resource mapping provides a simpler alternative to DRA's expressive and flexible approach by offering a straightforward way to describe resource capacity and consumption. This feature enables cluster administrators to advertise DRA-managed resources as <em>extended resources</em>, allowing application developers and operators to continue using the familiar container’s <code>.spec.resources</code> syntax to consume them.<br>
This enables existing workloads to adopt DRA without modifications, simplifying the transition to DRA for both application developers and cluster administrators.</p>
<p>This work was done as part of <a href="https://kep.k8s.io/5004">KEP #5004</a> led by WG Device Management.</p>
<h4>DRA consumable capacity</h4>
<p>Kubernetes v1.33 added support for resource drivers to advertise slices of a device that are available, rather than exposing the entire device as an all-or-nothing resource. However, this approach couldn't handle scenarios where device drivers manage fine-grained, dynamic portions of a device resource based on user demand, or share those resources independently of ResourceClaims, which are restricted by their spec and namespace.<br>
Enabling the <code>DRAConsumableCapacity</code> feature gate (introduced as alpha in v1.34) allows resource drivers to share the same device, or even a slice of a device, across multiple ResourceClaims or across multiple DeviceRequests. The feature also extends the scheduler to support allocating portions of device resources, as defined in the <code>capacity</code> field. This DRA feature improves device sharing across namespaces and claims, tailoring it to Pod needs. It enables drivers to enforce capacity limits, enhances scheduling, and supports new use cases like bandwidth-aware networking and multi-tenant sharing.</p>
<p>This work was done as part of <a href="https://kep.k8s.io/5075">KEP #5075</a> led by WG Device Management.</p>
<h4>Device binding conditions</h4>
<p>The Kubernetes scheduler gets more reliable by delaying binding a Pod to a Node until its required external resources, such as attachable devices or FPGAs, are confirmed to be ready.<br>
This delay mechanism is implemented in the <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/scheduling-framework/#pre-bind">PreBind phase</a> of the scheduling framework. During this phase, the scheduler checks whether all required device conditions are satisfied before proceeding with binding. This enables coordination with external device controllers, ensuring more robust, predictable scheduling.</p>
<p>This work was done as part of <a href="https://kep.k8s.io/5007">KEP #5007</a> led by WG Device Management.</p>
<h3>Container restart rules</h3>
<p>Currently, all containers within a Pod will follow the same <code>.spec.restartPolicy</code> when exited or crashed. However, Pods that run multiple containers might have different restart requirements for each container. For example, for init containers used to perform initialization, you may not want to retry initialization if they fail. Similarly, in ML research environments with long-running training workloads, containers that fail with retriable exit codes should restart quickly in place, rather than triggering Pod recreation and losing progress.<br>
Kubernetes v1.34 introduces the <code>ContainerRestartRules</code> feature gate. When enabled, a <code>restartPolicy</code> can be specified for each container within a Pod. A <code>restartPolicyRules</code> list can also be defined to override <code>restartPolicy</code> based on the last exit code. This provides the fine-grained control needed to handle complex scenarios and better utilization of compute resources.</p>
<p>This work was done as part of <a href="https://kep.k8s.io/5307">KEP #5307</a> led by SIG Node.</p>
<h3>Load environment variables from files created in runtime</h3>
<p>Application developers have long requested greater flexibility in declaring environment variables. Traditionally, environment variables are declared on the API server side via static values, ConfigMaps, or Secrets.</p>
<p>Behind the <code>EnvFiles</code> feature gate, Kubernetes v1.34 introduces the ability to declare environment variables at runtime. One container (typically an init container) can generate the variable and store it in a file, and a subsequent container can start with the environment variable loaded from that file. This approach eliminates the need to "wrap" the target container's entry point, enabling more flexible in-Pod container orchestration.</p>
<p>This feature particularly benefits AI/ML training workloads, where each Pod in a training Job requires initialization with runtime-defined values.</p>
<p>This work was done as part of <a href="https://kep.k8s.io/3721">KEP #5307</a> led by SIG Node.</p>
<h2>Graduations, deprecations, and removals in v1.34</h2>
<h3>Graduations to stable</h3>
<p>This lists all the features that graduated to stable (also known as <em>general availability</em>). For a full list of updates including new features and graduations from alpha to beta, see the release notes.</p>
<p>This release includes a total of 23 enhancements promoted to stable:</p>
<ul>
<li><a href="https://kep.k8s.io/4369">Allow almost all printable ASCII characters in environment variables</a></li>
<li><a href="https://kep.k8s.io/3939">Allow for recreation of pods once fully terminated in the job controller</a></li>
<li><a href="https://kep.k8s.io/4818">Allow zero value for Sleep Action of PreStop Hook</a></li>
<li><a href="https://kep.k8s.io/647">API Server tracing</a></li>
<li><a href="https://kep.k8s.io/24">AppArmor support</a></li>
<li><a href="https://kep.k8s.io/4601">Authorize with Field and Label Selectors</a></li>
<li><a href="https://kep.k8s.io/2340">Consistent Reads from Cache</a></li>
<li><a href="https://kep.k8s.io/3902">Decouple TaintManager from NodeLifecycleController</a></li>
<li><a href="https://kep.k8s.io/4033">Discover cgroup driver from CRI</a></li>
<li><a href="https://kep.k8s.io/4381">DRA: structured parameters</a></li>
<li><a href="https://kep.k8s.io/3960">Introducing Sleep Action for PreStop Hook</a></li>
<li><a href="https://kep.k8s.io/2831">Kubelet OpenTelemetry Tracing</a></li>
<li><a href="https://kep.k8s.io/3751">Kubernetes VolumeAttributesClass ModifyVolume</a></li>
<li><a href="https://kep.k8s.io/2400">Node memory swap support</a></li>
<li><a href="https://kep.k8s.io/4633">Only allow anonymous auth for configured endpoints</a></li>
<li><a href="https://kep.k8s.io/5080">Ordered namespace deletion</a></li>
<li><a href="https://kep.k8s.io/4247">Per-plugin callback functions for accurate requeueing in kube-scheduler</a></li>
<li><a href="https://kep.k8s.io/4427">Relaxed DNS search string validation</a></li>
<li><a href="https://kep.k8s.io/4568">Resilient Watchcache Initialization</a></li>
<li><a href="https://kep.k8s.io/5116">Streaming Encoding for LIST Responses</a></li>
<li><a href="https://kep.k8s.io/3331">Structured Authentication Config</a></li>
<li><a href="https://kep.k8s.io/5100">Support for Direct Service Return (DSR) and overlay networking in Windows kube-proxy</a></li>
<li><a href="https://kep.k8s.io/1790">Support recovery from volume expansion failure</a></li>
</ul>
<h3>Deprecations and removals</h3>
<p>As Kubernetes develops and matures, features may be deprecated, removed, or replaced with better ones to improve the project's overall health. See the Kubernetes <a href="https://kubernetes.io/docs/reference/using-api/deprecation-policy/">deprecation and removal policy</a> for more details on this process. Kubernetes v1.34 includes a couple of deprecations.</p>
<h4>Manual cgroup driver configuration is deprecated</h4>
<p>Historically, configuring the correct cgroup driver has been a pain point for users running Kubernetes clusters. Kubernetes v1.28 added a way for the <code>kubelet</code> to query the CRI implementation and find which cgroup driver to use. That automated detection is now <strong>strongly recommended</strong> and support for it has graduated to stable in v1.34. If your CRI container runtime does not support the ability to report the cgroup driver it needs, you should upgrade or change your container runtime. The <code>cgroupDriver</code> configuration setting in the <code>kubelet</code> configuration file is now deprecated. The corresponding command-line option <code>--cgroup-driver</code> was previously deprecated, as Kubernetes recommends using the configuration file instead. Both the configuration setting and command-line option will be removed in a future release, that removal will not happen before the v1.36 minor release.</p>
<p>This work was done as part of <a href="https://kep.k8s.io/4033">KEP #4033</a> led by SIG Node.</p>
<h4>Kubernetes to end containerd 1.x support in v1.36</h4>
<p>While Kubernetes v1.34 still supports containerd 1.7 and other LTS releases of containerd, as a consequence of automated cgroup driver detection, the Kubernetes SIG Node community has formally agreed upon a final support timeline for containerd v1.X. The last Kubernetes release to offer this support will be v1.35 (aligned with containerd 1.7 EOL). This is an early warning that if you are using containerd 1.X, consider switching to 2.0+ soon. You are able to monitor the <code>kubelet_cri_losing_support</code> metric to determine if any nodes in your cluster are using a containerd version that will soon be outdated.</p>
<p>This work was done as part of <a href="https://kep.k8s.io/4033">KEP #4033</a> led by SIG Node.</p>
<h4>PreferClose traffic distribution is deprecated</h4>
<p>The <code>spec.trafficDistribution</code> field within a Kubernetes <a href="https://kubernetes.io/docs/concepts/services-networking/service/">Service</a> allows users to express preferences for how traffic should be routed to Service endpoints.</p>
<p><a href="https://kep.k8s.io/3015">KEP-3015</a> deprecates <code>PreferClose</code> and introduces two additional values: <code>PreferSameZone</code> and <code>PreferSameNode</code>. <code>PreferSameZone</code> is an alias for the existing <code>PreferClose</code> to clarify its semantics. <code>PreferSameNode</code> allows connections to be delivered to a local endpoint when possible, falling back to a remote endpoint when not possible.</p>
<p>This feature was introduced in v1.33 behind the <code>PreferSameTrafficDistribution</code> feature gate. It has graduated to beta in v1.34 and is enabled by default.</p>
<p>This work was done as part of <a href="https://kep.k8s.io/3015">KEP #3015</a> led by SIG Network.</p>
<h2>Release notes</h2>
<p>Check out the full details of the Kubernetes v1.34 release in our <a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.34.md">release notes</a>.</p>
<h2>Availability</h2>
<p>Kubernetes v1.34 is available for download on or on the <a href="https://kubernetes.io/releases/download/">Kubernetes download page</a>.</p>
<p>To get started with Kubernetes, check out these <a href="https://kubernetes.io/docs/tutorials/">interactive tutorials</a> or run local Kubernetes clusters using <a href="https://minikube.sigs.k8s.io/">minikube</a>. You can also easily install v1.34 using <a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/">kubeadm</a>.</p>
<h2>Release Team</h2>
<p>Kubernetes is only possible with the support, commitment, and hard work of its community. Each release team is made up of dedicated community volunteers who work together to build the many pieces that make up the Kubernetes releases you rely on. This requires the specialized skills of people from all corners of our community, from the code itself to its documentation and project management.</p>
<p><a href="https://github.com/cncf/memorials/blob/main/rodolfo-martinez.md">We honor the memory of Rodolfo "Rodo" Martínez Vega</a>, a dedicated contributor whose passion for technology and community building left a mark on the Kubernetes community. Rodo served as a member of the Kubernetes Release Team across multiple releases, including v1.22-v1.23 and v1.25-v1.30, demonstrating unwavering commitment to the project's success and stability.<br>
Beyond his Release Team contributions, Rodo was deeply involved in fostering the Cloud Native LATAM community, helping to bridge language and cultural barriers in the space. His work on the Spanish version of Kubernetes documentation and the CNCF Glossary exemplified his dedication to making knowledge accessible to Spanish-speaking developers worldwide. Rodo's legacy lives on through the countless community members he mentored, the releases he helped deliver, and the vibrant LATAM Kubernetes community he helped cultivate.</p>
<p>We would like to thank the entire <a href="https://github.com/kubernetes/sig-release/blob/master/releases/release-1.34/release-team.md">Release Team</a> for the hours spent hard at work to deliver the Kubernetes v1.34 release to our community. The Release Team's membership ranges from first-time shadows to returning team leads with experience forged over several release cycles. A very special thanks goes out to our release lead, Vyom Yadav, for guiding us through a successful release cycle, for his hands-on approach to solving challenges, and for bringing the energy and care that drives our community forward.</p>
<h2>Project Velocity</h2>
<p>The CNCF K8s <a href="https://k8s.devstats.cncf.io/d/11/companies-contributing-in-repository-groups?orgId=1&#x26;var-period=m&#x26;var-repogroup_name=All">DevStats</a> project aggregates a number of interesting data points related to the velocity of Kubernetes and various sub-projects. This includes everything from individual contributions to the number of companies that are contributing and is an illustration of the depth and breadth of effort that goes into evolving this ecosystem.</p>
<p>During the v1.34 release cycle, which spanned 15 weeks from 19th May 2025 to 27th August 2025, Kubernetes received contributions from as many as 106 different companies and 491 individuals. In the wider cloud native ecosystem, the figure goes up to 370 companies, counting 2235 total contributors.</p>
<p>Note that "contribution" counts when someone makes a commit, code review, comment, creates an issue or PR, reviews a PR (including blogs and documentation) or comments on issues and PRs.<br>
If you are interested in contributing, visit <a href="https://www.kubernetes.dev/docs/guide/#getting-started">Getting Started</a> on our contributor website.</p>
<p>Source for this data:</p>
<ul>
<li><a href="https://k8s.devstats.cncf.io/d/11/companies-contributing-in-repository-groups?orgId=1&#x26;from=1747609200000&#x26;to=1756335599000&#x26;var-period=d28&#x26;var-repogroup_name=Kubernetes&#x26;var-repo_name=kubernetes%2Fkubernetes">Companies contributing to Kubernetes</a></li>
<li><a href="https://k8s.devstats.cncf.io/d/11/companies-contributing-in-repository-groups?orgId=1&#x26;from=1747609200000&#x26;to=1756335599000&#x26;var-period=d28&#x26;var-repogroup_name=All&#x26;var-repo_name=kubernetes%2Fkubernetes">Overall ecosystem contributions</a></li>
</ul>
<h2>Event Update</h2>
<p>Explore upcoming Kubernetes and cloud native events, including KubeCon + CloudNativeCon, KCD, and other notable conferences worldwide. Stay informed and get involved with the Kubernetes community!</p>
<p><strong>August 2025</strong></p>
<ul>
<li><a href="https://community.cncf.io/events/details/cncf-kcd-colombia-presents-kcd-colombia-2025/"><strong>KCD - Kubernetes Community Days: Colombia</strong></a>: Aug 28, 2025 | Bogotá, Colombia</li>
</ul>
<p><strong>September 2025</strong></p>
<ul>
<li><a href="https://community.cncf.io/events/details/cncf-cloud-native-sydney-presents-cloudcon-sydney-sydney-international-convention-centre-910-september/"><strong>CloudCon Sydney</strong></a>: Sep 9–10, 2025 | Sydney, Australia.</li>
<li><a href="https://community.cncf.io/events/details/cncf-kcd-sf-bay-area-presents-kcd-san-francisco-bay-area/"><strong>KCD - Kubernetes Community Days: San Francisco Bay Area</strong></a>: Sep 9, 2025 | San Francisco, USA</li>
<li><a href="https://community.cncf.io/events/details/cncf-kcd-washington-dc-presents-kcd-washington-dc-2025/"><strong>KCD - Kubernetes Community Days: Washington DC</strong></a>: Sep 16, 2025 | Washington, D.C., USA</li>
<li><a href="https://community.cncf.io/events/details/cncf-kcd-sofia-presents-kubernetes-community-days-sofia/"><strong>KCD - Kubernetes Community Days: Sofia</strong></a>: Sep 18, 2025 | Sofia, Bulgaria</li>
<li><a href="https://community.cncf.io/events/details/cncf-kcd-el-salvador-presents-kcd-el-salvador/"><strong>KCD - Kubernetes Community Days: El Salvador</strong></a>: Sep 20, 2025 | San Salvador, El Salvador</li>
</ul>
<p><strong>October 2025</strong></p>
<ul>
<li><a href="https://community.cncf.io/events/details/cncf-kcd-warsaw-presents-kcd-warsaw-2025/"><strong>KCD - Kubernetes Community Days: Warsaw</strong></a>: Oct 9, 2025 | Warsaw, Poland</li>
<li><a href="https://community.cncf.io/events/details/cncf-kcd-uk-presents-kubernetes-community-days-uk-edinburgh-2025/"><strong>KCD - Kubernetes Community Days: Edinburgh</strong></a>: Oct 21, 2025 | Edinburgh, United Kingdom</li>
<li><a href="https://community.cncf.io/events/details/cncf-kcd-sri-lanka-presents-kcd-sri-lanka-2025/"><strong>KCD - Kubernetes Community Days: Sri Lanka</strong></a>: Oct 26, 2025 | Colombo, Sri Lanka</li>
</ul>
<p><strong>November 2025</strong></p>
<ul>
<li><a href="https://community.cncf.io/events/details/cncf-kcd-porto-presents-kcd-porto-2025/"><strong>KCD - Kubernetes Community Days: Porto</strong></a>: Nov 3, 2025 | Porto, Portugal</li>
<li><a href="https://events.linuxfoundation.org/kubecon-cloudnativecon-north-america/"><strong>KubeCon + CloudNativeCon North America 2025</strong></a>: Nov 10-13, 2025 | Atlanta, USA</li>
<li><a href="https://sessionize.com/kcd-hangzhou-and-oicd-2025/"><strong>KCD - Kubernetes Community Days: Hangzhou</strong></a>: Nov 15, 2025 | Hangzhou, China</li>
</ul>
<p><strong>December 2025</strong></p>
<ul>
<li><a href="https://community.cncf.io/events/details/cncf-kcd-suisse-romande-presents-kcd-suisse-romande/"><strong>KCD - Kubernetes Community Days: Suisse Romande</strong></a>: Dec 4, 2025 | Geneva, Switzerland</li>
</ul>
<p>You can find the latest event details <a href="https://community.cncf.io/events/#/list">here</a>.</p>
<h2>Upcoming Release Webinar</h2>
<p>Join members of the Kubernetes v1.34 Release Team on <strong>Wednesday, September 24th 2025 at 4:00 PM (UTC)</strong>, to learn about the release highlights of this release. For more information and registration, visit the <a href="https://community.cncf.io/events/details/cncf-cncf-online-programs-presents-cloud-native-live-kubernetes-v134-release/">event page</a> on the CNCF Online Programs site.</p>
<h2>Get Involved</h2>
<p>The simplest way to get involved with Kubernetes is by joining one of the many <a href="https://github.com/kubernetes/community/blob/master/sig-list.md">Special Interest Groups</a> (SIGs) that align with your interests. Have something you’d like to broadcast to the Kubernetes community? Share your voice at our weekly <a href="https://github.com/kubernetes/community/tree/master/communication">community meeting</a>, and through the channels below. Thank you for your continued feedback and support.</p>
<ul>
<li>Follow us on Bluesky <a href="https://bsky.app/profile/kubernetes.io">@Kubernetesio</a> for the latest updates</li>
<li>Join the community discussion on <a href="https://discuss.kubernetes.io/">Discuss</a></li>
<li>Join the community on <a href="http://slack.k8s.io/">Slack</a></li>
<li>Post questions (or answer questions) on <a href="http://stackoverflow.com/questions/tagged/kubernetes">Stack Overflow</a></li>
<li>Share your Kubernetes <a href="https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform">story</a></li>
<li>Read more about what’s happening with Kubernetes on the <a href="https://kubernetes.io/blog/">blog</a></li>
<li>Learn more about the <a href="https://github.com/kubernetes/sig-release/tree/master/release-team">Kubernetes Release Team</a></li>
</ul>
</div></div></article></div></div></div></div></div></div></main><footer class="mt-32 flex-none"><div class="sm:px-8"><div class="mx-auto w-full max-w-7xl lg:px-8"><div class="border-t border-zinc-100 pb-16 pt-10 dark:border-zinc-700/40"><div class="relative px-4 sm:px-8 lg:px-12"><div class="mx-auto max-w-2xl lg:max-w-5xl"><div class="flex flex-col items-center justify-between gap-6 sm:flex-row"><div class="flex flex-wrap justify-center gap-x-6 gap-y-1 text-sm font-medium text-zinc-800 dark:text-zinc-200"><a class="transition hover:text-teal-500 dark:hover:text-teal-400" href="/about-me/">About</a><a class="transition hover:text-teal-500 dark:hover:text-teal-400" href="/projects/">Projects</a><a class="transition hover:text-teal-500 dark:hover:text-teal-400" href="/speaking/">Speaking</a></div><p class="text-sm text-zinc-500 dark:text-zinc-400">© <!-- -->2025<!-- --> Graziano Casto. All rights reserved.</p></div></div></div></div></div></div></footer></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"data":{"article":{"title":"Kubernetes v1.34: Of Wind \u0026 Will (O' WaW)","date":"2025-08-27"},"children":"\u003cp\u003e\u003ca href=\"https://kubernetes.io/blog/2025/08/27/kubernetes-v1-34-release/\"\u003ePublished by Kubernetes co-authored with Agustina Barbetta, Alejandro Josue Leon Bellido, Melony Qin, Dipesh Rawat\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSimilar to previous releases, the release of Kubernetes v1.34 introduces new stable, beta, and alpha features. The consistent delivery of high-quality releases underscores the strength of our development cycle and the vibrant support from our community.\u003c/p\u003e\n\u003cp\u003eThis release consists of 58 enhancements. Of those enhancements, 23 have graduated to Stable, 22 have entered Beta, and 13 have entered Alpha.\u003c/p\u003e\n\u003cp\u003eThere are also some \u003ca href=\"https://kubernetes.io/blog/2025/08/27/kubernetes-v1-34-release/#deprecations-and-removals\"\u003edeprecations and removals\u003c/a\u003e in this release; make sure to read about those.\u003c/p\u003e\n\u003ch2\u003eRelease theme and logo\u003c/h2\u003e\n\u003cp\u003e\u003cimg src=\"https://kubernetes.io/blog/2025/08/27/kubernetes-v1-34-release/k8s-v1.34.png\" alt=\"Kubernetes v1.34 logo: Three bears sail a wooden ship with a flag featuring a paw and a helm symbol on the sail, as wind blows across the ocean\"\u003e\u003c/p\u003e\n\u003cp\u003eKubernetes v1.34 logo: Three bears sail a wooden ship with a flag featuring a paw and a helm symbol on the sail, as wind blows across the ocean\u003c/p\u003e\n\u003cp\u003eA release powered by the wind around us — and the will within us.\u003c/p\u003e\n\u003cp\u003eEvery release cycle, we inherit winds that we don't really control — the state of our tooling, documentation, and the historical quirks of our project. Sometimes these winds fill our sails, sometimes they push us sideways or die down.\u003c/p\u003e\n\u003cp\u003eWhat keeps Kubernetes moving isn't the perfect winds, but the will of our sailors who adjust the sails, man the helm, chart the courses and keep the ship steady. The release happens not because conditions are always ideal, but because of the people who build it, the people who release it, and the bears ^, cats, dogs, wizards, and curious minds who keep Kubernetes sailing strong — no matter which way the wind blows.\u003c/p\u003e\n\u003cp\u003eThis release, \u003cstrong\u003eOf Wind \u0026#x26; Will (O' WaW)\u003c/strong\u003e, honors the winds that have shaped us, and the will that propels us forward.\u003c/p\u003e\n\u003cp\u003e^ Oh, and you wonder why bears? Keep wondering!\u003c/p\u003e\n\u003ch2\u003eSpotlight on key updates\u003c/h2\u003e\n\u003cp\u003eKubernetes v1.34 is packed with new features and improvements. Here are a few select updates the Release Team would like to highlight!\u003c/p\u003e\n\u003ch3\u003eStable: The core of DRA is GA\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://kubernetes.io/docs/concepts/scheduling-eviction/dynamic-resource-allocation/\"\u003eDynamic Resource Allocation\u003c/a\u003e (DRA) enables more powerful ways to select, allocate, share, and configure GPUs, TPUs, NICs and other devices.\u003c/p\u003e\n\u003cp\u003eSince the v1.30 release, DRA has been based around claiming devices using \u003cem\u003estructured parameters\u003c/em\u003e that are opaque to the core of Kubernetes. This enhancement took inspiration from dynamic provisioning for storage volumes. DRA with structured parameters relies on a set of supporting API kinds: ResourceClaim, DeviceClass, ResourceClaimTemplate, and ResourceSlice API types under \u003ccode\u003eresource.k8s.io\u003c/code\u003e, while extending the \u003ccode\u003e.spec\u003c/code\u003e for Pods with a new \u003ccode\u003eresourceClaims\u003c/code\u003e field.\u003cbr\u003e\nThe \u003ccode\u003eresource.k8s.io/v1\u003c/code\u003e APIs have graduated to stable and are now available by default.\u003c/p\u003e\n\u003cp\u003eThis work was done as part of \u003ca href=\"https://kep.k8s.io/4381\"\u003eKEP #4381\u003c/a\u003e led by WG Device Management.\u003c/p\u003e\n\u003ch3\u003eBeta: Projected ServiceAccount tokens for kubelet image credential providers\u003c/h3\u003e\n\u003cp\u003eThe \u003ccode\u003ekubelet\u003c/code\u003e credential providers, used for pulling private container images, traditionally relied on long-lived Secrets stored on the node or in the cluster. This approach increased security risks and management overhead, as these credentials were not tied to the specific workload and did not rotate automatically.\u003cbr\u003e\nTo solve this, the \u003ccode\u003ekubelet\u003c/code\u003e can now request short-lived, audience-bound ServiceAccount tokens for authenticating to container registries. This allows image pulls to be authorized based on the Pod's own identity rather than a node-level credential.\u003cbr\u003e\nThe primary benefit is a significant security improvement. It eliminates the need for long-lived Secrets for image pulls, reducing the attack surface and simplifying credential management for both administrators and developers.\u003c/p\u003e\n\u003cp\u003eThis work was done as part of \u003ca href=\"https://kep.k8s.io/4412\"\u003eKEP #4412\u003c/a\u003e led by SIG Auth and SIG Node.\u003c/p\u003e\n\u003ch3\u003eAlpha: Support for KYAML, a Kubernetes dialect of YAML\u003c/h3\u003e\n\u003cp\u003eKYAML aims to be a safer and less ambiguous YAML subset, and was designed specifically for Kubernetes. Whatever version of Kubernetes you use, starting from Kubernetes v1.34 you are able to use KYAML as a new output format for kubectl.\u003c/p\u003e\n\u003cp\u003eKYAML addresses specific challenges with both YAML and JSON. YAML's significant whitespace requires careful attention to indentation and nesting, while its optional string-quoting can lead to unexpected type coercion (for example: \u003ca href=\"https://hitchdev.com/strictyaml/why/implicit-typing-removed/\"\u003e\"The Norway Bug\"\u003c/a\u003e). Meanwhile, JSON lacks comment support and has strict requirements for trailing commas and quoted keys.\u003c/p\u003e\n\u003cp\u003eYou can write KYAML and pass it as an input to any version of \u003ccode\u003ekubectl\u003c/code\u003e, because all KYAML files are also valid as YAML. With \u003ccode\u003ekubectl\u003c/code\u003e v1.34, you are also able to \u003ca href=\"https://kubernetes.io/docs/reference/kubectl/#syntax-1\"\u003erequest KYAML output\u003c/a\u003e (as in kubectl get -o kyaml …) by setting environment variable \u003ccode\u003eKUBECTL_KYAML=true\u003c/code\u003e. If you prefer, you can still request the output in JSON or YAML format.\u003c/p\u003e\n\u003cp\u003eThis work was done as part of \u003ca href=\"https://kep.k8s.io/5295\"\u003eKEP #5295\u003c/a\u003e led by SIG CLI.\u003c/p\u003e\n\u003ch2\u003eFeatures graduating to Stable\u003c/h2\u003e\n\u003cp\u003e\u003cem\u003eThis is a selection of some of the improvements that are now stable following the v1.34 release.\u003c/em\u003e\u003c/p\u003e\n\u003ch3\u003eDelayed creation of Job’s replacement Pods\u003c/h3\u003e\n\u003cp\u003eBy default, Job controllers create replacement Pods immediately when a Pod starts terminating, causing both Pods to run simultaneously. This can cause resource contention in constrained clusters, where the replacement Pod may struggle to find available nodes until the original Pod fully terminates. The situation can also trigger unwanted cluster autoscaler scale-ups. Additionally, some machine learning frameworks like TensorFlow and \u003ca href=\"https://jax.readthedocs.io/en/latest/\"\u003eJAX\u003c/a\u003e require only one Pod per index to run at a time, making simultaneous Pod execution problematic. This feature introduces \u003ccode\u003e.spec.podReplacementPolicy\u003c/code\u003e in Jobs. You may choose to create replacement Pods only when the Pod is fully terminated (has \u003ccode\u003e.status.phase: Failed\u003c/code\u003e). To do this, set \u003ccode\u003e.spec.podReplacementPolicy: Failed\u003c/code\u003e.\u003cbr\u003e\nIntroduced as alpha in v1.28, this feature has graduated to stable in v1.34.\u003c/p\u003e\n\u003cp\u003eThis work was done as part of \u003ca href=\"https://kep.k8s.io/3939\"\u003eKEP #3939\u003c/a\u003e led by SIG Apps.\u003c/p\u003e\n\u003ch3\u003eRecovery from volume expansion failure\u003c/h3\u003e\n\u003cp\u003eThis feature allows users to cancel volume expansions that are unsupported by the underlying storage provider, and retry volume expansion with smaller values that may succeed.\u003cbr\u003e\nIntroduced as alpha in v1.23, this feature has graduated to stable in v1.34.\u003c/p\u003e\n\u003cp\u003eThis work was done as part of \u003ca href=\"https://kep.k8s.io/1790\"\u003eKEP #1790\u003c/a\u003e led by SIG Storage.\u003c/p\u003e\n\u003ch3\u003eVolumeAttributesClass for volume modification\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://kubernetes.io/docs/concepts/storage/volume-attributes-classes/\"\u003eVolumeAttributesClass\u003c/a\u003e has graduated to stable in v1.34. VolumeAttributesClass is a generic, Kubernetes-native API for modifying volume parameters like provisioned IO. It allows workloads to vertically scale their volumes on-line to balance cost and performance, if supported by their provider.\u003cbr\u003e\nLike all new volume features in Kubernetes, this API is implemented via the \u003ca href=\"https://kubernetes-csi.github.io/docs/\"\u003econtainer storage interface (CSI)\u003c/a\u003e. Your provisioner-specific CSI driver must support the new ModifyVolume API which is the CSI side of this feature.\u003c/p\u003e\n\u003cp\u003eThis work was done as part of \u003ca href=\"https://kep.k8s.io/3751\"\u003eKEP #3751\u003c/a\u003e led by SIG Storage.\u003c/p\u003e\n\u003ch3\u003eStructured authentication configuration\u003c/h3\u003e\n\u003cp\u003eKubernetes v1.29 introduced a configuration file format to manage API server client authentication, moving away from the previous reliance on a large set of command-line options. The \u003ca href=\"https://kubernetes.io/docs/reference/access-authn-authz/authentication/#using-authentication-configuration\"\u003eAuthenticationConfiguration\u003c/a\u003e kind allows administrators to support multiple JWT authenticators, CEL expression validation, and dynamic reloading. This change significantly improves the manageability and auditability of the cluster's authentication settings - and has graduated to stable in v1.34.\u003c/p\u003e\n\u003cp\u003eThis work was done as part of \u003ca href=\"https://kep.k8s.io/3331\"\u003eKEP #3331\u003c/a\u003e led by SIG Auth.\u003c/p\u003e\n\u003ch3\u003eFiner-grained authorization based on selectors\u003c/h3\u003e\n\u003cp\u003eKubernetes authorizers, including webhook authorizers and the built-in node authorizer, can now make authorization decisions based on field and label selectors in incoming requests. When you send \u003cstrong\u003elist\u003c/strong\u003e, \u003cstrong\u003ewatch\u003c/strong\u003e or \u003cstrong\u003edeletecollection\u003c/strong\u003e requests with selectors, the authorization layer can now evaluate access with that additional context.\u003c/p\u003e\n\u003cp\u003eFor example, you can write an authorization policy that only allows listing Pods bound to a specific \u003ccode\u003e.spec.nodeName\u003c/code\u003e. The client (perhaps the kubelet on a particular node) must specify the field selector that the policy requires, otherwise the request is forbidden. This change makes it feasible to set up least privilege rules, provided that the client knows how to conform to the restrictions you set. Kubernetes v1.34 now supports more granular control in environments like per-node isolation or custom multi-tenant setups.\u003c/p\u003e\n\u003cp\u003eThis work was done as part of \u003ca href=\"https://kep.k8s.io/4601\"\u003eKEP #4601\u003c/a\u003e led by SIG Auth.\u003c/p\u003e\n\u003ch3\u003eRestrict anonymous requests with fine-grained controls\u003c/h3\u003e\n\u003cp\u003eInstead of fully enabling or disabling anonymous access, you can now configure a strict list of endpoints where unauthenticated requests are allowed. This provides a safer alternative for clusters that rely on anonymous access to health or bootstrap endpoints like \u003ccode\u003e/healthz\u003c/code\u003e, \u003ccode\u003e/readyz\u003c/code\u003e, or \u003ccode\u003e/livez\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eWith this feature, accidental RBAC misconfigurations that grant broad access to anonymous users can be avoided without requiring changes to external probes or bootstrapping tools.\u003c/p\u003e\n\u003cp\u003eThis work was done as part of \u003ca href=\"https://kep.k8s.io/4633\"\u003eKEP #4633\u003c/a\u003e led by SIG Auth.\u003c/p\u003e\n\u003cp\u003eThe \u003ccode\u003ekube-scheduler\u003c/code\u003e can now make more accurate decisions about when to retry scheduling Pods that were previously unschedulable. Each scheduling plugin can now register callback functions that tell the scheduler whether an incoming cluster event is likely to make a rejected Pod schedulable again.\u003c/p\u003e\n\u003cp\u003eThis reduces unnecessary retries and improves overall scheduling throughput - especially in clusters using dynamic resource allocation. The feature also lets certain plugins skip the usual backoff delay when it is safe to do so, making scheduling faster in specific cases.\u003c/p\u003e\n\u003cp\u003eThis work was done as part of \u003ca href=\"https://kep.k8s.io/4247\"\u003eKEP #4247\u003c/a\u003e led by SIG Scheduling.\u003c/p\u003e\n\u003ch3\u003eOrdered Namespace deletion\u003c/h3\u003e\n\u003cp\u003eSemi-random resource deletion order can create security gaps or unintended behavior, such as Pods persisting after their associated NetworkPolicies are deleted.\u003cbr\u003e\nThis improvement introduces a more structured deletion process for Kubernetes \u003ca href=\"https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/\"\u003enamespaces\u003c/a\u003e to ensure secure and deterministic resource removal. By enforcing a structured deletion sequence that respects logical and security dependencies, this approach ensures Pods are removed before other resources.\u003cbr\u003e\nThis feature was introduced in Kubernetes v1.33 and graduated to stable in v1.34. The graduation improves security and reliability by mitigating risks from non-deterministic deletions, including the vulnerability described in \u003ca href=\"https://github.com/advisories/GHSA-r56h-j38w-hrqq\"\u003eCVE-2024-7598\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eThis work was done as part of \u003ca href=\"https://kep.k8s.io/5080\"\u003eKEP #5080\u003c/a\u003e led by SIG API Machinery.\u003c/p\u003e\n\u003ch3\u003eStreaming list responses\u003c/h3\u003e\n\u003cp\u003eHandling large \u003cstrong\u003elist\u003c/strong\u003e responses in Kubernetes previously posed a significant scalability challenge. When clients requested extensive resource lists, such as thousands of Pods or Custom Resources, the API server was required to serialize the entire collection of objects into a single, large memory buffer before sending it. This process created substantial memory pressure and could lead to performance degradation, impacting the overall stability of the cluster.\u003cbr\u003e\nTo address this limitation, a streaming encoding mechanism for collections (list responses) has been introduced. For the JSON and Kubernetes Protobuf response formats, that streaming mechanism is automatically active and the associated feature gate is stable. The primary benefit of this approach is the avoidance of large memory allocations on the API server, resulting in a much smaller and more predictable memory footprint. Consequently, the cluster becomes more resilient and performant, especially in large-scale environments where frequent requests for extensive resource lists are common.\u003c/p\u003e\n\u003cp\u003eThis work was done as part of \u003ca href=\"https://kep.k8s.io/5116\"\u003eKEP #5116\u003c/a\u003e led by SIG API Machinery.\u003c/p\u003e\n\u003ch3\u003eResilient watch cache initialization\u003c/h3\u003e\n\u003cp\u003eWatch cache is a caching layer inside \u003ccode\u003ekube-apiserver\u003c/code\u003e that maintains an eventually consistent cache of cluster state stored in etcd. In the past, issues could occur when the watch cache was not yet initialized during \u003ccode\u003ekube-apiserver\u003c/code\u003e startup or when it required re-initialization.\u003c/p\u003e\n\u003cp\u003eTo address these issues, the watch cache initialization process has been made more resilient to failures, improving control plane robustness and ensuring controllers and clients can reliably establish watches. This improvement was introduced as beta in v1.31 and is now stable.\u003c/p\u003e\n\u003cp\u003eThis work was done as part of \u003ca href=\"https://kep.k8s.io/4568\"\u003eKEP #4568\u003c/a\u003e led by SIG API Machinery and SIG Scalability.\u003c/p\u003e\n\u003cp\u003ePreviously, the strict validation of a Pod's DNS \u003ccode\u003esearch\u003c/code\u003e path in Kubernetes often created integration challenges in complex or legacy network environments. This restrictiveness could block configurations that were necessary for an organization's infrastructure, forcing administrators to implement difficult workarounds.\u003cbr\u003e\nTo address this, relaxed DNS validation was introduced as alpha in v1.32 and has now graduated to stable in v1.34. A common use case involves Pods that need to communicate with both internal Kubernetes services and external domains. By setting a single dot (\u003ccode\u003e.\u003c/code\u003e) as the first entry in the \u003ccode\u003esearches\u003c/code\u003e list of the Pod's \u003ccode\u003e.spec.dnsConfig\u003c/code\u003e, administrators can prevent the system's resolver from appending the cluster's internal search domains to external queries. This avoids generating unnecessary DNS requests to the internal DNS server for external hostnames, improving efficiency and preventing potential resolution errors.\u003c/p\u003e\n\u003cp\u003eThis work was done as part of \u003ca href=\"https://kep.k8s.io/4427\"\u003eKEP #4427\u003c/a\u003e led by SIG Network.\u003c/p\u003e\n\u003ch3\u003eSupport for Direct Service Return (DSR) in Windows kube-proxy\u003c/h3\u003e\n\u003cp\u003eDSR provides performance optimizations by allowing return traffic routed through load balancers to bypass the load balancer and respond directly to the client, reducing load on the load balancer and improving overall latency. For information on DSR on Windows, read \u003ca href=\"https://techcommunity.microsoft.com/blog/networkingblog/direct-server-return-dsr-in-a-nutshell/693710\"\u003eDirect Server Return (DSR) in a nutshell\u003c/a\u003e.\u003cbr\u003e\nInitially introduced in v1.14, this feature has graduated to stable in v1.34.\u003c/p\u003e\n\u003cp\u003eThis work was done as part of \u003ca href=\"https://kep.k8s.io/5100\"\u003eKEP #5100\u003c/a\u003e led by SIG Windows.\u003c/p\u003e\n\u003ch3\u003eSleep action for Container lifecycle hooks\u003c/h3\u003e\n\u003cp\u003eA Sleep action for containers’ PreStop and PostStart lifecycle hooks was introduced to provide a straightforward way to manage graceful shutdowns and improve overall container lifecycle management.\u003cbr\u003e\nThe Sleep action allows containers to pause for a specified duration after starting or before termination. Using a negative or zero sleep duration returns immediately, resulting in a no-op.\u003cbr\u003e\nThe Sleep action was introduced in Kubernetes v1.29, with zero value support added in v1.32. Both features graduated to stable in v1.34.\u003c/p\u003e\n\u003cp\u003eThis work was done as part of \u003ca href=\"https://kep.k8s.io/3960\"\u003eKEP #3960\u003c/a\u003e and \u003ca href=\"https://kep.k8s.io/4818\"\u003eKEP #4818\u003c/a\u003e led by SIG Node.\u003c/p\u003e\n\u003ch3\u003eLinux node swap support\u003c/h3\u003e\n\u003cp\u003eHistorically, the lack of swap support in Kubernetes could lead to workload instability, as nodes under memory pressure often had to terminate processes abruptly. This particularly affected applications with large but infrequently accessed memory footprints and prevented more graceful resource management.\u003c/p\u003e\n\u003cp\u003eTo address this, configurable per-node swap support was introduced in v1.22. It has progressed through alpha and beta stages and has graduated to stable in v1.34. The primary mode, \u003ccode\u003eLimitedSwap\u003c/code\u003e, allows Pods to use swap within their existing memory limits, providing a direct solution to the problem. By default, the \u003ccode\u003ekubelet\u003c/code\u003e is configured with \u003ccode\u003eNoSwap\u003c/code\u003e mode, which means Kubernetes workloads cannot use swap.\u003c/p\u003e\n\u003cp\u003eThis feature improves workload stability and allows for more efficient resource utilization. It enables clusters to support a wider variety of applications, especially in resource-constrained environments, though administrators must consider the potential performance impact of swapping.\u003c/p\u003e\n\u003cp\u003eThis work was done as part of \u003ca href=\"https://kep.k8s.io/2400\"\u003eKEP #2400\u003c/a\u003e led by SIG Node.\u003c/p\u003e\n\u003ch3\u003eAllow special characters in environment variables\u003c/h3\u003e\n\u003cp\u003eThe environment variable validation rules in Kubernetes have been relaxed to allow nearly all printable ASCII characters in variable names, excluding \u003ccode\u003e=\u003c/code\u003e. This change supports scenarios where workloads require nonstandard characters in variable names - for example, frameworks like.NET Core that use \u003ccode\u003e:\u003c/code\u003e to represent nested configuration keys.\u003c/p\u003e\n\u003cp\u003eThe relaxed validation applies to environment variables defined directly in Pod spec, as well as those injected using \u003ccode\u003eenvFrom\u003c/code\u003e references to ConfigMaps and Secrets.\u003c/p\u003e\n\u003cp\u003eThis work was done as part of \u003ca href=\"https://kep.k8s.io/4369\"\u003eKEP #4369\u003c/a\u003e led by SIG Node.\u003c/p\u003e\n\u003ch3\u003eTaint management is separated from Node lifecycle\u003c/h3\u003e\n\u003cp\u003eHistorically, the \u003ccode\u003eTaintManager\u003c/code\u003e 's logic for applying NoSchedule and NoExecute taints to nodes based on their condition (NotReady, Unreachable, etc.) was tightly coupled with the node lifecycle controller. This tight coupling made the code harder to maintain and test, and it also limited the flexibility of the taint-based eviction mechanism. This KEP refactors the \u003ccode\u003eTaintManager\u003c/code\u003e into its own separate controller within the Kubernetes controller manager. It is an internal architectural improvement designed to increase code modularity and maintainability. This change allows the logic for taint-based evictions to be tested and evolved independently, but it has no direct user-facing impact on how taints are used.\u003c/p\u003e\n\u003cp\u003eThis work was done as part of \u003ca href=\"https://kep.k8s.io/3902\"\u003eKEP #3902\u003c/a\u003e led by SIG Scheduling and SIG Node.\u003c/p\u003e\n\u003ch2\u003eNew features in Beta\u003c/h2\u003e\n\u003cp\u003e\u003cem\u003eThis is a selection of some of the improvements that are now beta following the v1.34 release.\u003c/em\u003e\u003c/p\u003e\n\u003ch3\u003ePod-level resource requests and limits\u003c/h3\u003e\n\u003cp\u003eDefining resource needs for Pods with multiple containers has been challenging, as requests and limits could only be set on a per-container basis. This forced developers to either over-provision resources for each container or meticulously divide the total desired resources, making configuration complex and often leading to inefficient resource allocation. To simplify this, the ability to specify resource requests and limits at the Pod level was introduced. This allows developers to define an overall resource budget for a Pod, which is then shared among its constituent containers. This feature was introduced as alpha in v1.32 and has graduated to beta in v1.34, with HPA now supporting pod-level resource specifications.\u003c/p\u003e\n\u003cp\u003eThe primary benefit is a more intuitive and straightforward way to manage resources for multi-container Pods. It ensures that the total resources used by all containers do not exceed the Pod's defined limits, leading to better resource planning, more accurate scheduling, and more efficient utilization of cluster resources.\u003c/p\u003e\n\u003cp\u003eThis work was done as part of \u003ca href=\"https://kep.k8s.io/2837\"\u003eKEP #2837\u003c/a\u003e led by SIG Scheduling and SIG Autoscaling.\u003c/p\u003e\n\u003ch3\u003e.kuberc file for kubectl user preferences\u003c/h3\u003e\n\u003cp\u003eA \u003ccode\u003e.kuberc\u003c/code\u003e configuration file allows you to define preferences for \u003ccode\u003ekubectl\u003c/code\u003e, such as default options and command aliases. Unlike the kubeconfig file, the \u003ccode\u003e.kuberc\u003c/code\u003e configuration file does not contain cluster details, usernames or passwords.\u003cbr\u003e\nThis feature was introduced as alpha in v1.33, gated behind the environment variable \u003ccode\u003eKUBECTL_KUBERC\u003c/code\u003e. It has graduated to beta in v1.34 and is enabled by default.\u003c/p\u003e\n\u003cp\u003eThis work was done as part of \u003ca href=\"https://kep.k8s.io/3104\"\u003eKEP #3104\u003c/a\u003e led by SIG CLI.\u003c/p\u003e\n\u003ch3\u003eExternal ServiceAccount token signing\u003c/h3\u003e\n\u003cp\u003eTraditionally, Kubernetes manages ServiceAccount tokens using static signing keys that are loaded from disk at \u003ccode\u003ekube-apiserver\u003c/code\u003e startup. This feature introduces an \u003ccode\u003eExternalJWTSigner\u003c/code\u003e gRPC service for out-of-process signing, enabling Kubernetes distributions to integrate with external key management solutions (for example, HSMs, cloud KMSes) for ServiceAccount token signing instead of static disk-based keys.\u003c/p\u003e\n\u003cp\u003eIntroduced as alpha in v1.32, this external JWT signing capability advances to beta and is enabled by default in v1.34.\u003c/p\u003e\n\u003cp\u003eThis work was done as part of \u003ca href=\"https://kep.k8s.io/740\"\u003eKEP #740\u003c/a\u003e led by SIG Auth.\u003c/p\u003e\n\u003ch3\u003eDRA features in beta\u003c/h3\u003e\n\u003ch4\u003eAdmin access for secure resource monitoring\u003c/h4\u003e\n\u003cp\u003eDRA supports controlled administrative access via the \u003ccode\u003eadminAccess\u003c/code\u003e field in ResourceClaims or ResourceClaimTemplates, allowing cluster operators to access devices already in use by others for monitoring or diagnostics. This privileged mode is limited to users authorized to create such objects in namespaces labeled \u003ccode\u003eresource.k8s.io/admin-access: \"true\"\u003c/code\u003e, ensuring regular workloads remain unaffected. Graduating to beta in v1.34, this feature provides secure introspection capabilities while preserving workload isolation through namespace-based authorization checks.\u003c/p\u003e\n\u003cp\u003eThis work was done as part of \u003ca href=\"https://kep.k8s.io/5018\"\u003eKEP #5018\u003c/a\u003e led by WG Device Management and SIG Auth.\u003c/p\u003e\n\u003ch4\u003ePrioritized alternatives in ResourceClaims and ResourceClaimTemplates\u003c/h4\u003e\n\u003cp\u003eWhile a workload might run best on a single high-performance GPU, it might also be able to run on two mid-level GPUs.\u003cbr\u003e\nWith the feature gate \u003ccode\u003eDRAPrioritizedList\u003c/code\u003e (now enabled by default), ResourceClaims and ResourceClaimTemplates get a new field named \u003ccode\u003efirstAvailable\u003c/code\u003e. This field is an ordered list that allows users to specify that a request may be satisfied in different ways, including allocating nothing at all if specific hardware is not available. The scheduler will attempt to satisfy the alternatives in the list in order, so the workload will be allocated the best set of devices available in the cluster.\u003c/p\u003e\n\u003cp\u003eThis work was done as part of \u003ca href=\"https://kep.k8s.io/4816\"\u003eKEP #4816\u003c/a\u003e led by WG Device Management.\u003c/p\u003e\n\u003ch4\u003eThe kubelet reports allocated DRA resources\u003c/h4\u003e\n\u003cp\u003eThe \u003ccode\u003ekubelet\u003c/code\u003e 's API has been updated to report on Pod resources allocated through DRA. This allows node monitoring agents to discover the allocated DRA resources for Pods on a node. Additionally, it enables node components to use the PodResourcesAPI and leverage this DRA information when developing new features and integrations.\u003cbr\u003e\nStarting from Kubernetes v1.34, this feature is enabled by default.\u003c/p\u003e\n\u003cp\u003eThis work was done as part of \u003ca href=\"https://kep.k8s.io/3695\"\u003eKEP #3695\u003c/a\u003e led by WG Device Management.\u003c/p\u003e\n\u003ch3\u003ekube-scheduler non-blocking API calls\u003c/h3\u003e\n\u003cp\u003eThe \u003ccode\u003ekube-scheduler\u003c/code\u003e makes blocking API calls during scheduling cycles, creating performance bottlenecks. This feature introduces asynchronous API handling through a prioritized queue system with request deduplication, allowing the scheduler to continue processing Pods while API operations complete in the background. Key benefits include reduced scheduling latency, prevention of scheduler thread starvation during API delays, and immediate retry capability for unschedulable Pods. The implementation maintains backward compatibility and adds metrics for monitoring pending API operations.\u003c/p\u003e\n\u003cp\u003eThis work was done as part of \u003ca href=\"https://kep.k8s.io/5229\"\u003eKEP #5229\u003c/a\u003e led by SIG Scheduling.\u003c/p\u003e\n\u003ch3\u003eMutating admission policies\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://kubernetes.io/docs/reference/access-authn-authz/mutating-admission-policy/\"\u003eMutatingAdmissionPolicies\u003c/a\u003e offer a declarative, in-process alternative to mutating admission webhooks. This feature leverages CEL's object instantiation and JSON Patch strategies, combined with Server Side Apply’s merge algorithms.\u003cbr\u003e\nThis significantly simplifies admission control by allowing administrators to define mutation rules directly in the API server.\u003cbr\u003e\nIntroduced as alpha in v1.32, mutating admission policies has graduated to beta in v1.34.\u003c/p\u003e\n\u003cp\u003eThis work was done as part of \u003ca href=\"https://kep.k8s.io/3962\"\u003eKEP #3962\u003c/a\u003e led by SIG API Machinery.\u003c/p\u003e\n\u003ch3\u003eSnapshottable API server cache\u003c/h3\u003e\n\u003cp\u003eThe \u003ccode\u003ekube-apiserver\u003c/code\u003e 's caching mechanism (watch cache) efficiently serves requests for the latest observed state. However, \u003cstrong\u003elist\u003c/strong\u003e requests for previous states (for example, via pagination or by specifying a \u003ccode\u003eresourceVersion\u003c/code\u003e) often bypass this cache and are served directly from etcd. This direct etcd access significantly increases performance costs and can lead to stability issues, particularly with large resources, due to memory pressure from transferring large data blobs.\u003cbr\u003e\nWith the \u003ccode\u003eListFromCacheSnapshot\u003c/code\u003e feature gate enabled by default, \u003ccode\u003ekube-apiserver\u003c/code\u003e will attempt to serve the response from snapshots if one is available with \u003ccode\u003eresourceVersion\u003c/code\u003e older than requested. The \u003ccode\u003ekube-apiserver\u003c/code\u003e starts with no snapshots, creates a new snapshot on every watch event, and keeps them until it detects etcd is compacted or if cache is full with events older than 75 seconds. If the provided \u003ccode\u003eresourceVersion\u003c/code\u003e is unavailable, the server will fallback to etcd.\u003c/p\u003e\n\u003cp\u003eThis work was done as part of \u003ca href=\"https://kep.k8s.io/4988\"\u003eKEP #4988\u003c/a\u003e led by SIG API Machinery.\u003c/p\u003e\n\u003ch3\u003eTooling for declarative validation of Kubernetes-native types\u003c/h3\u003e\n\u003cp\u003ePrior to this release, validation rules for the APIs built into Kubernetes were written entirely by hand, which makes them difficult for maintainers to discover, understand, improve or test. There was no single way to find all the validation rules that might apply to an API.\u003cem\u003eDeclarative validation\u003c/em\u003e benefits Kubernetes maintainers by making API development, maintenance, and review easier while enabling programmatic inspection for better tooling and documentation. For people using Kubernetes libraries to write their own code (for example: a controller), the new approach streamlines adding new fields through IDL tags, rather than complex validation functions. This change helps speed up API creation by automating validation boilerplate, and provides more relevant error messages by performing validation on versioned types.\u003cbr\u003e\nThis enhancement (which graduated to beta in v1.33 and continues as beta in v1.34) brings CEL-based validation rules to native Kubernetes types. It allows for more granular and declarative validation to be defined directly in the type definitions, improving API consistency and developer experience.\u003c/p\u003e\n\u003cp\u003eThis work was done as part of \u003ca href=\"https://kep.k8s.io/5073\"\u003eKEP #5073\u003c/a\u003e led by SIG API Machinery.\u003c/p\u003e\n\u003ch3\u003eStreaming informers for list requests\u003c/h3\u003e\n\u003cp\u003eThe streaming informers feature, which has been in beta since v1.32, gains further beta refinements in v1.34. This capability allows \u003cstrong\u003elist\u003c/strong\u003e requests to return data as a continuous stream of objects from the API server’s watch cache, rather than assembling paged results directly from etcd. By reusing the same mechanics used for \u003cstrong\u003ewatch\u003c/strong\u003e operations, the API server can serve large datasets while keeping memory usage steady and avoiding allocation spikes that can affect stability.\u003c/p\u003e\n\u003cp\u003eIn this release, the \u003ccode\u003ekube-apiserver\u003c/code\u003e and \u003ccode\u003ekube-controller-manager\u003c/code\u003e both take advantage of the new \u003ccode\u003eWatchList\u003c/code\u003e mechanism by default. For the \u003ccode\u003ekube-apiserver\u003c/code\u003e, this means list requests are streamed more efficiently, while the \u003ccode\u003ekube-controller-manager\u003c/code\u003e benefits from a more memory-efficient and predictable way to work with informers. Together, these improvements reduce memory pressure during large list operations, and improve reliability under sustained load, making list streaming more predictable and efficient.\u003c/p\u003e\n\u003cp\u003eThis work was done as part of \u003ca href=\"https://kep.k8s.io/3157\"\u003eKEP #3157\u003c/a\u003e led by SIG API Machinery and SIG Scalability.\u003c/p\u003e\n\u003ch3\u003eGraceful node shutdown handling for Windows nodes\u003c/h3\u003e\n\u003cp\u003eThe \u003ccode\u003ekubelet\u003c/code\u003e on Windows nodes can now detect system shutdown events and begin graceful termination of running Pods. This mirrors existing behavior on Linux and helps ensure workloads exit cleanly during planned shutdowns or restarts.\u003cbr\u003e\nWhen the system begins shutting down, the \u003ccode\u003ekubelet\u003c/code\u003e reacts by using standard termination logic. It respects the configured lifecycle hooks and grace periods, giving Pods time to stop before the node powers off. The feature relies on Windows pre-shutdown notifications to coordinate this process. This enhancement improves workload reliability during maintenance, restarts, or system updates. It is now in beta and enabled by default.\u003c/p\u003e\n\u003cp\u003eThis work was done as part of \u003ca href=\"https://kep.k8s.io/4802\"\u003eKEP #4802\u003c/a\u003e led by SIG Windows.\u003c/p\u003e\n\u003ch3\u003eIn-place Pod resize improvements\u003c/h3\u003e\n\u003cp\u003eGraduated to beta and enabled by default in v1.33, in-place Pod resizing receives further improvements in v1.34. These include support for decreasing memory usage and integration with Pod-level resources.\u003c/p\u003e\n\u003cp\u003eThis feature remains in beta in v1.34. For detailed usage instructions and examples, refer to the documentation: \u003ca href=\"https://kubernetes.io/docs/tasks/configure-pod-container/resize-container-resources/\"\u003eResize CPU and Memory Resources assigned to Containers\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eThis work was done as part of \u003ca href=\"https://kep.k8s.io/1287\"\u003eKEP #1287\u003c/a\u003e led by SIG Node and SIG Autoscaling.\u003c/p\u003e\n\u003ch2\u003eNew features in Alpha\u003c/h2\u003e\n\u003cp\u003e\u003cem\u003eThis is a selection of some of the improvements that are now alpha following the v1.34 release.\u003c/em\u003e\u003c/p\u003e\n\u003ch3\u003ePod certificates for mTLS authentication\u003c/h3\u003e\n\u003cp\u003eAuthenticating workloads within a cluster, especially for communication with the API server, has primarily relied on ServiceAccount tokens. While effective, these tokens aren't always ideal for establishing a strong, verifiable identity for mutual TLS (mTLS) and can present challenges when integrating with external systems that expect certificate-based authentication.\u003cbr\u003e\nKubernetes v1.34 introduces a built-in mechanism for Pods to obtain X.509 certificates via \u003ca href=\"https://kubernetes.io/docs/reference/access-authn-authz/certificate-signing-requests/#pod-certificate-requests\"\u003ePodCertificateRequests\u003c/a\u003e. The \u003ccode\u003ekubelet\u003c/code\u003e can request and manage certificates for Pods, which can then be used to authenticate to the Kubernetes API server and other services using mTLS. The primary benefit is a more robust and flexible identity mechanism for Pods. It provides a native way to implement strong mTLS authentication without relying solely on bearer tokens, aligning Kubernetes with standard security practices and simplifying integrations with certificate-aware observability and security tooling.\u003c/p\u003e\n\u003cp\u003eThis work was done as part of \u003ca href=\"https://kep.k8s.io/4317\"\u003eKEP #4317\u003c/a\u003e led by SIG Auth.\u003c/p\u003e\n\u003ch3\u003e\"Restricted\" Pod security standard now forbids remote probes\u003c/h3\u003e\n\u003cp\u003eThe \u003ccode\u003ehost\u003c/code\u003e field within probes and lifecycle handlers allows users to specify an entity other than the \u003ccode\u003epodIP\u003c/code\u003e for the \u003ccode\u003ekubelet\u003c/code\u003e to probe. However, this opens up a route for misuse and for attacks that bypass security controls, since the \u003ccode\u003ehost\u003c/code\u003e field could be set to \u003cstrong\u003eany\u003c/strong\u003e value, including security sensitive external hosts, or localhost on the node. In Kubernetes v1.34, Pods only meet the \u003ca href=\"https://kubernetes.io/docs/concepts/security/pod-security-standards/#restricted\"\u003eRestricted\u003c/a\u003e Pod security standard if they either leave the \u003ccode\u003ehost\u003c/code\u003e field unset, or if they don't even use this kind of probe. You can use \u003cem\u003ePod security admission\u003c/em\u003e, or a third party solution, to enforce that Pods meet this standard. Because these are security controls, check the documentation to understand the limitations and behavior of the enforcement mechanism you choose.\u003c/p\u003e\n\u003cp\u003eThis work was done as part of \u003ca href=\"https://kep.k8s.io/4940\"\u003eKEP #4940\u003c/a\u003e led by SIG Auth.\u003c/p\u003e\n\u003ch3\u003eUse.status.nominatedNodeName to express Pod placement\u003c/h3\u003e\n\u003cp\u003eWhen the \u003ccode\u003ekube-scheduler\u003c/code\u003e takes time to bind Pods to Nodes, cluster autoscalers may not understand that a Pod will be bound to a specific Node. Consequently, they may mistakenly consider the Node as underutilized and delete it.\u003cbr\u003e\nTo address this issue, the \u003ccode\u003ekube-scheduler\u003c/code\u003e can use \u003ccode\u003e.status.nominatedNodeName\u003c/code\u003e not only to indicate ongoing preemption but also to express Pod placement intentions. By enabling the \u003ccode\u003eNominatedNodeNameForExpectation\u003c/code\u003e feature gate, the scheduler uses this field to indicate where a Pod will be bound. This exposes internal reservations to help external components make informed decisions.\u003c/p\u003e\n\u003cp\u003eThis work was done as part of \u003ca href=\"https://kep.k8s.io/5278\"\u003eKEP #5278\u003c/a\u003e led by SIG Scheduling.\u003c/p\u003e\n\u003ch3\u003eDRA features in alpha\u003c/h3\u003e\n\u003ch4\u003eResource health status for DRA\u003c/h4\u003e\n\u003cp\u003eIt can be difficult to know when a Pod is using a device that has failed or is temporarily unhealthy, which makes troubleshooting Pod crashes challenging or impossible.\u003cbr\u003e\nResource Health Status for DRA improves observability by exposing the health status of devices allocated to a Pod in the Pod’s status. This makes it easier to identify the cause of Pod issues related to unhealthy devices and respond appropriately.\u003cbr\u003e\nTo enable this functionality, the \u003ccode\u003eResourceHealthStatus\u003c/code\u003e feature gate must be enabled, and the DRA driver must implement the \u003ccode\u003eDRAResourceHealth\u003c/code\u003e gRPC service.\u003c/p\u003e\n\u003cp\u003eThis work was done as part of \u003ca href=\"https://kep.k8s.io/4680\"\u003eKEP #4680\u003c/a\u003e led by WG Device Management.\u003c/p\u003e\n\u003ch4\u003eExtended resource mapping\u003c/h4\u003e\n\u003cp\u003eExtended resource mapping provides a simpler alternative to DRA's expressive and flexible approach by offering a straightforward way to describe resource capacity and consumption. This feature enables cluster administrators to advertise DRA-managed resources as \u003cem\u003eextended resources\u003c/em\u003e, allowing application developers and operators to continue using the familiar container’s \u003ccode\u003e.spec.resources\u003c/code\u003e syntax to consume them.\u003cbr\u003e\nThis enables existing workloads to adopt DRA without modifications, simplifying the transition to DRA for both application developers and cluster administrators.\u003c/p\u003e\n\u003cp\u003eThis work was done as part of \u003ca href=\"https://kep.k8s.io/5004\"\u003eKEP #5004\u003c/a\u003e led by WG Device Management.\u003c/p\u003e\n\u003ch4\u003eDRA consumable capacity\u003c/h4\u003e\n\u003cp\u003eKubernetes v1.33 added support for resource drivers to advertise slices of a device that are available, rather than exposing the entire device as an all-or-nothing resource. However, this approach couldn't handle scenarios where device drivers manage fine-grained, dynamic portions of a device resource based on user demand, or share those resources independently of ResourceClaims, which are restricted by their spec and namespace.\u003cbr\u003e\nEnabling the \u003ccode\u003eDRAConsumableCapacity\u003c/code\u003e feature gate (introduced as alpha in v1.34) allows resource drivers to share the same device, or even a slice of a device, across multiple ResourceClaims or across multiple DeviceRequests. The feature also extends the scheduler to support allocating portions of device resources, as defined in the \u003ccode\u003ecapacity\u003c/code\u003e field. This DRA feature improves device sharing across namespaces and claims, tailoring it to Pod needs. It enables drivers to enforce capacity limits, enhances scheduling, and supports new use cases like bandwidth-aware networking and multi-tenant sharing.\u003c/p\u003e\n\u003cp\u003eThis work was done as part of \u003ca href=\"https://kep.k8s.io/5075\"\u003eKEP #5075\u003c/a\u003e led by WG Device Management.\u003c/p\u003e\n\u003ch4\u003eDevice binding conditions\u003c/h4\u003e\n\u003cp\u003eThe Kubernetes scheduler gets more reliable by delaying binding a Pod to a Node until its required external resources, such as attachable devices or FPGAs, are confirmed to be ready.\u003cbr\u003e\nThis delay mechanism is implemented in the \u003ca href=\"https://kubernetes.io/docs/concepts/scheduling-eviction/scheduling-framework/#pre-bind\"\u003ePreBind phase\u003c/a\u003e of the scheduling framework. During this phase, the scheduler checks whether all required device conditions are satisfied before proceeding with binding. This enables coordination with external device controllers, ensuring more robust, predictable scheduling.\u003c/p\u003e\n\u003cp\u003eThis work was done as part of \u003ca href=\"https://kep.k8s.io/5007\"\u003eKEP #5007\u003c/a\u003e led by WG Device Management.\u003c/p\u003e\n\u003ch3\u003eContainer restart rules\u003c/h3\u003e\n\u003cp\u003eCurrently, all containers within a Pod will follow the same \u003ccode\u003e.spec.restartPolicy\u003c/code\u003e when exited or crashed. However, Pods that run multiple containers might have different restart requirements for each container. For example, for init containers used to perform initialization, you may not want to retry initialization if they fail. Similarly, in ML research environments with long-running training workloads, containers that fail with retriable exit codes should restart quickly in place, rather than triggering Pod recreation and losing progress.\u003cbr\u003e\nKubernetes v1.34 introduces the \u003ccode\u003eContainerRestartRules\u003c/code\u003e feature gate. When enabled, a \u003ccode\u003erestartPolicy\u003c/code\u003e can be specified for each container within a Pod. A \u003ccode\u003erestartPolicyRules\u003c/code\u003e list can also be defined to override \u003ccode\u003erestartPolicy\u003c/code\u003e based on the last exit code. This provides the fine-grained control needed to handle complex scenarios and better utilization of compute resources.\u003c/p\u003e\n\u003cp\u003eThis work was done as part of \u003ca href=\"https://kep.k8s.io/5307\"\u003eKEP #5307\u003c/a\u003e led by SIG Node.\u003c/p\u003e\n\u003ch3\u003eLoad environment variables from files created in runtime\u003c/h3\u003e\n\u003cp\u003eApplication developers have long requested greater flexibility in declaring environment variables. Traditionally, environment variables are declared on the API server side via static values, ConfigMaps, or Secrets.\u003c/p\u003e\n\u003cp\u003eBehind the \u003ccode\u003eEnvFiles\u003c/code\u003e feature gate, Kubernetes v1.34 introduces the ability to declare environment variables at runtime. One container (typically an init container) can generate the variable and store it in a file, and a subsequent container can start with the environment variable loaded from that file. This approach eliminates the need to \"wrap\" the target container's entry point, enabling more flexible in-Pod container orchestration.\u003c/p\u003e\n\u003cp\u003eThis feature particularly benefits AI/ML training workloads, where each Pod in a training Job requires initialization with runtime-defined values.\u003c/p\u003e\n\u003cp\u003eThis work was done as part of \u003ca href=\"https://kep.k8s.io/3721\"\u003eKEP #5307\u003c/a\u003e led by SIG Node.\u003c/p\u003e\n\u003ch2\u003eGraduations, deprecations, and removals in v1.34\u003c/h2\u003e\n\u003ch3\u003eGraduations to stable\u003c/h3\u003e\n\u003cp\u003eThis lists all the features that graduated to stable (also known as \u003cem\u003egeneral availability\u003c/em\u003e). For a full list of updates including new features and graduations from alpha to beta, see the release notes.\u003c/p\u003e\n\u003cp\u003eThis release includes a total of 23 enhancements promoted to stable:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://kep.k8s.io/4369\"\u003eAllow almost all printable ASCII characters in environment variables\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://kep.k8s.io/3939\"\u003eAllow for recreation of pods once fully terminated in the job controller\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://kep.k8s.io/4818\"\u003eAllow zero value for Sleep Action of PreStop Hook\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://kep.k8s.io/647\"\u003eAPI Server tracing\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://kep.k8s.io/24\"\u003eAppArmor support\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://kep.k8s.io/4601\"\u003eAuthorize with Field and Label Selectors\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://kep.k8s.io/2340\"\u003eConsistent Reads from Cache\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://kep.k8s.io/3902\"\u003eDecouple TaintManager from NodeLifecycleController\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://kep.k8s.io/4033\"\u003eDiscover cgroup driver from CRI\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://kep.k8s.io/4381\"\u003eDRA: structured parameters\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://kep.k8s.io/3960\"\u003eIntroducing Sleep Action for PreStop Hook\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://kep.k8s.io/2831\"\u003eKubelet OpenTelemetry Tracing\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://kep.k8s.io/3751\"\u003eKubernetes VolumeAttributesClass ModifyVolume\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://kep.k8s.io/2400\"\u003eNode memory swap support\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://kep.k8s.io/4633\"\u003eOnly allow anonymous auth for configured endpoints\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://kep.k8s.io/5080\"\u003eOrdered namespace deletion\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://kep.k8s.io/4247\"\u003ePer-plugin callback functions for accurate requeueing in kube-scheduler\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://kep.k8s.io/4427\"\u003eRelaxed DNS search string validation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://kep.k8s.io/4568\"\u003eResilient Watchcache Initialization\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://kep.k8s.io/5116\"\u003eStreaming Encoding for LIST Responses\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://kep.k8s.io/3331\"\u003eStructured Authentication Config\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://kep.k8s.io/5100\"\u003eSupport for Direct Service Return (DSR) and overlay networking in Windows kube-proxy\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://kep.k8s.io/1790\"\u003eSupport recovery from volume expansion failure\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eDeprecations and removals\u003c/h3\u003e\n\u003cp\u003eAs Kubernetes develops and matures, features may be deprecated, removed, or replaced with better ones to improve the project's overall health. See the Kubernetes \u003ca href=\"https://kubernetes.io/docs/reference/using-api/deprecation-policy/\"\u003edeprecation and removal policy\u003c/a\u003e for more details on this process. Kubernetes v1.34 includes a couple of deprecations.\u003c/p\u003e\n\u003ch4\u003eManual cgroup driver configuration is deprecated\u003c/h4\u003e\n\u003cp\u003eHistorically, configuring the correct cgroup driver has been a pain point for users running Kubernetes clusters. Kubernetes v1.28 added a way for the \u003ccode\u003ekubelet\u003c/code\u003e to query the CRI implementation and find which cgroup driver to use. That automated detection is now \u003cstrong\u003estrongly recommended\u003c/strong\u003e and support for it has graduated to stable in v1.34. If your CRI container runtime does not support the ability to report the cgroup driver it needs, you should upgrade or change your container runtime. The \u003ccode\u003ecgroupDriver\u003c/code\u003e configuration setting in the \u003ccode\u003ekubelet\u003c/code\u003e configuration file is now deprecated. The corresponding command-line option \u003ccode\u003e--cgroup-driver\u003c/code\u003e was previously deprecated, as Kubernetes recommends using the configuration file instead. Both the configuration setting and command-line option will be removed in a future release, that removal will not happen before the v1.36 minor release.\u003c/p\u003e\n\u003cp\u003eThis work was done as part of \u003ca href=\"https://kep.k8s.io/4033\"\u003eKEP #4033\u003c/a\u003e led by SIG Node.\u003c/p\u003e\n\u003ch4\u003eKubernetes to end containerd 1.x support in v1.36\u003c/h4\u003e\n\u003cp\u003eWhile Kubernetes v1.34 still supports containerd 1.7 and other LTS releases of containerd, as a consequence of automated cgroup driver detection, the Kubernetes SIG Node community has formally agreed upon a final support timeline for containerd v1.X. The last Kubernetes release to offer this support will be v1.35 (aligned with containerd 1.7 EOL). This is an early warning that if you are using containerd 1.X, consider switching to 2.0+ soon. You are able to monitor the \u003ccode\u003ekubelet_cri_losing_support\u003c/code\u003e metric to determine if any nodes in your cluster are using a containerd version that will soon be outdated.\u003c/p\u003e\n\u003cp\u003eThis work was done as part of \u003ca href=\"https://kep.k8s.io/4033\"\u003eKEP #4033\u003c/a\u003e led by SIG Node.\u003c/p\u003e\n\u003ch4\u003ePreferClose traffic distribution is deprecated\u003c/h4\u003e\n\u003cp\u003eThe \u003ccode\u003espec.trafficDistribution\u003c/code\u003e field within a Kubernetes \u003ca href=\"https://kubernetes.io/docs/concepts/services-networking/service/\"\u003eService\u003c/a\u003e allows users to express preferences for how traffic should be routed to Service endpoints.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://kep.k8s.io/3015\"\u003eKEP-3015\u003c/a\u003e deprecates \u003ccode\u003ePreferClose\u003c/code\u003e and introduces two additional values: \u003ccode\u003ePreferSameZone\u003c/code\u003e and \u003ccode\u003ePreferSameNode\u003c/code\u003e. \u003ccode\u003ePreferSameZone\u003c/code\u003e is an alias for the existing \u003ccode\u003ePreferClose\u003c/code\u003e to clarify its semantics. \u003ccode\u003ePreferSameNode\u003c/code\u003e allows connections to be delivered to a local endpoint when possible, falling back to a remote endpoint when not possible.\u003c/p\u003e\n\u003cp\u003eThis feature was introduced in v1.33 behind the \u003ccode\u003ePreferSameTrafficDistribution\u003c/code\u003e feature gate. It has graduated to beta in v1.34 and is enabled by default.\u003c/p\u003e\n\u003cp\u003eThis work was done as part of \u003ca href=\"https://kep.k8s.io/3015\"\u003eKEP #3015\u003c/a\u003e led by SIG Network.\u003c/p\u003e\n\u003ch2\u003eRelease notes\u003c/h2\u003e\n\u003cp\u003eCheck out the full details of the Kubernetes v1.34 release in our \u003ca href=\"https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.34.md\"\u003erelease notes\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003eAvailability\u003c/h2\u003e\n\u003cp\u003eKubernetes v1.34 is available for download on or on the \u003ca href=\"https://kubernetes.io/releases/download/\"\u003eKubernetes download page\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eTo get started with Kubernetes, check out these \u003ca href=\"https://kubernetes.io/docs/tutorials/\"\u003einteractive tutorials\u003c/a\u003e or run local Kubernetes clusters using \u003ca href=\"https://minikube.sigs.k8s.io/\"\u003eminikube\u003c/a\u003e. You can also easily install v1.34 using \u003ca href=\"https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/\"\u003ekubeadm\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003eRelease Team\u003c/h2\u003e\n\u003cp\u003eKubernetes is only possible with the support, commitment, and hard work of its community. Each release team is made up of dedicated community volunteers who work together to build the many pieces that make up the Kubernetes releases you rely on. This requires the specialized skills of people from all corners of our community, from the code itself to its documentation and project management.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/cncf/memorials/blob/main/rodolfo-martinez.md\"\u003eWe honor the memory of Rodolfo \"Rodo\" Martínez Vega\u003c/a\u003e, a dedicated contributor whose passion for technology and community building left a mark on the Kubernetes community. Rodo served as a member of the Kubernetes Release Team across multiple releases, including v1.22-v1.23 and v1.25-v1.30, demonstrating unwavering commitment to the project's success and stability.\u003cbr\u003e\nBeyond his Release Team contributions, Rodo was deeply involved in fostering the Cloud Native LATAM community, helping to bridge language and cultural barriers in the space. His work on the Spanish version of Kubernetes documentation and the CNCF Glossary exemplified his dedication to making knowledge accessible to Spanish-speaking developers worldwide. Rodo's legacy lives on through the countless community members he mentored, the releases he helped deliver, and the vibrant LATAM Kubernetes community he helped cultivate.\u003c/p\u003e\n\u003cp\u003eWe would like to thank the entire \u003ca href=\"https://github.com/kubernetes/sig-release/blob/master/releases/release-1.34/release-team.md\"\u003eRelease Team\u003c/a\u003e for the hours spent hard at work to deliver the Kubernetes v1.34 release to our community. The Release Team's membership ranges from first-time shadows to returning team leads with experience forged over several release cycles. A very special thanks goes out to our release lead, Vyom Yadav, for guiding us through a successful release cycle, for his hands-on approach to solving challenges, and for bringing the energy and care that drives our community forward.\u003c/p\u003e\n\u003ch2\u003eProject Velocity\u003c/h2\u003e\n\u003cp\u003eThe CNCF K8s \u003ca href=\"https://k8s.devstats.cncf.io/d/11/companies-contributing-in-repository-groups?orgId=1\u0026#x26;var-period=m\u0026#x26;var-repogroup_name=All\"\u003eDevStats\u003c/a\u003e project aggregates a number of interesting data points related to the velocity of Kubernetes and various sub-projects. This includes everything from individual contributions to the number of companies that are contributing and is an illustration of the depth and breadth of effort that goes into evolving this ecosystem.\u003c/p\u003e\n\u003cp\u003eDuring the v1.34 release cycle, which spanned 15 weeks from 19th May 2025 to 27th August 2025, Kubernetes received contributions from as many as 106 different companies and 491 individuals. In the wider cloud native ecosystem, the figure goes up to 370 companies, counting 2235 total contributors.\u003c/p\u003e\n\u003cp\u003eNote that \"contribution\" counts when someone makes a commit, code review, comment, creates an issue or PR, reviews a PR (including blogs and documentation) or comments on issues and PRs.\u003cbr\u003e\nIf you are interested in contributing, visit \u003ca href=\"https://www.kubernetes.dev/docs/guide/#getting-started\"\u003eGetting Started\u003c/a\u003e on our contributor website.\u003c/p\u003e\n\u003cp\u003eSource for this data:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://k8s.devstats.cncf.io/d/11/companies-contributing-in-repository-groups?orgId=1\u0026#x26;from=1747609200000\u0026#x26;to=1756335599000\u0026#x26;var-period=d28\u0026#x26;var-repogroup_name=Kubernetes\u0026#x26;var-repo_name=kubernetes%2Fkubernetes\"\u003eCompanies contributing to Kubernetes\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://k8s.devstats.cncf.io/d/11/companies-contributing-in-repository-groups?orgId=1\u0026#x26;from=1747609200000\u0026#x26;to=1756335599000\u0026#x26;var-period=d28\u0026#x26;var-repogroup_name=All\u0026#x26;var-repo_name=kubernetes%2Fkubernetes\"\u003eOverall ecosystem contributions\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eEvent Update\u003c/h2\u003e\n\u003cp\u003eExplore upcoming Kubernetes and cloud native events, including KubeCon + CloudNativeCon, KCD, and other notable conferences worldwide. Stay informed and get involved with the Kubernetes community!\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eAugust 2025\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://community.cncf.io/events/details/cncf-kcd-colombia-presents-kcd-colombia-2025/\"\u003e\u003cstrong\u003eKCD - Kubernetes Community Days: Colombia\u003c/strong\u003e\u003c/a\u003e: Aug 28, 2025 | Bogotá, Colombia\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eSeptember 2025\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://community.cncf.io/events/details/cncf-cloud-native-sydney-presents-cloudcon-sydney-sydney-international-convention-centre-910-september/\"\u003e\u003cstrong\u003eCloudCon Sydney\u003c/strong\u003e\u003c/a\u003e: Sep 9–10, 2025 | Sydney, Australia.\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://community.cncf.io/events/details/cncf-kcd-sf-bay-area-presents-kcd-san-francisco-bay-area/\"\u003e\u003cstrong\u003eKCD - Kubernetes Community Days: San Francisco Bay Area\u003c/strong\u003e\u003c/a\u003e: Sep 9, 2025 | San Francisco, USA\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://community.cncf.io/events/details/cncf-kcd-washington-dc-presents-kcd-washington-dc-2025/\"\u003e\u003cstrong\u003eKCD - Kubernetes Community Days: Washington DC\u003c/strong\u003e\u003c/a\u003e: Sep 16, 2025 | Washington, D.C., USA\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://community.cncf.io/events/details/cncf-kcd-sofia-presents-kubernetes-community-days-sofia/\"\u003e\u003cstrong\u003eKCD - Kubernetes Community Days: Sofia\u003c/strong\u003e\u003c/a\u003e: Sep 18, 2025 | Sofia, Bulgaria\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://community.cncf.io/events/details/cncf-kcd-el-salvador-presents-kcd-el-salvador/\"\u003e\u003cstrong\u003eKCD - Kubernetes Community Days: El Salvador\u003c/strong\u003e\u003c/a\u003e: Sep 20, 2025 | San Salvador, El Salvador\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eOctober 2025\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://community.cncf.io/events/details/cncf-kcd-warsaw-presents-kcd-warsaw-2025/\"\u003e\u003cstrong\u003eKCD - Kubernetes Community Days: Warsaw\u003c/strong\u003e\u003c/a\u003e: Oct 9, 2025 | Warsaw, Poland\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://community.cncf.io/events/details/cncf-kcd-uk-presents-kubernetes-community-days-uk-edinburgh-2025/\"\u003e\u003cstrong\u003eKCD - Kubernetes Community Days: Edinburgh\u003c/strong\u003e\u003c/a\u003e: Oct 21, 2025 | Edinburgh, United Kingdom\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://community.cncf.io/events/details/cncf-kcd-sri-lanka-presents-kcd-sri-lanka-2025/\"\u003e\u003cstrong\u003eKCD - Kubernetes Community Days: Sri Lanka\u003c/strong\u003e\u003c/a\u003e: Oct 26, 2025 | Colombo, Sri Lanka\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eNovember 2025\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://community.cncf.io/events/details/cncf-kcd-porto-presents-kcd-porto-2025/\"\u003e\u003cstrong\u003eKCD - Kubernetes Community Days: Porto\u003c/strong\u003e\u003c/a\u003e: Nov 3, 2025 | Porto, Portugal\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://events.linuxfoundation.org/kubecon-cloudnativecon-north-america/\"\u003e\u003cstrong\u003eKubeCon + CloudNativeCon North America 2025\u003c/strong\u003e\u003c/a\u003e: Nov 10-13, 2025 | Atlanta, USA\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://sessionize.com/kcd-hangzhou-and-oicd-2025/\"\u003e\u003cstrong\u003eKCD - Kubernetes Community Days: Hangzhou\u003c/strong\u003e\u003c/a\u003e: Nov 15, 2025 | Hangzhou, China\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eDecember 2025\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://community.cncf.io/events/details/cncf-kcd-suisse-romande-presents-kcd-suisse-romande/\"\u003e\u003cstrong\u003eKCD - Kubernetes Community Days: Suisse Romande\u003c/strong\u003e\u003c/a\u003e: Dec 4, 2025 | Geneva, Switzerland\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eYou can find the latest event details \u003ca href=\"https://community.cncf.io/events/#/list\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003eUpcoming Release Webinar\u003c/h2\u003e\n\u003cp\u003eJoin members of the Kubernetes v1.34 Release Team on \u003cstrong\u003eWednesday, September 24th 2025 at 4:00 PM (UTC)\u003c/strong\u003e, to learn about the release highlights of this release. For more information and registration, visit the \u003ca href=\"https://community.cncf.io/events/details/cncf-cncf-online-programs-presents-cloud-native-live-kubernetes-v134-release/\"\u003eevent page\u003c/a\u003e on the CNCF Online Programs site.\u003c/p\u003e\n\u003ch2\u003eGet Involved\u003c/h2\u003e\n\u003cp\u003eThe simplest way to get involved with Kubernetes is by joining one of the many \u003ca href=\"https://github.com/kubernetes/community/blob/master/sig-list.md\"\u003eSpecial Interest Groups\u003c/a\u003e (SIGs) that align with your interests. Have something you’d like to broadcast to the Kubernetes community? Share your voice at our weekly \u003ca href=\"https://github.com/kubernetes/community/tree/master/communication\"\u003ecommunity meeting\u003c/a\u003e, and through the channels below. Thank you for your continued feedback and support.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eFollow us on Bluesky \u003ca href=\"https://bsky.app/profile/kubernetes.io\"\u003e@Kubernetesio\u003c/a\u003e for the latest updates\u003c/li\u003e\n\u003cli\u003eJoin the community discussion on \u003ca href=\"https://discuss.kubernetes.io/\"\u003eDiscuss\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eJoin the community on \u003ca href=\"http://slack.k8s.io/\"\u003eSlack\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003ePost questions (or answer questions) on \u003ca href=\"http://stackoverflow.com/questions/tagged/kubernetes\"\u003eStack Overflow\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eShare your Kubernetes \u003ca href=\"https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform\"\u003estory\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eRead more about what’s happening with Kubernetes on the \u003ca href=\"https://kubernetes.io/blog/\"\u003eblog\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eLearn more about the \u003ca href=\"https://github.com/kubernetes/sig-release/tree/master/release-team\"\u003eKubernetes Release Team\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n","pageTitle":"Kubernetes v1.34: Of Wind \u0026 Will (O' WaW) - by Graziano Casto","pageDescription":"Kubernetes v1.34: Of Wind \u0026 Will (O' WaW) - by Graziano Casto","pageLink":"https://castograziano.com/articles/kubernetes-v134-announcement","pageImage":"https://castograziano.com/casto_graziano_personal_website.png"},"schema":{"@context":"https://schema.org","@type":"BlogPosting","headline":"Kubernetes v1.34: Of Wind \u0026 Will (O' WaW)","datePublished":"2025-08-27T00:00:00.000Z","dateModified":"2025-08-27T00:00:00.000Z","author":[{"@type":"Person","name":"Graziano Casto","url":"https://castograziano.com/about-me"}]}},"__N_SSG":true},"page":"/articles/[slug]","query":{"slug":"kubernetes-v134-announcement"},"buildId":"91HY78FfS8Xc4CAJLov4-","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>