<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><link rel="preload" href="/_next/static/media/graziano-casto.17cc67b9.jpg" as="image" fetchpriority="high"/><title>Kubernetes v1.35 Sneak Peek - by Graziano Casto</title><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","headline":"Kubernetes v1.35 Sneak Peek","datePublished":"2025-11-26T00:00:00.000Z","dateModified":"2025-11-26T00:00:00.000Z","author":[{"@type":"Person","name":"Graziano Casto","url":"https://castograziano.com/about-me"}]}</script><meta name="description" content="Kubernetes v1.35 Sneak Peek - by Graziano Casto"/><link rel="canonical" href="https://www.castograziano.comhttps://castograziano.com/articles/kubernetes-v135-sneak-peak"/><meta property="og:title" content="Kubernetes v1.35 Sneak Peek - by Graziano Casto"/><meta property="og:description" content="Kubernetes v1.35 Sneak Peek - by Graziano Casto"/><meta property="og:url" content="https://www.castograziano.comhttps://castograziano.com/articles/kubernetes-v135-sneak-peak"/><meta property="og:image" content="https://castograziano.com/casto_graziano_personal_website.png"/><meta property="og:type" content="article"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:title" content="Kubernetes v1.35 Sneak Peek - by Graziano Casto"/><meta name="twitter:description" content="Kubernetes v1.35 Sneak Peek - by Graziano Casto"/><meta name="twitter:image" content="https://castograziano.com/casto_graziano_personal_website.png"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="icon" href="/images/favicon.ico"/><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png"/><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png"/><meta name="next-head-count" content="20"/><link rel="preload" href="/_next/static/css/29ef3251d4b26823.css" as="style"/><link rel="stylesheet" href="/_next/static/css/29ef3251d4b26823.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-78c92fac7aa8fdd8.js"></script><script src="/_next/static/chunks/webpack-0b5d8249fb15f5f3.js" defer=""></script><script src="/_next/static/chunks/framework-2c16ac744b6cdea6.js" defer=""></script><script src="/_next/static/chunks/main-69b16c27ce463005.js" defer=""></script><script src="/_next/static/chunks/pages/_app-666f3c6bf21653ac.js" defer=""></script><script src="/_next/static/chunks/pages/articles/%5Bslug%5D-0c229d73ad55d035.js" defer=""></script><script src="/_next/static/fUz6ulm9CFhYd9hv47CZ0/_buildManifest.js" defer=""></script><script src="/_next/static/fUz6ulm9CFhYd9hv47CZ0/_ssgManifest.js" defer=""></script></head><body><div id="__next"><script>!function(){try{var d=document.documentElement,c=d.classList;c.remove('light','dark');var e=localStorage.getItem('theme');if('system'===e||(!e&&true)){var t='(prefers-color-scheme: dark)',m=window.matchMedia(t);if(m.media!==t||m.matches){d.style.colorScheme = 'dark';c.add('dark')}else{d.style.colorScheme = 'light';c.add('light')}}else if(e){c.add(e|| '')}if(e==='light'||e==='dark')d.style.colorScheme=e}catch(e){}}()</script><div class="flex w-full"><div class="fixed inset-0 flex justify-center sm:px-8"><div class="flex w-full max-w-7xl lg:px-8"><div class="w-full bg-white ring-1 ring-zinc-100 dark:bg-zinc-900 dark:ring-zinc-300/20"></div></div></div><div class="relative flex w-full flex-col"><header class="pointer-events-none relative z-50 flex flex-none flex-col" style="height:var(--header-height);margin-bottom:var(--header-mb)"><div class="top-0 z-10 h-16 pt-6" style="position:var(--header-position)"><div class="sm:px-8 top-[var(--header-top,theme(spacing.6))] w-full" style="position:var(--header-inner-position)"><div class="mx-auto w-full max-w-7xl lg:px-8"><div class="relative px-4 sm:px-8 lg:px-12"><div class="mx-auto max-w-2xl lg:max-w-5xl"><div class="relative flex gap-4"><div class="flex flex-1"><div class="h-10 w-10 rounded-full bg-white/90 p-0.5 shadow-lg shadow-zinc-800/5 ring-1 ring-zinc-900/5 backdrop-blur dark:bg-zinc-800/90 dark:ring-white/10"><a aria-label="Home" class="pointer-events-auto" href="/"><img alt="" fetchpriority="high" width="1872" height="1914" decoding="async" data-nimg="1" class="rounded-full bg-zinc-100 object-cover dark:bg-zinc-800 h-9 w-9" style="color:transparent" src="/_next/static/media/graziano-casto.17cc67b9.jpg"/></a></div></div><div class="flex flex-1 justify-end md:justify-center"><div class="pointer-events-auto md:hidden" data-headlessui-state=""><button class="group flex items-center rounded-full bg-white/90 px-4 py-2 text-sm font-medium text-zinc-800 shadow-lg shadow-zinc-800/5 ring-1 ring-zinc-900/5 backdrop-blur dark:bg-zinc-800/90 dark:text-zinc-200 dark:ring-white/10 dark:hover:ring-white/20" type="button" aria-expanded="false" data-headlessui-state="">Menu<svg viewBox="0 0 8 6" aria-hidden="true" class="ml-3 h-auto w-2 stroke-zinc-500 group-hover:stroke-zinc-700 dark:group-hover:stroke-zinc-400"><path d="M1.75 1.75 4 4.25l2.25-2.5" fill="none" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></button></div><div hidden="" style="position:fixed;top:1px;left:1px;width:1px;height:0;padding:0;margin:-1px;overflow:hidden;clip:rect(0, 0, 0, 0);white-space:nowrap;border-width:0;display:none"></div><nav class="pointer-events-auto hidden md:block"><ul class="flex rounded-full bg-white/90 px-3 text-sm font-medium text-zinc-800 shadow-lg shadow-zinc-800/5 ring-1 ring-zinc-900/5 backdrop-blur dark:bg-zinc-800/90 dark:text-zinc-200 dark:ring-white/10"><li><a class="relative block px-3 py-2 transition hover:text-teal-500 dark:hover:text-teal-400" href="/about-me/">About</a></li><li><a class="relative block px-3 py-2 transition hover:text-teal-500 dark:hover:text-teal-400" href="/articles/">Articles</a></li><li><a class="relative block px-3 py-2 transition hover:text-teal-500 dark:hover:text-teal-400" href="/speaking/">Speaking</a></li><li><a class="relative block px-3 py-2 transition hover:text-teal-500 dark:hover:text-teal-400" href="/projects/">Projects</a></li></ul></nav></div><div class="flex justify-end md:flex-1"><div class="pointer-events-auto"><button type="button" aria-label="Toggle theme" class="group rounded-full bg-white/90 px-3 py-2 shadow-lg shadow-zinc-800/5 ring-1 ring-zinc-900/5 backdrop-blur transition dark:bg-zinc-800/90 dark:ring-white/10 dark:hover:ring-white/20"><svg viewBox="0 0 24 24" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" aria-hidden="true" class="h-6 w-6 fill-zinc-100 stroke-zinc-500 transition group-hover:fill-zinc-200 group-hover:stroke-zinc-700 dark:hidden [@media(prefers-color-scheme:dark)]:fill-teal-50 [@media(prefers-color-scheme:dark)]:stroke-teal-500 [@media(prefers-color-scheme:dark)]:group-hover:fill-teal-50 [@media(prefers-color-scheme:dark)]:group-hover:stroke-teal-600"><path d="M8 12.25A4.25 4.25 0 0 1 12.25 8v0a4.25 4.25 0 0 1 4.25 4.25v0a4.25 4.25 0 0 1-4.25 4.25v0A4.25 4.25 0 0 1 8 12.25v0Z"></path><path d="M12.25 3v1.5M21.5 12.25H20M18.791 18.791l-1.06-1.06M18.791 5.709l-1.06 1.06M12.25 20v1.5M4.5 12.25H3M6.77 6.77 5.709 5.709M6.77 17.73l-1.061 1.061" fill="none"></path></svg><svg viewBox="0 0 24 24" aria-hidden="true" class="hidden h-6 w-6 fill-zinc-700 stroke-zinc-500 transition dark:block [@media(prefers-color-scheme:dark)]:group-hover:stroke-zinc-400 [@media_not_(prefers-color-scheme:dark)]:fill-teal-400/10 [@media_not_(prefers-color-scheme:dark)]:stroke-teal-500"><path d="M17.25 16.22a6.937 6.937 0 0 1-9.47-9.47 7.451 7.451 0 1 0 9.47 9.47ZM12.75 7C17 7 17 2.75 17 2.75S17 7 21.25 7C17 7 17 11.25 17 11.25S17 7 12.75 7Z" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></button></div></div></div></div></div></div></div></div></header><main class="flex-auto"><div class="sm:px-8 mt-16 lg:mt-32"><div class="mx-auto w-full max-w-7xl lg:px-8"><div class="relative px-4 sm:px-8 lg:px-12"><div class="mx-auto max-w-2xl lg:max-w-5xl"><div class="xl:relative"><div class="mx-auto max-w-2xl"><article><header class="flex flex-col"><h1 class="mt-6 text-4xl font-bold tracking-tight text-zinc-800 sm:text-5xl dark:text-zinc-100">Kubernetes v1.35 Sneak Peek</h1><time dateTime="2025-11-26" class="order-first flex items-center text-base text-zinc-500 dark:text-zinc-400"><span class="h-4 w-0.5 rounded-full bg-zinc-200 dark:bg-zinc-500"></span><span class="ml-3">November 26, 2025</span></time></header><div class="mt-8 prose dark:prose-invert" data-mdx-content="true"><div><p><a href="https://kubernetes.io/blog/2025/11/26/kubernetes-v1-35-sneak-peek/">Published by Kubernetes co-authored with Aakanksha Bhende, Arujjwal Negi, Chad M. Crowell, Swathi Rao</a></p>
<p>As the release of Kubernetes v1.35 approaches, the Kubernetes project continues to evolve. Features may be deprecated, removed, or replaced to improve the project's overall health. This blog post outlines planned changes for the v1.35 release that the release team believes you should be aware of to ensure the continued smooth operation of your Kubernetes cluster(s), and to keep you up to date with the latest developments. The information below is based on the current status of the v1.35 release and is subject to change before the final release date.</p>
<h3>Deprecations and removals for Kubernetes v1.35</h3>
<h4>cgroup v1 support</h4>
<p>On Linux nodes, container runtimes typically rely on cgroups (short for "control groups"). Support for using cgroup v2 has been stable in Kubernetes since v1.25, providing an alternative to the original v1 cgroup support. While cgroup v1 provided the initial resource control mechanism, it suffered from well-known inconsistencies and limitations. Adding support for cgroup v2 allowed use of a unified control group hierarchy, improved resource isolation, and served as the foundation for modern features, making legacy cgroup v1 support ready for removal. The removal of cgroup v1 support will only impact cluster administrators running nodes on older Linux distributions that do not support cgroup v2; on those nodes, the <code>kubelet</code> will fail to start. Administrators must migrate their nodes to systems with cgroup v2 enabled. More details on compatibility requirements will be available in a blog post soon after the v1.35 release.</p>
<p>To learn more, read <a href="https://kubernetes.io/docs/concepts/architecture/cgroups/">about cgroup v2</a>;<br>
you can also track the switchover work via <a href="https://kep.k8s.io/5573">KEP-5573: Remove cgroup v1 support</a>.</p>
<h4>Deprecation of ipvs mode in kube-proxy</h4>
<p>Many releases ago, the Kubernetes project implemented an <a href="https://kubernetes.io/docs/reference/networking/virtual-ips/#proxy-mode-ipvs">ipvs</a> mode in <code>kube-proxy</code>. It was adopted as a way to provide high-performance service load balancing, with better performance than the existing <code>iptables</code> mode. However, maintaining feature parity between ipvs and other kube-proxy modes became difficult, due to technical complexity and diverging requirements. This created significant technical debt and made the ipvs backend impractical to support alongside newer networking capabilities.</p>
<p>The Kubernetes project intends to deprecate kube-proxy <code>ipvs</code> mode in the v1.35 release, to streamline the <code>kube-proxy</code> codebase. For Linux nodes, the recommended <code>kube-proxy</code> mode is already <a href="https://kubernetes.io/docs/reference/networking/virtual-ips/#proxy-mode-nftables">nftables</a>.</p>
<p>You can find more in <a href="https://kep.k8s.io/5495">KEP-5495: Deprecate ipvs mode in kube-proxy</a></p>
<h4>Kubernetes is deprecating containerd v1.y support</h4>
<p>While Kubernetes v1.35 still supports containerd 1.7 and other LTS releases of containerd, as a consequence of automated cgroup driver detection, the Kubernetes SIG Node community has formally agreed upon a final support timeline for containerd v1.X. Kubernetes v1.35 is the last release to offer this support (aligned with containerd 1.7 EOL).</p>
<p>This is a final warning that if you are using containerd 1.X, you must switch to 2.0 or later before upgrading Kubernetes to the next version. You are able to monitor the <code>kubelet_cri_losing_support</code> metric to determine if any nodes in your cluster are using a containerd version that will soon be unsupported.</p>
<p>You can find more in the <a href="https://kubernetes.io/blog/2025/09/12/kubernetes-v1-34-cri-cgroup-driver-lookup-now-ga/#announcement-kubernetes-is-deprecating-containerd-v1-y-support">official blog post</a> or in <a href="https://kep.k8s.io/4033">KEP-4033: Discover cgroup driver from CRI</a></p>
<p>The following enhancements are some of those likely to be included in the v1.35 release. This is not a commitment, and the release content is subject to change.</p>
<h4>Node declared features</h4>
<p>When scheduling Pods, Kubernetes uses node labels, taints, and tolerations to match workload requirements with node capabilities. However, managing feature compatibility becomes challenging during cluster upgrades due to version skew between the control plane and nodes. This can lead to Pods being scheduled on nodes that lack required features, resulting in runtime failures.</p>
<p>The <em>node declared features</em> framework will introduce a standard mechanism for nodes to declare their supported Kubernetes features. With the new alpha feature enabled, a Node reports the features it can support, publishing this information to the control plane through a new <code>.status.declaredFeatures</code> field. Then, the <code>kube-scheduler</code>, admission controllers and third-party components can use these declarations. For example, you can enforce scheduling and API validation constraints, ensuring that Pods run only on compatible nodes.</p>
<p>This approach reduces manual node labeling, improves scheduling accuracy, and prevents incompatible pod placements proactively. It also integrates with the Cluster Autoscaler for informed scale-up decisions. Feature declarations are temporary and tied to Kubernetes feature gates, enabling safe rollout and cleanup.</p>
<p>Targeting alpha in v1.35, <em>node declared features</em> aims to solve version skew scheduling issues by making node capabilities explicit, enhancing reliability and cluster stability in heterogeneous version environments.</p>
<p>To learn more about this before the official documentation is published, you can read <a href="https://kep.k8s.io/5328">KEP-5328</a>.</p>
<h4>In-place update of Pod resources</h4>
<p>Kubernetes is graduating in-place updates for Pod resources to General Availability (GA). This feature allows users to adjust <code>cpu</code> and <code>memory</code> resources without restarting Pods or Containers. Previously, such modifications required recreating Pods, which could disrupt workloads, particularly for stateful or batch applications. Previous Kubernetes releases already allowed you to change infrastructure resources settings (requests and limits) for existing Pods. This allows for smoother <a href="https://kubernetes.io/docs/concepts/workloads/autoscaling/vertical-pod-autoscale/">vertical scaling</a>, improves efficiency, and can also simplify solution development.</p>
<p>The Container Runtime Interface (CRI) has also been improved, extending the <code>UpdateContainerResources</code> API for Windows and future runtimes while allowing <code>ContainerStatus</code> to report real-time resource configurations. Together, these changes make scaling in Kubernetes faster, more flexible, and disruption-free. The feature was introduced as alpha in v1.27, graduated to beta in v1.33, and is targeting graduation to stable in v1.35.</p>
<p>You can find more in <a href="https://kep.k8s.io/1287">KEP-1287: In-place Update of Pod Resources</a></p>
<h4>Pod certificates</h4>
<p>When running microservices, Pods often require a strong cryptographic identity to authenticate with each other using mutual TLS (mTLS). While Kubernetes provides Service Account tokens, these are designed for authenticating to the API server, not for general-purpose workload identity.</p>
<p>Before this enhancement, operators had to rely on complex, external projects like SPIFFE/SPIRE or cert-manager to provision and rotate certificates for their workloads. But what if you could issue a unique, short-lived certificate to your Pods natively and automatically? KEP-4317 is designed to enable such native workload identity. It opens up various possibilities for securing pod-to-pod communication by allowing the <code>kubelet</code> to request and mount certificates for a Pod via a projected volume.</p>
<p>This provides a built-in mechanism for workload identity, complete with automated certificate rotation, significantly simplifying the setup of service meshes and other zero-trust network policies. This feature was introduced as alpha in v1.34 and is targeting beta in v1.35.</p>
<p>You can find more in <a href="https://kep.k8s.io/4317">KEP-4317: Pod Certificates</a></p>
<h4>Numeric values for taints</h4>
<p>Kubernetes is enhancing <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/">taints and tolerations</a> by adding numeric comparison operators, such as <code>Gt</code> (Greater Than) and <code>Lt</code> (Less Than).</p>
<p>Previously, tolerations supported only exact (<code>Equal</code>) or existence (<code>Exists</code>) matches, which were not suitable for numeric properties such as reliability SLAs.</p>
<p>With this change, a Pod can use a toleration to "opt-in" to nodes that meet a specific numeric threshold. For example, a Pod can require a Node with an SLA taint value greater than 950 (<code>operator: Gt</code>, <code>value: "950"</code>).</p>
<p>This approach is more powerful than Node Affinity because it supports the NoExecute effect, allowing Pods to be automatically evicted if a node's numeric value drops below the tolerated threshold.</p>
<p>You can find more in <a href="https://kep.k8s.io/5471">KEP-5471: Enable SLA-based Scheduling</a></p>
<h4>User namespaces</h4>
<p>When running Pods, you can use <code>securityContext</code> to drop privileges, but containers inside the pod often still run as root (UID 0). This simplicity poses a significant challenge, as that container UID 0 maps directly to the host's root user.</p>
<p>Before this enhancement, a container breakout vulnerability could grant an attacker full root access to the node. But what if you could dynamically remap the container's root user to a safe, unprivileged user on the host? KEP-127 specifically allows such native support for Linux User Namespaces. It opens up various possibilities for pod security by isolating container and host user/group IDs. This allows a process to have root privileges (UID 0) within its namespace, while running as a non-privileged, high-numbered UID on the host.</p>
<p>Released as alpha in v1.25 and beta in v1.30, this feature continues to progress through beta maturity, paving the way for truly "rootless" containers that drastically reduce the attack surface for a whole class of security vulnerabilities.</p>
<p>You can find more in <a href="https://kep.k8s.io/127">KEP-127: User Namespaces</a></p>
<h4>Support for mounting OCI images as volumes</h4>
<p>When provisioning a Pod, you often need to bundle data, binaries, or configuration files for your containers. Before this enhancement, people often included that kind of data directly into the main container image, or required a custom init container to download and unpack files into an <code>emptyDir</code>. You can still take either of those approaches, of course.</p>
<p>But what if you could populate a volume directly from a data-only artifact in an OCI registry, just like pulling a container image? Kubernetes v1.31 added support for the <code>image</code> volume type, allowing Pods to pull and unpack OCI container image artifacts into a volume declaratively.</p>
<p>This allows for seamless distribution of data, binaries, or ML models using standard registry tooling, completely decoupling data from the container image and eliminating the need for complex init containers or startup scripts. This volume type has been in beta since v1.33 and will likely be enabled by default in v1.35.</p>
<p>You can try out the beta version of <a href="https://kubernetes.io/docs/concepts/storage/volumes/#image"><code>image</code> volumes</a>, or you can learn more about the plans from <a href="https://kep.k8s.io/4639">KEP-4639: OCI Volume Source</a>.</p>
<h3>Want to know more?</h3>
<p>New features and deprecations are also announced in the Kubernetes release notes. We will formally announce what's new in <a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.35.md">Kubernetes v1.35</a> as part of the CHANGELOG for that release.</p>
<p>The Kubernetes v1.35 release is planned for <strong>December 17, 2025</strong>. Stay tuned for updates!</p>
<p>You can also see the announcements of changes in the release notes for:</p>
<ul>
<li><a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.34.md">Kubernetes v1.34</a></li>
<li><a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.33.md">Kubernetes v1.33</a></li>
<li><a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.32.md">Kubernetes v1.32</a></li>
<li><a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.31.md">Kubernetes v1.31</a></li>
<li><a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.30.md">Kubernetes v1.30</a></li>
</ul>
<h3>Get involved</h3>
<p>The simplest way to get involved with Kubernetes is by joining one of the many <a href="https://github.com/kubernetes/community/blob/master/sig-list.md">Special Interest Groups</a> (SIGs) that align with your interests. Have something you’d like to broadcast to the Kubernetes community? Share your voice at our weekly <a href="https://github.com/kubernetes/community/tree/master/communication">community meeting</a>, and through the channels below. Thank you for your continued feedback and support.</p>
<ul>
<li>Follow us on Bluesky <a href="https://bsky.app/profile/kubernetes.io">@kubernetes.io</a> for the latest updates</li>
<li>Join the community discussion on <a href="https://discuss.kubernetes.io/">Discuss</a></li>
<li>Join the community on <a href="http://slack.k8s.io/">Slack</a></li>
<li>Post questions (or answer questions) on <a href="https://serverfault.com/questions/tagged/kubernetes">Server Fault</a> or <a href="http://stackoverflow.com/questions/tagged/kubernetes">Stack Overflow</a></li>
<li>Share your Kubernetes <a href="https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform">story</a></li>
<li>Read more about what’s happening with Kubernetes on the <a href="https://kubernetes.io/blog/">blog</a></li>
<li>Learn more about the <a href="https://github.com/kubernetes/sig-release/tree/master/release-team">Kubernetes Release Team</a></li>
</ul>
</div></div></article></div></div></div></div></div></div></main><footer class="mt-32 flex-none"><div class="sm:px-8"><div class="mx-auto w-full max-w-7xl lg:px-8"><div class="border-t border-zinc-100 pb-16 pt-10 dark:border-zinc-700/40"><div class="relative px-4 sm:px-8 lg:px-12"><div class="mx-auto max-w-2xl lg:max-w-5xl"><div class="flex flex-col items-center justify-between gap-6 sm:flex-row"><div class="flex flex-wrap justify-center gap-x-6 gap-y-1 text-sm font-medium text-zinc-800 dark:text-zinc-200"><a class="transition hover:text-teal-500 dark:hover:text-teal-400" href="/about-me/">About</a><a class="transition hover:text-teal-500 dark:hover:text-teal-400" href="/projects/">Projects</a><a class="transition hover:text-teal-500 dark:hover:text-teal-400" href="/speaking/">Speaking</a></div><p class="text-sm text-zinc-500 dark:text-zinc-400">© <!-- -->2026<!-- --> Graziano Casto. All rights reserved.</p></div></div></div></div></div></div></footer></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"data":{"article":{"title":"Kubernetes v1.35 Sneak Peek","date":"2025-11-26"},"children":"\u003cp\u003e\u003ca href=\"https://kubernetes.io/blog/2025/11/26/kubernetes-v1-35-sneak-peek/\"\u003ePublished by Kubernetes co-authored with Aakanksha Bhende, Arujjwal Negi, Chad M. Crowell, Swathi Rao\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eAs the release of Kubernetes v1.35 approaches, the Kubernetes project continues to evolve. Features may be deprecated, removed, or replaced to improve the project's overall health. This blog post outlines planned changes for the v1.35 release that the release team believes you should be aware of to ensure the continued smooth operation of your Kubernetes cluster(s), and to keep you up to date with the latest developments. The information below is based on the current status of the v1.35 release and is subject to change before the final release date.\u003c/p\u003e\n\u003ch3\u003eDeprecations and removals for Kubernetes v1.35\u003c/h3\u003e\n\u003ch4\u003ecgroup v1 support\u003c/h4\u003e\n\u003cp\u003eOn Linux nodes, container runtimes typically rely on cgroups (short for \"control groups\"). Support for using cgroup v2 has been stable in Kubernetes since v1.25, providing an alternative to the original v1 cgroup support. While cgroup v1 provided the initial resource control mechanism, it suffered from well-known inconsistencies and limitations. Adding support for cgroup v2 allowed use of a unified control group hierarchy, improved resource isolation, and served as the foundation for modern features, making legacy cgroup v1 support ready for removal. The removal of cgroup v1 support will only impact cluster administrators running nodes on older Linux distributions that do not support cgroup v2; on those nodes, the \u003ccode\u003ekubelet\u003c/code\u003e will fail to start. Administrators must migrate their nodes to systems with cgroup v2 enabled. More details on compatibility requirements will be available in a blog post soon after the v1.35 release.\u003c/p\u003e\n\u003cp\u003eTo learn more, read \u003ca href=\"https://kubernetes.io/docs/concepts/architecture/cgroups/\"\u003eabout cgroup v2\u003c/a\u003e;\u003cbr\u003e\nyou can also track the switchover work via \u003ca href=\"https://kep.k8s.io/5573\"\u003eKEP-5573: Remove cgroup v1 support\u003c/a\u003e.\u003c/p\u003e\n\u003ch4\u003eDeprecation of ipvs mode in kube-proxy\u003c/h4\u003e\n\u003cp\u003eMany releases ago, the Kubernetes project implemented an \u003ca href=\"https://kubernetes.io/docs/reference/networking/virtual-ips/#proxy-mode-ipvs\"\u003eipvs\u003c/a\u003e mode in \u003ccode\u003ekube-proxy\u003c/code\u003e. It was adopted as a way to provide high-performance service load balancing, with better performance than the existing \u003ccode\u003eiptables\u003c/code\u003e mode. However, maintaining feature parity between ipvs and other kube-proxy modes became difficult, due to technical complexity and diverging requirements. This created significant technical debt and made the ipvs backend impractical to support alongside newer networking capabilities.\u003c/p\u003e\n\u003cp\u003eThe Kubernetes project intends to deprecate kube-proxy \u003ccode\u003eipvs\u003c/code\u003e mode in the v1.35 release, to streamline the \u003ccode\u003ekube-proxy\u003c/code\u003e codebase. For Linux nodes, the recommended \u003ccode\u003ekube-proxy\u003c/code\u003e mode is already \u003ca href=\"https://kubernetes.io/docs/reference/networking/virtual-ips/#proxy-mode-nftables\"\u003enftables\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eYou can find more in \u003ca href=\"https://kep.k8s.io/5495\"\u003eKEP-5495: Deprecate ipvs mode in kube-proxy\u003c/a\u003e\u003c/p\u003e\n\u003ch4\u003eKubernetes is deprecating containerd v1.y support\u003c/h4\u003e\n\u003cp\u003eWhile Kubernetes v1.35 still supports containerd 1.7 and other LTS releases of containerd, as a consequence of automated cgroup driver detection, the Kubernetes SIG Node community has formally agreed upon a final support timeline for containerd v1.X. Kubernetes v1.35 is the last release to offer this support (aligned with containerd 1.7 EOL).\u003c/p\u003e\n\u003cp\u003eThis is a final warning that if you are using containerd 1.X, you must switch to 2.0 or later before upgrading Kubernetes to the next version. You are able to monitor the \u003ccode\u003ekubelet_cri_losing_support\u003c/code\u003e metric to determine if any nodes in your cluster are using a containerd version that will soon be unsupported.\u003c/p\u003e\n\u003cp\u003eYou can find more in the \u003ca href=\"https://kubernetes.io/blog/2025/09/12/kubernetes-v1-34-cri-cgroup-driver-lookup-now-ga/#announcement-kubernetes-is-deprecating-containerd-v1-y-support\"\u003eofficial blog post\u003c/a\u003e or in \u003ca href=\"https://kep.k8s.io/4033\"\u003eKEP-4033: Discover cgroup driver from CRI\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThe following enhancements are some of those likely to be included in the v1.35 release. This is not a commitment, and the release content is subject to change.\u003c/p\u003e\n\u003ch4\u003eNode declared features\u003c/h4\u003e\n\u003cp\u003eWhen scheduling Pods, Kubernetes uses node labels, taints, and tolerations to match workload requirements with node capabilities. However, managing feature compatibility becomes challenging during cluster upgrades due to version skew between the control plane and nodes. This can lead to Pods being scheduled on nodes that lack required features, resulting in runtime failures.\u003c/p\u003e\n\u003cp\u003eThe \u003cem\u003enode declared features\u003c/em\u003e framework will introduce a standard mechanism for nodes to declare their supported Kubernetes features. With the new alpha feature enabled, a Node reports the features it can support, publishing this information to the control plane through a new \u003ccode\u003e.status.declaredFeatures\u003c/code\u003e field. Then, the \u003ccode\u003ekube-scheduler\u003c/code\u003e, admission controllers and third-party components can use these declarations. For example, you can enforce scheduling and API validation constraints, ensuring that Pods run only on compatible nodes.\u003c/p\u003e\n\u003cp\u003eThis approach reduces manual node labeling, improves scheduling accuracy, and prevents incompatible pod placements proactively. It also integrates with the Cluster Autoscaler for informed scale-up decisions. Feature declarations are temporary and tied to Kubernetes feature gates, enabling safe rollout and cleanup.\u003c/p\u003e\n\u003cp\u003eTargeting alpha in v1.35, \u003cem\u003enode declared features\u003c/em\u003e aims to solve version skew scheduling issues by making node capabilities explicit, enhancing reliability and cluster stability in heterogeneous version environments.\u003c/p\u003e\n\u003cp\u003eTo learn more about this before the official documentation is published, you can read \u003ca href=\"https://kep.k8s.io/5328\"\u003eKEP-5328\u003c/a\u003e.\u003c/p\u003e\n\u003ch4\u003eIn-place update of Pod resources\u003c/h4\u003e\n\u003cp\u003eKubernetes is graduating in-place updates for Pod resources to General Availability (GA). This feature allows users to adjust \u003ccode\u003ecpu\u003c/code\u003e and \u003ccode\u003ememory\u003c/code\u003e resources without restarting Pods or Containers. Previously, such modifications required recreating Pods, which could disrupt workloads, particularly for stateful or batch applications. Previous Kubernetes releases already allowed you to change infrastructure resources settings (requests and limits) for existing Pods. This allows for smoother \u003ca href=\"https://kubernetes.io/docs/concepts/workloads/autoscaling/vertical-pod-autoscale/\"\u003evertical scaling\u003c/a\u003e, improves efficiency, and can also simplify solution development.\u003c/p\u003e\n\u003cp\u003eThe Container Runtime Interface (CRI) has also been improved, extending the \u003ccode\u003eUpdateContainerResources\u003c/code\u003e API for Windows and future runtimes while allowing \u003ccode\u003eContainerStatus\u003c/code\u003e to report real-time resource configurations. Together, these changes make scaling in Kubernetes faster, more flexible, and disruption-free. The feature was introduced as alpha in v1.27, graduated to beta in v1.33, and is targeting graduation to stable in v1.35.\u003c/p\u003e\n\u003cp\u003eYou can find more in \u003ca href=\"https://kep.k8s.io/1287\"\u003eKEP-1287: In-place Update of Pod Resources\u003c/a\u003e\u003c/p\u003e\n\u003ch4\u003ePod certificates\u003c/h4\u003e\n\u003cp\u003eWhen running microservices, Pods often require a strong cryptographic identity to authenticate with each other using mutual TLS (mTLS). While Kubernetes provides Service Account tokens, these are designed for authenticating to the API server, not for general-purpose workload identity.\u003c/p\u003e\n\u003cp\u003eBefore this enhancement, operators had to rely on complex, external projects like SPIFFE/SPIRE or cert-manager to provision and rotate certificates for their workloads. But what if you could issue a unique, short-lived certificate to your Pods natively and automatically? KEP-4317 is designed to enable such native workload identity. It opens up various possibilities for securing pod-to-pod communication by allowing the \u003ccode\u003ekubelet\u003c/code\u003e to request and mount certificates for a Pod via a projected volume.\u003c/p\u003e\n\u003cp\u003eThis provides a built-in mechanism for workload identity, complete with automated certificate rotation, significantly simplifying the setup of service meshes and other zero-trust network policies. This feature was introduced as alpha in v1.34 and is targeting beta in v1.35.\u003c/p\u003e\n\u003cp\u003eYou can find more in \u003ca href=\"https://kep.k8s.io/4317\"\u003eKEP-4317: Pod Certificates\u003c/a\u003e\u003c/p\u003e\n\u003ch4\u003eNumeric values for taints\u003c/h4\u003e\n\u003cp\u003eKubernetes is enhancing \u003ca href=\"https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/\"\u003etaints and tolerations\u003c/a\u003e by adding numeric comparison operators, such as \u003ccode\u003eGt\u003c/code\u003e (Greater Than) and \u003ccode\u003eLt\u003c/code\u003e (Less Than).\u003c/p\u003e\n\u003cp\u003ePreviously, tolerations supported only exact (\u003ccode\u003eEqual\u003c/code\u003e) or existence (\u003ccode\u003eExists\u003c/code\u003e) matches, which were not suitable for numeric properties such as reliability SLAs.\u003c/p\u003e\n\u003cp\u003eWith this change, a Pod can use a toleration to \"opt-in\" to nodes that meet a specific numeric threshold. For example, a Pod can require a Node with an SLA taint value greater than 950 (\u003ccode\u003eoperator: Gt\u003c/code\u003e, \u003ccode\u003evalue: \"950\"\u003c/code\u003e).\u003c/p\u003e\n\u003cp\u003eThis approach is more powerful than Node Affinity because it supports the NoExecute effect, allowing Pods to be automatically evicted if a node's numeric value drops below the tolerated threshold.\u003c/p\u003e\n\u003cp\u003eYou can find more in \u003ca href=\"https://kep.k8s.io/5471\"\u003eKEP-5471: Enable SLA-based Scheduling\u003c/a\u003e\u003c/p\u003e\n\u003ch4\u003eUser namespaces\u003c/h4\u003e\n\u003cp\u003eWhen running Pods, you can use \u003ccode\u003esecurityContext\u003c/code\u003e to drop privileges, but containers inside the pod often still run as root (UID 0). This simplicity poses a significant challenge, as that container UID 0 maps directly to the host's root user.\u003c/p\u003e\n\u003cp\u003eBefore this enhancement, a container breakout vulnerability could grant an attacker full root access to the node. But what if you could dynamically remap the container's root user to a safe, unprivileged user on the host? KEP-127 specifically allows such native support for Linux User Namespaces. It opens up various possibilities for pod security by isolating container and host user/group IDs. This allows a process to have root privileges (UID 0) within its namespace, while running as a non-privileged, high-numbered UID on the host.\u003c/p\u003e\n\u003cp\u003eReleased as alpha in v1.25 and beta in v1.30, this feature continues to progress through beta maturity, paving the way for truly \"rootless\" containers that drastically reduce the attack surface for a whole class of security vulnerabilities.\u003c/p\u003e\n\u003cp\u003eYou can find more in \u003ca href=\"https://kep.k8s.io/127\"\u003eKEP-127: User Namespaces\u003c/a\u003e\u003c/p\u003e\n\u003ch4\u003eSupport for mounting OCI images as volumes\u003c/h4\u003e\n\u003cp\u003eWhen provisioning a Pod, you often need to bundle data, binaries, or configuration files for your containers. Before this enhancement, people often included that kind of data directly into the main container image, or required a custom init container to download and unpack files into an \u003ccode\u003eemptyDir\u003c/code\u003e. You can still take either of those approaches, of course.\u003c/p\u003e\n\u003cp\u003eBut what if you could populate a volume directly from a data-only artifact in an OCI registry, just like pulling a container image? Kubernetes v1.31 added support for the \u003ccode\u003eimage\u003c/code\u003e volume type, allowing Pods to pull and unpack OCI container image artifacts into a volume declaratively.\u003c/p\u003e\n\u003cp\u003eThis allows for seamless distribution of data, binaries, or ML models using standard registry tooling, completely decoupling data from the container image and eliminating the need for complex init containers or startup scripts. This volume type has been in beta since v1.33 and will likely be enabled by default in v1.35.\u003c/p\u003e\n\u003cp\u003eYou can try out the beta version of \u003ca href=\"https://kubernetes.io/docs/concepts/storage/volumes/#image\"\u003e\u003ccode\u003eimage\u003c/code\u003e volumes\u003c/a\u003e, or you can learn more about the plans from \u003ca href=\"https://kep.k8s.io/4639\"\u003eKEP-4639: OCI Volume Source\u003c/a\u003e.\u003c/p\u003e\n\u003ch3\u003eWant to know more?\u003c/h3\u003e\n\u003cp\u003eNew features and deprecations are also announced in the Kubernetes release notes. We will formally announce what's new in \u003ca href=\"https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.35.md\"\u003eKubernetes v1.35\u003c/a\u003e as part of the CHANGELOG for that release.\u003c/p\u003e\n\u003cp\u003eThe Kubernetes v1.35 release is planned for \u003cstrong\u003eDecember 17, 2025\u003c/strong\u003e. Stay tuned for updates!\u003c/p\u003e\n\u003cp\u003eYou can also see the announcements of changes in the release notes for:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.34.md\"\u003eKubernetes v1.34\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.33.md\"\u003eKubernetes v1.33\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.32.md\"\u003eKubernetes v1.32\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.31.md\"\u003eKubernetes v1.31\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.30.md\"\u003eKubernetes v1.30\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eGet involved\u003c/h3\u003e\n\u003cp\u003eThe simplest way to get involved with Kubernetes is by joining one of the many \u003ca href=\"https://github.com/kubernetes/community/blob/master/sig-list.md\"\u003eSpecial Interest Groups\u003c/a\u003e (SIGs) that align with your interests. Have something you’d like to broadcast to the Kubernetes community? Share your voice at our weekly \u003ca href=\"https://github.com/kubernetes/community/tree/master/communication\"\u003ecommunity meeting\u003c/a\u003e, and through the channels below. Thank you for your continued feedback and support.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eFollow us on Bluesky \u003ca href=\"https://bsky.app/profile/kubernetes.io\"\u003e@kubernetes.io\u003c/a\u003e for the latest updates\u003c/li\u003e\n\u003cli\u003eJoin the community discussion on \u003ca href=\"https://discuss.kubernetes.io/\"\u003eDiscuss\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eJoin the community on \u003ca href=\"http://slack.k8s.io/\"\u003eSlack\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003ePost questions (or answer questions) on \u003ca href=\"https://serverfault.com/questions/tagged/kubernetes\"\u003eServer Fault\u003c/a\u003e or \u003ca href=\"http://stackoverflow.com/questions/tagged/kubernetes\"\u003eStack Overflow\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eShare your Kubernetes \u003ca href=\"https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform\"\u003estory\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eRead more about what’s happening with Kubernetes on the \u003ca href=\"https://kubernetes.io/blog/\"\u003eblog\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eLearn more about the \u003ca href=\"https://github.com/kubernetes/sig-release/tree/master/release-team\"\u003eKubernetes Release Team\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n","pageTitle":"Kubernetes v1.35 Sneak Peek - by Graziano Casto","pageDescription":"Kubernetes v1.35 Sneak Peek - by Graziano Casto","pageLink":"https://castograziano.com/articles/kubernetes-v135-sneak-peak","pageImage":"https://castograziano.com/casto_graziano_personal_website.png"},"schema":{"@context":"https://schema.org","@type":"BlogPosting","headline":"Kubernetes v1.35 Sneak Peek","datePublished":"2025-11-26T00:00:00.000Z","dateModified":"2025-11-26T00:00:00.000Z","author":[{"@type":"Person","name":"Graziano Casto","url":"https://castograziano.com/about-me"}]}},"__N_SSG":true},"page":"/articles/[slug]","query":{"slug":"kubernetes-v135-sneak-peak"},"buildId":"fUz6ulm9CFhYd9hv47CZ0","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>