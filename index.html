<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><link rel="preload" href="/_next/static/media/graziano-casto.17cc67b9.jpg" as="image" fetchpriority="high"/><title>Graziano Casto - DevRel Engineer</title><script type="application/ld+json">{"@context":"https://schema.org","@type":"WebSite","name":"Graziano Casto WebSite","alternateName":"Graziano's WebSite","url":"https://castograziano.com"}</script><meta name="description" content="Welcome to the personal website of Graziano Casto."/><meta name="keywords" content="Cloud Native, Kubernetes, Developer Experience, Platform Engineering, DevRel"/><meta name="google-site-verification" content="nwBPkfmfDfedHF9qe1qSlQtAuBQICJ5STXjbfnTzEDA"/><link rel="canonical" href="https://www.castograziano.comhttps://castograziano.com"/><link rel="manifest" href="/site.webmanifest"/><meta property="og:title" content="Graziano Casto - DevRel Engineer"/><meta property="og:description" content="Welcome to the personal website of Graziano Casto."/><meta property="og:url" content="https://www.castograziano.comhttps://castograziano.com"/><meta property="og:image" content="https://castograziano.com/casto-graziano.jpg"/><meta property="og:type" content="article"/><meta property="og:locale" content="en_US"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:title" content="Graziano Casto - DevRel Engineer"/><meta name="twitter:description" content="Welcome to the personal website of Graziano Casto."/><meta name="twitter:image" content="https://castograziano.com/casto-graziano.jpg"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="icon" href="/images/favicon.ico"/><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png"/><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png"/><meta name="next-head-count" content="24"/><link rel="preload" href="/_next/static/css/bcb19791dba4e2b1.css" as="style"/><link rel="stylesheet" href="/_next/static/css/bcb19791dba4e2b1.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-78c92fac7aa8fdd8.js"></script><script src="/_next/static/chunks/webpack-6ef43a8d4a395f49.js" defer=""></script><script src="/_next/static/chunks/framework-2c16ac744b6cdea6.js" defer=""></script><script src="/_next/static/chunks/main-69b16c27ce463005.js" defer=""></script><script src="/_next/static/chunks/pages/_app-a8fe774887236273.js" defer=""></script><script src="/_next/static/chunks/762-c2c6b728b629a445.js" defer=""></script><script src="/_next/static/chunks/pages/index-6a8e7e5e9fef5e95.js" defer=""></script><script src="/_next/static/aI1FMStRJNvpGWbdYQdRM/_buildManifest.js" defer=""></script><script src="/_next/static/aI1FMStRJNvpGWbdYQdRM/_ssgManifest.js" defer=""></script></head><body><div id="__next"><script>!function(){try{var d=document.documentElement,c=d.classList;c.remove('light','dark');var e=localStorage.getItem('theme');if('system'===e||(!e&&true)){var t='(prefers-color-scheme: dark)',m=window.matchMedia(t);if(m.media!==t||m.matches){d.style.colorScheme = 'dark';c.add('dark')}else{d.style.colorScheme = 'light';c.add('light')}}else if(e){c.add(e|| '')}if(e==='light'||e==='dark')d.style.colorScheme=e}catch(e){}}()</script><div class="flex w-full"><div class="fixed inset-0 flex justify-center sm:px-8"><div class="flex w-full max-w-7xl lg:px-8"><div class="w-full bg-white ring-1 ring-zinc-100 dark:bg-zinc-900 dark:ring-zinc-300/20"></div></div></div><div class="relative flex w-full flex-col"><header class="pointer-events-none relative z-50 flex flex-none flex-col" style="height:var(--header-height);margin-bottom:var(--header-mb)"><div class="order-last mt-[calc(theme(spacing.16)-theme(spacing.3))]"></div><div class="sm:px-8 top-0 order-last -mb-3 pt-3" style="position:var(--header-position)"><div class="mx-auto w-full max-w-7xl lg:px-8"><div class="relative px-4 sm:px-8 lg:px-12"><div class="mx-auto max-w-2xl lg:max-w-5xl"><div class="top-[var(--avatar-top,theme(spacing.3))] w-full" style="position:var(--header-inner-position)"><div class="relative"><div class="absolute left-0 top-3 origin-left transition-opacity h-10 w-10 rounded-full bg-white/90 p-0.5 shadow-lg shadow-zinc-800/5 ring-1 ring-zinc-900/5 backdrop-blur dark:bg-zinc-800/90 dark:ring-white/10" style="opacity:var(--avatar-border-opacity, 0);transform:var(--avatar-border-transform)"></div><a aria-label="Home" class="block h-16 w-16 origin-left pointer-events-auto" style="transform:var(--avatar-image-transform)" href="/"><img alt="" fetchpriority="high" width="1872" height="1914" decoding="async" data-nimg="1" class="rounded-full bg-zinc-100 object-cover dark:bg-zinc-800 h-16 w-16" style="color:transparent" src="/_next/static/media/graziano-casto.17cc67b9.jpg"/></a></div></div></div></div></div></div><div class="top-0 z-10 h-16 pt-6" style="position:var(--header-position)"><div class="sm:px-8 top-[var(--header-top,theme(spacing.6))] w-full" style="position:var(--header-inner-position)"><div class="mx-auto w-full max-w-7xl lg:px-8"><div class="relative px-4 sm:px-8 lg:px-12"><div class="mx-auto max-w-2xl lg:max-w-5xl"><div class="relative flex gap-4"><div class="flex flex-1"></div><div class="flex flex-1 justify-end md:justify-center"><div class="pointer-events-auto md:hidden" data-headlessui-state=""><button class="group flex items-center rounded-full bg-white/90 px-4 py-2 text-sm font-medium text-zinc-800 shadow-lg shadow-zinc-800/5 ring-1 ring-zinc-900/5 backdrop-blur dark:bg-zinc-800/90 dark:text-zinc-200 dark:ring-white/10 dark:hover:ring-white/20" type="button" aria-expanded="false" data-headlessui-state="">Menu<svg viewBox="0 0 8 6" aria-hidden="true" class="ml-3 h-auto w-2 stroke-zinc-500 group-hover:stroke-zinc-700 dark:group-hover:stroke-zinc-400"><path d="M1.75 1.75 4 4.25l2.25-2.5" fill="none" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></button></div><div hidden="" style="position:fixed;top:1px;left:1px;width:1px;height:0;padding:0;margin:-1px;overflow:hidden;clip:rect(0, 0, 0, 0);white-space:nowrap;border-width:0;display:none"></div><nav class="pointer-events-auto hidden md:block"><ul class="flex rounded-full bg-white/90 px-3 text-sm font-medium text-zinc-800 shadow-lg shadow-zinc-800/5 ring-1 ring-zinc-900/5 backdrop-blur dark:bg-zinc-800/90 dark:text-zinc-200 dark:ring-white/10"><li><a class="relative block px-3 py-2 transition hover:text-teal-500 dark:hover:text-teal-400" href="/about-me/">About</a></li><li><a class="relative block px-3 py-2 transition hover:text-teal-500 dark:hover:text-teal-400" href="/articles/">Articles</a></li><li><a class="relative block px-3 py-2 transition hover:text-teal-500 dark:hover:text-teal-400" href="/speaking/">Speaking</a></li><li><a class="relative block px-3 py-2 transition hover:text-teal-500 dark:hover:text-teal-400" href="/projects/">Projects</a></li></ul></nav></div><div class="flex justify-end md:flex-1"><div class="pointer-events-auto"><button type="button" aria-label="Toggle theme" class="group rounded-full bg-white/90 px-3 py-2 shadow-lg shadow-zinc-800/5 ring-1 ring-zinc-900/5 backdrop-blur transition dark:bg-zinc-800/90 dark:ring-white/10 dark:hover:ring-white/20"><svg viewBox="0 0 24 24" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" aria-hidden="true" class="h-6 w-6 fill-zinc-100 stroke-zinc-500 transition group-hover:fill-zinc-200 group-hover:stroke-zinc-700 dark:hidden [@media(prefers-color-scheme:dark)]:fill-teal-50 [@media(prefers-color-scheme:dark)]:stroke-teal-500 [@media(prefers-color-scheme:dark)]:group-hover:fill-teal-50 [@media(prefers-color-scheme:dark)]:group-hover:stroke-teal-600"><path d="M8 12.25A4.25 4.25 0 0 1 12.25 8v0a4.25 4.25 0 0 1 4.25 4.25v0a4.25 4.25 0 0 1-4.25 4.25v0A4.25 4.25 0 0 1 8 12.25v0Z"></path><path d="M12.25 3v1.5M21.5 12.25H20M18.791 18.791l-1.06-1.06M18.791 5.709l-1.06 1.06M12.25 20v1.5M4.5 12.25H3M6.77 6.77 5.709 5.709M6.77 17.73l-1.061 1.061" fill="none"></path></svg><svg viewBox="0 0 24 24" aria-hidden="true" class="hidden h-6 w-6 fill-zinc-700 stroke-zinc-500 transition dark:block [@media(prefers-color-scheme:dark)]:group-hover:stroke-zinc-400 [@media_not_(prefers-color-scheme:dark)]:fill-teal-400/10 [@media_not_(prefers-color-scheme:dark)]:stroke-teal-500"><path d="M17.25 16.22a6.937 6.937 0 0 1-9.47-9.47 7.451 7.451 0 1 0 9.47 9.47ZM12.75 7C17 7 17 2.75 17 2.75S17 7 21.25 7C17 7 17 11.25 17 11.25S17 7 12.75 7Z" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></button></div></div></div></div></div></div></div></div></header><div class="flex-none" style="height:var(--content-offset)"></div><main class="flex-auto"><div class="sm:px-8 mt-9"><div class="mx-auto w-full max-w-7xl lg:px-8"><div class="relative px-4 sm:px-8 lg:px-12"><div class="mx-auto max-w-2xl lg:max-w-5xl"><div class="max-w-2xl"><h1 class="text-4xl font-bold tracking-tight text-zinc-800 sm:text-5xl dark:text-zinc-100">Graziano Casto üëãüèª</h1><p class="mt-6 text-base text-zinc-600 dark:text-zinc-400"> Devloper Relations Engineer @ Akamas | Tech Lead @ CNCF TAG Developer Experience | Kubernetes v1.35 Release Comms Lead | Kubernetes Blog Maintainer | OSS &amp; Cloud Native Advocate </p><div class="mt-6 flex gap-6"><a class="group -m-1 p-1" aria-label="Follow on GitHub" target="_blank" rel="noopener noreferrer" href="https://github.com/graz-dev"><svg viewBox="0 0 24 24" aria-hidden="true" class="h-6 w-6 fill-zinc-500 transition group-hover:fill-zinc-600 dark:fill-zinc-400 dark:group-hover:fill-zinc-300"><path fill-rule="evenodd" clip-rule="evenodd" d="M12 2C6.475 2 2 6.588 2 12.253c0 4.537 2.862 8.369 6.838 9.727.5.09.687-.218.687-.487 0-.243-.013-1.05-.013-1.91C7 20.059 6.35 18.957 6.15 18.38c-.113-.295-.6-1.205-1.025-1.448-.35-.192-.85-.667-.013-.68.788-.012 1.35.744 1.538 1.051.9 1.551 2.338 1.116 2.912.846.088-.666.35-1.115.638-1.371-2.225-.256-4.55-1.14-4.55-5.062 0-1.115.387-2.038 1.025-2.756-.1-.256-.45-1.307.1-2.717 0 0 .837-.269 2.75 1.051.8-.23 1.65-.346 2.5-.346.85 0 1.7.115 2.5.346 1.912-1.333 2.75-1.05 2.75-1.05.55 1.409.2 2.46.1 2.716.637.718 1.025 1.628 1.025 2.756 0 3.934-2.337 4.806-4.562 5.062.362.32.675.936.675 1.897 0 1.371-.013 2.473-.013 2.82 0 .268.188.589.688.486a10.039 10.039 0 0 0 4.932-3.74A10.447 10.447 0 0 0 22 12.253C22 6.588 17.525 2 12 2Z"></path></svg></a><a class="group -m-1 p-1" aria-label="Follow on LinkedIn" target="_blank" rel="noopener noreferrer" href="https://www.linkedin.com/in/castograziano/"><svg viewBox="0 0 24 24" aria-hidden="true" class="h-6 w-6 fill-zinc-500 transition group-hover:fill-zinc-600 dark:fill-zinc-400 dark:group-hover:fill-zinc-300"><path d="M18.335 18.339H15.67v-4.177c0-.996-.02-2.278-1.39-2.278-1.389 0-1.601 1.084-1.601 2.205v4.25h-2.666V9.75h2.56v1.17h.035c.358-.674 1.228-1.387 2.528-1.387 2.7 0 3.2 1.778 3.2 4.091v4.715zM7.003 8.575a1.546 1.546 0 01-1.548-1.549 1.548 1.548 0 111.547 1.549zm1.336 9.764H5.666V9.75H8.34v8.589zM19.67 3H4.329C3.593 3 3 3.58 3 4.297v15.406C3 20.42 3.594 21 4.328 21h15.338C20.4 21 21 20.42 21 19.703V4.297C21 3.58 20.4 3 19.666 3h.003z"></path></svg></a></div></div></div></div></div></div><div class="sm:px-8 mt-12"><div class="mx-auto w-full max-w-7xl lg:px-8"><div class="relative px-4 sm:px-8 lg:px-12"><div class="mx-auto max-w-2xl lg:max-w-5xl"><div class="mx-auto grid max-w-4xl grid-cols-1 gap-y-20 lg:max-w-none lg:grid-cols-2 lg:gap-x-12"><a href="https://www.linkedin.com/build-relation/newsletter-follow?entityUrn=7267281680451731457" target="_blank" rel="noopener noreferrer" class="block"><div class="rounded-2xl border border-zinc-100 p-6 dark:border-zinc-700/40"><h2 class="text-base font-semibold tracking-tight text-zinc-800 dark:text-zinc-100">Cloud Native Friday Newsletter</h2><p class="relative z-10 mt-2 text-sm text-zinc-600 dark:text-zinc-400">Subscribe to my LinkedIn newsletter for weekly insights on Cloud Native technologies, Platform Engineering and Developer Experience.</p><div aria-hidden="true" class="relative z-10 mt-2 flex items-center text-sm font-medium text-teal-500">Subscribe on LinkedIn<svg viewBox="0 0 16 16" fill="none" aria-hidden="true" class="ml-1 h-4 w-4 stroke-current"><path d="M6.75 5.75 9.25 8l-2.5 2.25" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></div></div></a><a href="https://community.cncf.io/cloud-native-los-angeles/" target="_blank" rel="noopener noreferrer" class="block"><div class="rounded-2xl border border-zinc-100 p-6 dark:border-zinc-700/40"><h2 class="text-base font-semibold tracking-tight text-zinc-800 dark:text-zinc-100">Pods &amp; Platforms</h2><p class="relative z-10 mt-2 text-sm text-zinc-600 dark:text-zinc-400">Discover our event series at CNCF Los Angeles, where industry experts explore the latest trends and best practices in Cloud Native and Platform Engineering.</p><div aria-hidden="true" class="relative z-10 mt-2 flex items-center text-sm font-medium text-teal-500">Watch Now<svg viewBox="0 0 16 16" fill="none" aria-hidden="true" class="ml-1 h-4 w-4 stroke-current"><path d="M6.75 5.75 9.25 8l-2.5 2.25" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></div></div></a></div></div></div></div></div><div class="sm:px-8 mt-12"><div class="mx-auto w-full max-w-7xl lg:px-8"><div class="relative px-4 sm:px-8 lg:px-12"><div class="mx-auto max-w-2xl lg:max-w-5xl"><div class="mx-auto grid max-w-4xl grid-cols-1 gap-y-20 lg:max-w-none lg:grid-cols-2 lg:gap-x-12 lg:items-start"><div class="space-y-10"><div class="rounded-2xl border border-zinc-100 p-6 dark:border-zinc-700/40"><h2 class="text-sm font-semibold text-zinc-900 dark:text-zinc-100">Latest Articles</h2><div class="mt-6 space-y-8"><a class="block" href="/articles/kubernetes-v135-announcement/"><article class="group relative flex flex-col items-start"><div class="group relative flex flex-col items-start"><h2 class="flex items-center gap-3 mb-2"><span class="text-base font-semibold tracking-tight text-zinc-800 dark:text-zinc-100 flex-1 overflow-hidden" style="display:-webkit-box;-webkit-line-clamp:2;-webkit-box-orient:vertical">Kubernetes v1.35: Timbernetes (The World Tree Release)</span></h2><time class="relative z-10 order-first mb-3 flex items-center text-sm text-zinc-400 dark:text-zinc-500 pl-3.5" dateTime="2025-12-17"><span class="absolute inset-y-0 left-0 flex items-center" aria-hidden="true"><span class="h-4 w-0.5 rounded-full bg-zinc-200 dark:bg-zinc-500"></span></span>December 17, 2025</time><p class="relative z-10 mt-2 text-sm text-zinc-600 dark:text-zinc-400">Kubernetes v1.35 introduces 60 enhancements including 17 stable features. Discover the latest improvements in security, scheduling, and resource management in this comprehensive release.</p><div aria-hidden="true" class="relative z-10 mt-2 flex items-center text-sm font-medium text-teal-500">Read article<svg viewBox="0 0 16 16" fill="none" aria-hidden="true" class="ml-1 h-4 w-4 stroke-current"><path d="M6.75 5.75 9.25 8l-2.5 2.25" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></div></div></article></a><a class="block" href="/articles/kubernetes-v135-sneak-peak/"><article class="group relative flex flex-col items-start"><div class="group relative flex flex-col items-start"><h2 class="flex items-center gap-3 mb-2"><span class="text-base font-semibold tracking-tight text-zinc-800 dark:text-zinc-100 flex-1 overflow-hidden" style="display:-webkit-box;-webkit-line-clamp:2;-webkit-box-orient:vertical">Kubernetes v1.35 Sneak Peek</span></h2><time class="relative z-10 order-first mb-3 flex items-center text-sm text-zinc-400 dark:text-zinc-500 pl-3.5" dateTime="2025-11-26"><span class="absolute inset-y-0 left-0 flex items-center" aria-hidden="true"><span class="h-4 w-0.5 rounded-full bg-zinc-200 dark:bg-zinc-500"></span></span>November 26, 2025</time><p class="relative z-10 mt-2 text-sm text-zinc-600 dark:text-zinc-400">Explore the upcoming features in Kubernetes v1.35, including node declared features, in-place pod resource updates, and native pod certificates, along with important deprecations like cgroup v1 removal.</p><div aria-hidden="true" class="relative z-10 mt-2 flex items-center text-sm font-medium text-teal-500">Read article<svg viewBox="0 0 16 16" fill="none" aria-hidden="true" class="ml-1 h-4 w-4 stroke-current"><path d="M6.75 5.75 9.25 8l-2.5 2.25" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></div></div></article></a><a class="block" href="/articles/kubernetes-v134-announcement/"><article class="group relative flex flex-col items-start"><div class="group relative flex flex-col items-start"><h2 class="flex items-center gap-3 mb-2"><span class="text-base font-semibold tracking-tight text-zinc-800 dark:text-zinc-100 flex-1 overflow-hidden" style="display:-webkit-box;-webkit-line-clamp:2;-webkit-box-orient:vertical">Kubernetes v1.34: Of Wind &amp; Will (O&#x27; WaW)</span></h2><time class="relative z-10 order-first mb-3 flex items-center text-sm text-zinc-400 dark:text-zinc-500 pl-3.5" dateTime="2025-08-27"><span class="absolute inset-y-0 left-0 flex items-center" aria-hidden="true"><span class="h-4 w-0.5 rounded-full bg-zinc-200 dark:bg-zinc-500"></span></span>August 27, 2025</time><p class="relative z-10 mt-2 text-sm text-zinc-600 dark:text-zinc-400">Kubernetes v1.34 introduces 58 enhancements including 23 stable features, from Dynamic Resource Allocation (DRA) GA to KYAML support. Discover the latest improvements in security, scheduling, and resource management in this comprehensive release.</p><div aria-hidden="true" class="relative z-10 mt-2 flex items-center text-sm font-medium text-teal-500">Read article<svg viewBox="0 0 16 16" fill="none" aria-hidden="true" class="ml-1 h-4 w-4 stroke-current"><path d="M6.75 5.75 9.25 8l-2.5 2.25" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></div></div></article></a><a class="block" href="/articles/ephemetal-cloud-with-crossplane-and-kubegreen/"><article class="group relative flex flex-col items-start"><div class="group relative flex flex-col items-start"><h2 class="flex items-center gap-3 mb-2"><span class="text-base font-semibold tracking-tight text-zinc-800 dark:text-zinc-100 flex-1 overflow-hidden" style="display:-webkit-box;-webkit-line-clamp:2;-webkit-box-orient:vertical">The Ephemeral Cloud: A New Blueprint for Infrastructure Efficiency With Crossplane and kube-green</span></h2><time class="relative z-10 order-first mb-3 flex items-center text-sm text-zinc-400 dark:text-zinc-500 pl-3.5" dateTime="2025-08-25"><span class="absolute inset-y-0 left-0 flex items-center" aria-hidden="true"><span class="h-4 w-0.5 rounded-full bg-zinc-200 dark:bg-zinc-500"></span></span>August 25, 2025</time><p class="relative z-10 mt-2 text-sm text-zinc-600 dark:text-zinc-400">Use Crossplane and kube-green to replace wasteful, always-on cloud environments with on-demand, ephemeral ones, dramatically cutting costs and environmental impact.</p><div aria-hidden="true" class="relative z-10 mt-2 flex items-center text-sm font-medium text-teal-500">Read article<svg viewBox="0 0 16 16" fill="none" aria-hidden="true" class="ml-1 h-4 w-4 stroke-current"><path d="M6.75 5.75 9.25 8l-2.5 2.25" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></div></div></article></a></div></div></div><div class="space-y-10"><div class="rounded-2xl border border-zinc-100 p-6 dark:border-zinc-700/40"><h2 class="text-sm font-semibold text-zinc-900 dark:text-zinc-100">Upcoming Talks</h2><ol class="mt-6 space-y-4"><li class="flex gap-4"><div class="relative mt-1 flex h-10 w-10 flex-none items-center justify-center rounded-full shadow-md shadow-zinc-800/5 ring-1 ring-zinc-900/5 dark:border dark:border-zinc-700/50 dark:bg-zinc-800 dark:ring-0"><svg class="h-6 w-6 text-zinc-400 dark:text-zinc-500" viewBox="0 0 24 24" fill="none" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"><path d="M12 2a3 3 0 0 0-3 3v7a3 3 0 0 0 6 0V5a3 3 0 0 0-3-3Z" class="fill-zinc-100 stroke-zinc-400 dark:fill-zinc-100/10 dark:stroke-zinc-500"></path><path d="M19 10v2a7 7 0 0 1-14 0v-2" class="stroke-zinc-400 dark:stroke-zinc-500"></path><path d="M12 18v4M8 22h8" class="stroke-zinc-400 dark:stroke-zinc-500"></path></svg></div><dl class="flex flex-auto flex-wrap gap-x-2"><dt class="sr-only">Event</dt><dd class="w-full flex-none text-sm font-medium text-zinc-900 dark:text-zinc-100"><a target="_blank" rel="noopener noreferrer" href="https://fosdem.org/2026/schedule/event/UFFUHQ-your_cluster_is_lying_to_argocd_and_how_to_catch_it/">FOSDEM</a></dd><dt class="sr-only">Talk Title</dt><dd class="text-xs text-zinc-500 dark:text-zinc-400">Your Cluster is Lying to ArgoCD (And How to Catch It)</dd><dt class="sr-only">Location and Date</dt><dd class="text-xs text-zinc-500 dark:text-zinc-400">üáßüá™ Brussels<!-- --> - <time dateTime="2026-02-01">February 1, 2026</time></dd></dl></li><li class="flex gap-4"><div class="relative mt-1 flex h-10 w-10 flex-none items-center justify-center rounded-full shadow-md shadow-zinc-800/5 ring-1 ring-zinc-900/5 dark:border dark:border-zinc-700/50 dark:bg-zinc-800 dark:ring-0"><svg class="h-6 w-6 text-zinc-400 dark:text-zinc-500" viewBox="0 0 24 24" fill="none" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"><path d="M12 2a3 3 0 0 0-3 3v7a3 3 0 0 0 6 0V5a3 3 0 0 0-3-3Z" class="fill-zinc-100 stroke-zinc-400 dark:fill-zinc-100/10 dark:stroke-zinc-500"></path><path d="M19 10v2a7 7 0 0 1-14 0v-2" class="stroke-zinc-400 dark:stroke-zinc-500"></path><path d="M12 18v4M8 22h8" class="stroke-zinc-400 dark:stroke-zinc-500"></path></svg></div><dl class="flex flex-auto flex-wrap gap-x-2"><dt class="sr-only">Event</dt><dd class="w-full flex-none text-sm font-medium text-zinc-900 dark:text-zinc-100"><a target="_blank" rel="noopener noreferrer" href="https://colocatedeventseu2026.sched.com/event/2DY1Y/making-observability-work-at-the-edge-julia-furst-morgado-dash0-graziano-casto-akamas">Kubecon EU - Edge Day</a></dd><dt class="sr-only">Talk Title</dt><dd class="text-xs text-zinc-500 dark:text-zinc-400">Making Observability Work at the Edge</dd><dt class="sr-only">Location and Date</dt><dd class="text-xs text-zinc-500 dark:text-zinc-400">üá≥üá± Amsterdam<!-- --> - <time dateTime="2026-03-23">March 23, 2026</time></dd></dl></li><li class="flex gap-4"><div class="relative mt-1 flex h-10 w-10 flex-none items-center justify-center rounded-full shadow-md shadow-zinc-800/5 ring-1 ring-zinc-900/5 dark:border dark:border-zinc-700/50 dark:bg-zinc-800 dark:ring-0"><svg class="h-6 w-6 text-zinc-400 dark:text-zinc-500" viewBox="0 0 24 24" fill="none" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"><path d="M12 2a3 3 0 0 0-3 3v7a3 3 0 0 0 6 0V5a3 3 0 0 0-3-3Z" class="fill-zinc-100 stroke-zinc-400 dark:fill-zinc-100/10 dark:stroke-zinc-500"></path><path d="M19 10v2a7 7 0 0 1-14 0v-2" class="stroke-zinc-400 dark:stroke-zinc-500"></path><path d="M12 18v4M8 22h8" class="stroke-zinc-400 dark:stroke-zinc-500"></path></svg></div><dl class="flex flex-auto flex-wrap gap-x-2"><dt class="sr-only">Event</dt><dd class="w-full flex-none text-sm font-medium text-zinc-900 dark:text-zinc-100"><a target="_blank" rel="noopener noreferrer" href="https://kccnceu2026.sched.com/event/2EF5e/the-next-chapter-of-developer-experience-tag-devex-in-action-julien-semaan-kubex-graziano-casto-akamas-mona-borham-swenginio-kevin-dubois-ibm-joshua-bezaleel-abednego-goto-financial">Kubecon EU - Maintainer Track</a></dd><dt class="sr-only">Talk Title</dt><dd class="text-xs text-zinc-500 dark:text-zinc-400">The Next Chapter of Developer Experience: TAG DevEx in Action</dd><dt class="sr-only">Location and Date</dt><dd class="text-xs text-zinc-500 dark:text-zinc-400">üá≥üá± Amsterdam<!-- --> - <time dateTime="2026-03-23">March 25, 2026</time></dd></dl></li></ol></div></div></div></div></div></div></div></main><footer class="mt-32 flex-none"><div class="sm:px-8"><div class="mx-auto w-full max-w-7xl lg:px-8"><div class="border-t border-zinc-100 pb-16 pt-10 dark:border-zinc-700/40"><div class="relative px-4 sm:px-8 lg:px-12"><div class="mx-auto max-w-2xl lg:max-w-5xl"><div class="flex flex-col items-center justify-between gap-6 sm:flex-row"><div class="flex flex-wrap justify-center gap-x-6 gap-y-1 text-sm font-medium text-zinc-800 dark:text-zinc-200"><a class="transition hover:text-teal-500 dark:hover:text-teal-400" href="/about-me/">About</a><a class="transition hover:text-teal-500 dark:hover:text-teal-400" href="/projects/">Projects</a><a class="transition hover:text-teal-500 dark:hover:text-teal-400" href="/speaking/">Speaking</a></div><p class="text-sm text-zinc-500 dark:text-zinc-400">¬© <!-- -->2026<!-- --> Graziano Casto. All rights reserved.</p></div></div></div></div></div></div></footer></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"data":{"pageTitle":"Graziano Casto - DevRel Engineer","pageDescription":"Welcome to the personal website of Graziano Casto.","pageLink":"https://castograziano.com","pageImage":"https://castograziano.com/casto-graziano.jpg","articles":[{"slug":"kubernetes-v135-announcement","content":"\n[Published by Kubernetes co-authored with Aakanksha Bhende, Arujjwal Negi, Chad M. Crowell, Swathi Rao](https://kubernetes.io/blog/2025/12/17/kubernetes-v1-35-release/)\n\nSimilar to previous releases, the release of Kubernetes v1.35 introduces new stable, beta, and alpha features. The consistent delivery of high-quality releases underscores the strength of our development cycle and the vibrant support from our community.\n\nThis release consists of 60 enhancements, including 17 stable, 19 beta, and 22 alpha features.\n\nThere are also some [deprecations and removals](https://kubernetes.io/blog/2025/12/17/kubernetes-v1-35-release/#deprecations-and-removals) in this release; make sure to read about those.\n\n## Release theme and logo\n\n![Kubernetes v1.35 Timbernetes logo: a storybook hex badge with a glowing world tree whose branches cradle Earth and a white Kubernetes wheel; three cheerful squirrels stand below‚Äîa wizard in a plum robe holding an LGTM scroll, a warrior with an axe and blue Kubernetes shield, and a lantern-carrying rogue in a navy cloak‚Äîon green grass above a gold ribbon reading World Tree Release, backed by soft mountains and cloud-swept sky](https://kubernetes.io/blog/2025/12/17/kubernetes-v1-35-release/k8s-v1.35.png)\n\nKubernetes v1.35 Timbernetes logo: a storybook hex badge with a glowing world tree whose branches cradle Earth and a white Kubernetes wheel; three cheerful squirrels stand below‚Äîa wizard in a plum robe holding an LGTM scroll, a warrior with an axe and blue Kubernetes shield, and a lantern-carrying rogue in a navy cloak‚Äîon green grass above a gold ribbon reading World Tree Release, backed by soft mountains and cloud-swept sky\n\n2025 began in the shimmer of Octarine: The Color of Magic (v1.33) and rode the gusts Of Wind \u0026 Will (v1.34). We close the year with our hands on the World Tree, inspired by Yggdrasil, the tree of life that binds many realms. Like any great tree, Kubernetes grows ring by ring and release by release, shaped by the care of a global community.\n\nAt its center sits the Kubernetes wheel wrapped around the Earth, grounded by the resilient maintainers, contributors and users who keep showing up. Between day jobs, life changes, and steady open-source stewardship, they prune old APIs, graft new features and keep one of the world‚Äôs largest open source projects healthy.\n\nThree squirrels guard the tree: a wizard holding the LGTM scroll for reviewers, a warrior with an axe and Kubernetes shield for the release crews who cut new branches, and a rogue with a lantern for the triagers who bring light to dark issue queues.\n\nTogether, they stand in for a much larger adventuring party. Kubernetes v1.35 adds another growth ring to the World Tree, a fresh cut shaped by many hands, many paths and a community whose branches reach higher as its roots grow deeper.\n\n## Spotlight on key updates\n\nKubernetes v1.35 is packed with new features and improvements. Here are a few select updates the Release Team would like to highlight!\n\n### Stable: In-place update of Pod resources\n\nKubernetes has graduated in-place updates for Pod resources to General Availability (GA). This feature allows users to adjust CPU and memory resources without restarting Pods or Containers. Previously, such modifications required recreating Pods, which could disrupt workloads, particularly for stateful or batch applications. Earlier Kubernetes releases allowed you to change only infrastructure resource settings (requests and limits) for existing Pods. The new in-place functionality allows for smoother, nondisruptive vertical scaling, improves efficiency, and can also simplify development.\n\nThis work was done as part of [KEP #1287](https://kep.k8s.io/1287) led by SIG Node.\n\n### Beta: Pod certificates for workload identity and security\n\nPreviously, delivering certificates to pods required external controllers (cert-manager, SPIFFE/SPIRE), CRD orchestration, and Secret management, with rotation handled by sidecars or init containers. Kubernetes v1.35 enables native workload identity with automated certificate rotation, drastically simplifying service mesh and zero-trust architectures.\n\nNow, the `kubelet` generates keys, requests certificates via PodCertificateRequest, and writes credential bundles directly to the Pod's filesystem. The `kube-apiserver` enforces node restriction at admission time, eliminating the most common pitfall for third-party signers: accidentally violating node isolation boundaries. This enables pure mTLS flows with no bearer tokens in the issuance path.\n\nThis work was done as part of [KEP #4317](https://kep.k8s.io/4317) led by SIG Auth.\n\n### Alpha: Node declared features before scheduling\n\nWhen control planes enable new features but nodes lag behind (permitted by Kubernetes skew policy), the scheduler can place pods requiring those features onto incompatible older nodes. The node-declaration features framework allows nodes to declare their supported Kubernetes features. With the new alpha feature enabled, a Node reports the features it supports, publishing this information to the control plane via a new `.status.declaredFeatures` field. Then, the `kube-scheduler`, admission controllers, and third-party components can use these declarations. For example, you can enforce scheduling and API validation constraints to ensure that Pods run only on compatible nodes.\n\nThis work was done as part of [KEP #5328](https://kep.k8s.io/5328) led by SIG Node.\n\n## Features graduating to Stable\n\n*This is a selection of some of the improvements that are now stable following the v1.35 release.*\n\n### PreferSameNode traffic distribution\n\nThe `trafficDistribution` field for Services has been updated to provide more explicit control over traffic routing. A new option, `PreferSameNode`, has been introduced to let services strictly prioritize endpoints on the local node if available, falling back to remote endpoints otherwise.\n\nSimultaneously, the existing `PreferClose` option has been renamed to `PreferSameZone`. This change makes the API self-explanatory by explicitly indicating that traffic is preferred within the current availability zone. While `PreferClose` is preserved for backward compatibility, `PreferSameZone` is now the standard for zonal routing, ensuring that both node-level and zone-level preferences are clearly distinguished.\n\nThis work was done as part of [KEP #3015](https://kep.k8s.io/3015) led by SIG Network.\n\n### Job API managed-by mechanism\n\nThe Job API now includes a `managedBy` field that allows an external controller to handle Job status synchronization. This feature, which graduates to stable in Kubernetes v1.35, is primarily driven by [MultiKueue](https://github.com/kubernetes-sigs/kueue/tree/main/keps/693-multikueue), a multi-cluster dispatching system where a Job created in a management cluster is mirrored and executed in a worker cluster, with status updates propagated back. To enable this workflow, the built-in Job controller must not act on a particular Job resource so that the Kueue controller can manage status updates instead.\n\nThe goal is to allow clean delegation of Job synchronization to another controller. It does not aim to pass custom parameters to that controller or modify CronJob concurrency policies.\n\nThis work was done as part of [KEP #4368](https://kep.k8s.io/4368) led by SIG Apps.\n\nHistorically, the Pod API lacked the `metadata.generation` field found in other Kubernetes objects such as Deployments. Because of this omission, controllers and users had no reliable way to verify whether the `kubelet` had actually processed the latest changes to a Pod's specification. This ambiguity was particularly problematic for features like [In-Place Pod Vertical Scaling](https://kubernetes.io/blog/2025/12/17/kubernetes-v1-35-release/#stable-in-place-update-of-pod-resources), where it was difficult to know exactly when a resource resize request had been enacted.\n\nKubernetes v1.33 added `.metadata.generation` fields for Pods, as an alpha feature. That field is now stable in the v1.35 Pod API, which means that every time a Pod's `spec` is updated, the `.metadata.generation` value is incremented. As part of this improvement, the Pod API also gained a `.status.observedGeneration` field, which reports the generation that the `kubelet` has successfully seen and processed. Pod conditions also each contain their own individual `observedGeneration` field that clients can report and / or observe.\n\nBecause this feature has graduated to stable in v1.35, it is available for all workloads.\n\nThis work was done as part of [KEP #5067](https://kep.k8s.io/5067) led by SIG Node.\n\n### Configurable NUMA node limit for topology manager\n\nThe [topology manager](https://kubernetes.io/docs/concepts/policy/node-resource-managers/) historically used a hard-coded limit of 8 for the maximum number of NUMA nodes it can support, preventing state explosion during affinity calculation. (There's an important detail here; a *NUMA node* is not the same as a Node in the Kubernetes API.) This limit on the number of NUMA nodes prevented Kubernetes from fully utilizing modern high-end servers, which increasingly feature CPU architectures with more than 8 NUMA nodes.\n\nKubernetes v1.31 introduced a new, **beta** `max-allowable-numa-nodes` option to the topology manager policy configuration. In Kubernetes v1.35, that option is stable. Cluster administrators who enable it can use servers with more than 8 NUMA nodes.\n\nAlthough the configuration option is stable, the Kubernetes community is aware of the poor performance for large NUMA hosts, and there is a [proposed enhancement](https://kep.k8s.io/5726) (KEP-5726) that aims to improve on it. You can learn more about this by reading [Control Topology Management Policies on a node](https://kubernetes.io/docs/tasks/administer-cluster/topology-manager/).\n\nThis work was done as part of [KEP #4622](https://kep.k8s.io/4622) led by SIG Node.\n\n## New features in Beta\n\n*This is a selection of some of the improvements that are now beta following the v1.35 release.*\n\nAccessing node topology information, such as region and zone, from within a Pod has typically required querying the Kubernetes API server. While functional, this approach creates complexity and security risks by necessitating broad RBAC permissions or sidecar containers just to retrieve infrastructure metadata. Kubernetes v1.35 promotes the capability to expose node topology labels directly via the Downward API to beta.\n\nThe `kubelet` can now inject standard topology labels, such as `topology.kubernetes.io/zone` and `topology.kubernetes.io/region`, into Pods as environment variables or projected volume files. The primary benefit is a safer and more efficient way for workloads to be topology-aware. This allows applications to natively adapt to their availability zone or region without dependencies on the API server, strengthening security by upholding the principle of least privilege and simplifying cluster configuration.\n\n**Note:** Kubernetes now injects available topology labels to every Pod so that they can be used as inputs to the [downward API](https://kubernetes.io/docs/concepts/workloads/pods/downward-api/). With the v1.35 upgrade, most cluster administrators will see several new labels added to each Pod; this is expected as part of the design.\n\nThis work was done as part of [KEP #4742](https://kep.k8s.io/4742) led by SIG Node.\n\n### Native support for storage version migration\n\nIn Kubernetes v1.35, the native support for storage version migration graduates to beta and is enabled by default. This move integrates the migration logic directly into the core Kubernetes control plane (\"in-tree\"), eliminating the dependency on external tools.\n\nHistorically, administrators relied on manual \"read/write loops\"‚Äîoften piping `kubectl get` into `kubectl replace‚Äîto` update schemas or re-encrypt data at rest. This method was inefficient and prone to conflicts, especially for large resources like Secrets. With this release, the built-in controller automatically handles update conflicts and consistency tokens, providing a safe, streamlined, and reliable way to ensure stored data remains current with minimal operational overhead.\n\nThis work was done as part of [KEP #4192](https://kep.k8s.io/4192) led by SIG API Machinery.\n\n### Mutable Volume attach limits\n\nA CSI (Container Storage Interface) driver is a Kubernetes plugin that provides a consistent way for storage systems to be exposed to containerized workloads. The `CSINode` object records details about all CSI drivers installed on a node. However, a mismatch can arise between the reported and actual attachment capacity on nodes. When volume slots are consumed after a CSI driver starts up, the `kube-scheduler` may assign stateful pods to nodes without sufficient capacity, ultimately getting stuck in a `ContainerCreating` state.\n\nKubernetes v1.35 makes `CSINode.spec.drivers[*].allocatable.count` mutable so that a node‚Äôs available volume attachment capacity can be updated dynamically. It also allows CSI drivers to control how frequently the `allocatable.count` value is updated on all nodes by introducing a configurable refresh interval, defined through the `CSIDriver` object. Additionally, it automatically updates `CSINode.spec.drivers[*].allocatable.count` on detecting a failure in volume attachment due to insufficient capacity. Although this feature graduated to beta in v1.34 with the feature flag `MutableCSINodeAllocatableCount` disabled by default, it remains in beta for v1.35 to allow time for feedback, but the feature flag is enabled by default.\n\nThis work was done as part of [KEP #4876](https://kep.k8s.io/4876) led by SIG Storage.\n\n### Opportunistic batching\n\nHistorically, the Kubernetes scheduler processes pods sequentially with time complexity of `O(num pods √ó num nodes)`, which can result in redundant computation for compatible pods. This KEP introduces an opportunistic batching mechanism that aims to improve performance by identifying such compatible Pods via `Pod scheduling signature` and batching them together, allowing shared filtering and scoring results across them.\n\nThe pod scheduling signature ensures that two pods with the same signature are ‚Äúthe same‚Äù from a scheduling perspective. It takes into account not only the pod and node attributes, but also the other pods in the system and global data about the pod placement. This means that any pod with the given signature will get the same scores/feasibility results from any arbitrary set of nodes.\n\nThe batching mechanism consists of two operations that can be invoked whenever needed - *create* and *nominate*. Create leads to the creation of a new set of batch information from the scheduling results of Pods that have a valid signature. Nominate uses the batching information from create to set the nominated node name from a new Pod whose signature matches the canonical Pod‚Äôs signature.\n\nThis work was done as part of [KEP #5598](https://kep.k8s.io/5598) led by SIG Scheduling.\n\n### maxUnavailable for StatefulSets\n\nA StatefulSet runs a group of Pods and maintains a sticky identity for each of those Pods. This is critical for stateful workloads requiring stable network identifiers or persistent storage. When a StatefulSet's `.spec.updateStrategy.\u003ctype\u003e` is set to `RollingUpdate`, the StatefulSet controller will delete and recreate each Pod in the StatefulSet. It will proceed in the same order as Pod termination (from the largest ordinal to the smallest), updating each Pod one at a time.\n\nKubernetes v1.24 added a new **alpha** field to a StatefulSet's `rollingUpdate` configuration settings, called `maxUnavailable`. That field wasn't part of the Kubernetes API unless your cluster administrator explicitly opted in. In Kubernetes v1.35 that field is beta and is available by default. You can use it to define the maximum number of pods that can be unavailable during an update. This setting is most effective in combination with `.spec.podManagementPolicy` set to Parallel. You can set `maxUnavailable` as either a positive number (example: 2) or a percentage of the desired number of Pods (example: 10%). If this field is not specified, it will default to 1, to maintain the previous behavior of only updating one Pod at a time. This improvement allows stateful applications (that can tolerate more than one Pod being down) to finish updating faster.\n\nThis work was done as part of [KEP #961](https://kep.k8s.io/961) led by SIG Apps.\n\n### Configurable credential plugin policy in kuberc\n\nThe optional [`kuberc` file](https://kubernetes.io/docs/reference/kubectl/kuberc/) is a way to separate server configurations and cluster credentials from user preferences without disrupting already running CI pipelines with unexpected outputs.\n\nAs part of the v1.35 release, `kuberc` gains additional functionality which allows users to configure credential plugin policy. This change introduces two fields `credentialPluginPolicy`, which allows or denies all plugins, and allows specifying a list of allowed plugins using `credentialPluginAllowlist`.\n\nThis work was done as part of [KEP #3104](https://kep.k8s.io/3104) as a cooperation between SIG Auth and SIG CLI.\n\n### KYAML\n\nYAML is a human-readable format of data serialization. In Kubernetes, YAML files are used to define and configure resources, such as Pods, Services, and Deployments. However, complex YAML is difficult to read. YAML's significant whitespace requires careful attention to indentation and nesting, while its optional string-quoting can lead to unexpected type coercion (see: The Norway Bug). While JSON is an alternative, it lacks support for comments and has strict requirements for trailing commas and quoted keys.\n\nKYAML is a safer and less ambiguous subset of YAML designed specifically for Kubernetes. Introduced as an opt-in alpha feature in v1.34, this feature graduated to beta in Kubernetes v1.35 and has been enabled by default. It can be disabled by setting the environment variable `KUBECTL_KYAML=false`.\n\nKYAML addresses challenges pertaining to both YAML and JSON. All KYAML files are also valid YAML files. This means you can write KYAML and pass it as an input to any version of kubectl. This also means that you don‚Äôt need to write in strict KYAML for the input to be parsed.\n\nThis work was done as part of [KEP #5295](https://kep.k8s.io/5295) led by SIG CLI.\n\n### Configurable tolerance for HorizontalPodAutoscalers\n\nThe Horizontal Pod Autoscaler (HPA) has historically relied on a fixed, global 10% tolerance for scaling actions. A drawback of this hardcoded value was that workloads requiring high sensitivity, such as those needing to scale on a 5% load increase, were often blocked from scaling, while others might oscillate unnecessarily.\n\nWith Kubernetes v1.35, the configurable tolerance feature graduates to beta and is enabled by default. This enhancement allows users to define a custom tolerance window on a per-resource basis within the HPA `behavior` field. By setting a specific tolerance (e.g., lowering it to 0.05 for 5%), operators gain precise control over autoscaling sensitivity, ensuring that critical workloads react quickly to small metric changes, without requiring cluster-wide configuration adjustments.\n\nThis work was done as part of [KEP #4951](https://kep.k8s.io/4951) led by SIG Autoscaling.\n\n### Support for user namespaces in Pods\n\nKubernetes is adding support for user namespaces, allowing pods to run with isolated user and group ID mappings instead of sharing host IDs. This means containers can operate as root internally while actually being mapped to an unprivileged user on the host, reducing the risk of privilege escalation in the event of a compromise. The feature improves pod-level security and makes it safer to run workloads that need root inside the container. Over time, support has expanded to both stateless and stateful Pods through id-mapped mounts.\n\nThis work was done as part of [KEP #127](https://kep.k8s.io/127) led by SIG Node.\n\n### VolumeSource: OCI artifact and/or image\n\nWhen creating a Pod, you often need to provide data, binaries, or configuration files for your containers. This meant including the content into the main container image or using a custom init container to download and unpack files into an `emptyDir`. Both these approaches are still valid. Kubernetes v1.31 added support for the `image` volume type allowing Pods to declaratively pull and unpack OCI container image artifacts into a volume. This lets you package and deliver data-only artifacts such as configs, binaries, or machine learning models using standard OCI registry tools.\n\nWith this feature, you can fully separate your data from your container image and remove the need for extra init containers or startup scripts. The image volume type has been in beta since v1.33 and is enabled by default in v1.35. Please note that using this feature requires a compatible container runtime, such as containerd v2.1 or later.\n\nThis work was done as part of [KEP #4639](https://kep.k8s.io/4639) led by SIG Node.\n\n### Enforced kubelet credential verification for cached images\n\nThe `imagePullPolicy: IfNotPresent` setting currently allows a Pod to use a container image that is already cached on a node, even if the Pod itself does not possess the credentials to pull that image. A drawback of this behavior is that it creates a security vulnerability in multi-tenant clusters: if a Pod with valid credentials pulls a sensitive private image to a node, a subsequent unauthorized Pod on the same node can access that image simply by relying on the local cache.\n\nThis KEP introduces a mechanism where the `kubelet` enforces credential verification for cached images. Before allowing a Pod to use a locally cached image, the `kubelet` checks if the Pod has the valid credentials to pull it. This ensures that only authorized workloads can use private images, regardless of whether they are already present on the node, significantly hardening the security posture for shared clusters.\n\nIn Kubernetes v1.35, this feature has graduated to beta and is enabled by default. Users can still disable it by setting the `KubeletEnsureSecretPulledImages` feature gate to false. Additionally, the `imagePullCredentialsVerificationPolicy` flag allows operators to configure the desired security level, ranging from a mode that prioritizes backward compatibility to a strict enforcement mode that offers maximum security.\n\nThis work was done as part of [KEP #2535](https://kep.k8s.io/2535) led by SIG Node.\n\n### Fine-grained Container restart rules\n\nHistorically, the `restartPolicy` field was defined strictly at the Pod level, forcing the same behavior on all containers within a Pod. A drawback of this global setting was the lack of granularity for complex workloads, such as AI/ML training jobs. These often required `restartPolicy: Never` for the Pod to manage job completion, yet individual containers would benefit from in-place restarts for specific, retriable errors (like network glitches or GPU init failures).\n\nKubernetes v1.35 addresses this by enabling `restartPolicy` and `restartPolicyRules` within the container API itself. This allows users to define restart strategies for individual regular and init containers that operate independently of the Pod's overall policy. For example, a container can now be configured to restart automatically only if it exits with a specific error code, avoiding the expensive overhead of rescheduling the entire Pod for a transient failure.\n\nIn this release, the feature has graduated to beta and is enabled by default. Users can immediately leverage `restartPolicyRules` in their container specifications to optimize recovery times and resource utilization for long-running workloads, without altering the broader lifecycle logic of their Pods.\n\nThis work was done as part of [KEP #5307](https://kep.k8s.io/5307) led by SIG Node.\n\n### CSI driver opt-in for service account tokens via secrets field\n\nProviding ServiceAccount tokens to Container Storage Interface (CSI) drivers has traditionally relied on injecting them into the `volume_context` field. This approach presents a significant security risk because `volume_context` is intended for non-sensitive configuration data and is frequently logged in plain text by drivers and debugging tools, potentially leaking credentials.\n\nKubernetes v1.35 introduces an opt-in mechanism for CSI drivers to receive ServiceAccount tokens via the dedicated secrets field in the NodePublishVolume request. Drivers can now enable this behavior by setting the `serviceAccountTokenInSecrets` field to true in their CSIDriver object, instructing the `kubelet` to populate the token securely.\n\nThe primary benefit is the prevention of accidental credential exposure in logs and error messages. This change ensures that sensitive workload identities are handled via the appropriate secure channels, aligning with best practices for secret management while maintaining backward compatibility for existing drivers.\n\nThis work was done as part of [KEP #5538](https://kep.k8s.io/5538) led by SIG Auth in cooperation with SIG Storage.\n\n### Deployment status: count of terminating replicas\n\nHistorically, the Deployment status provided details on available and updated replicas but lacked explicit visibility into Pods that were in the process of shutting down. A drawback of this omission was that users and controllers could not easily distinguish between a stable Deployment and one that still had Pods executing cleanup tasks or adhering to long grace periods.\n\nKubernetes v1.35 promotes the `terminatingReplicas` field within the Deployment status to beta. This field provides a count of Pods that have a deletion timestamp set but have not yet been removed from the system. This feature is a foundational step in a larger initiative to improve how Deployments handle Pod replacement, laying the groundwork for future policies regarding when to create new Pods during a rollout.\n\nThe primary benefit is improved observability for lifecycle management tools and operators. By exposing the number of terminating Pods, external systems can now make more informed decisions such as waiting for a complete shutdown before proceeding with subsequent tasks without needing to manually query and filter individual Pod lists.\n\nThis work was done as part of [KEP #3973](https://kep.k8s.io/3973) led by SIG Apps.\n\n## New features in Alpha\n\n*This is a selection of some of the improvements that are now alpha following the v1.35 release.*\n\n### Gang scheduling support in Kubernetes\n\nScheduling interdependent workloads, such as AI/ML training jobs or HPC simulations, has traditionally been challenging because the default Kubernetes scheduler places Pods individually. This often leads to partial scheduling where some Pods start while others wait indefinitely for resources, resulting in deadlocks and wasted cluster capacity.\n\nKubernetes v1.35 introduces native support for so-called \"gang scheduling\" via the new Workload API and PodGroup concept. This feature implements an \"all-or-nothing\" scheduling strategy, ensuring that a defined group of Pods is scheduled only if the cluster has sufficient resources to accommodate the entire group simultaneously.\n\nThe primary benefit is improved reliability and efficiency for batch and parallel workloads. By preventing partial deployments, it eliminates resource deadlocks and ensures that expensive cluster capacity is utilized only when a complete job can run, significantly optimizing the orchestration of large-scale data processing tasks.\n\nThis work was done as part of [KEP #4671](https://kep.k8s.io/4671) led by SIG Scheduling.\n\n### Constrained impersonation\n\nHistorically, the `impersonate` verb in Kubernetes RBAC functioned on an all-or-nothing basis: once a user was authorized to impersonate a target identity, they gained all associated permissions. A drawback of this broad authorization was that it violated the principle of least privilege, preventing administrators from restricting impersonators to specific actions or resources.\n\nKubernetes v1.35 introduces a new alpha feature, constrained impersonation, which adds a secondary authorization check to the impersonation flow. When enabled via the `ConstrainedImpersonation` feature gate, the API server verifies not only the basic `impersonate` permission but also checks if the impersonator is authorized for the specific action using new verb prefixes (e.g., `impersonate-on:\u003cmode\u003e:\u003cverb\u003e`). This allows administrators to define fine-grained policies‚Äîsuch as permitting a support engineer to impersonate a cluster admin solely to view logs, without granting full administrative access.\n\nThis work was done as part of [KEP #5284](https://kep.k8s.io/5284) led by SIG Auth.\n\n### Flagz for Kubernetes components\n\nVerifying the runtime configuration of Kubernetes components, such as the API server or `kubelet`, has traditionally required privileged access to the host node or process arguments. To address this, the `/flagz` endpoint was introduced to expose command-line options via HTTP. However, its output was initially limited to plain text, making it difficult for automated tools to parse and validate configurations reliably.\n\nIn Kubernetes v1.35, the `/flagz` endpoint has been enhanced to support structured, machine-readable JSON output. Authorized users can now request a versioned JSON response using standard HTTP content negotiation, while the original plain text format remains available for human inspection. This update significantly improves observability and compliance workflows, allowing external systems to programmatically audit component configurations without fragile text parsing or direct infrastructure access.\n\nThis work was done as part of [KEP #4828](https://kep.k8s.io/4828) led by SIG Instrumentation.\n\n### Statusz for Kubernetes components\n\nTroubleshooting Kubernetes components like the `kube-apiserver` or `kubelet` has traditionally involved parsing unstructured logs or text output, which is brittle and difficult to automate. While a basic `/statusz` endpoint existed previously, it lacked a standardized, machine-readable format, limiting its utility for external monitoring systems.\n\nIn Kubernetes v1.35, the `/statusz` endpoint has been enhanced to support structured, machine-readable JSON output. Authorized users can now request this format using standard HTTP content negotiation to retrieve precise status data‚Äîsuch as version information and health indicators‚Äîwithout relying on fragile text parsing. This improvement provides a reliable, consistent interface for automated debugging and observability tools across all core components.\n\nThis work was done as part of [KEP #4827](https://kep.k8s.io/4827) led by SIG Instrumentation.\n\n### CCM: watch-based route controller reconciliation using informers\n\nManaging network routes within cloud environments has traditionally relied on the Cloud Controller Manager (CCM) periodically polling the cloud provider's API to verify and update route tables. This fixed-interval reconciliation approach can be inefficient, often generating a high volume of unnecessary API calls and introducing latency between a node state change and the corresponding route update.\n\nFor the Kubernetes v1.35 release, the cloud-controller-manager library introduces a watch-based reconciliation strategy for the route controller. Instead of relying on a timer, the controller now utilizes informers to watch for specific Node events, such as additions, deletions, or relevant field updates and triggers route synchronization only when a change actually occurs.\n\nThe primary benefit is a significant reduction in cloud provider API usage, which lowers the risk of hitting rate limits and reduces operational overhead. Additionally, this event-driven model improves the responsiveness of the cluster's networking layer by ensuring that route tables are updated immediately following changes in cluster topology.\n\nThis work was done as part of [KEP #5237](https://kep.k8s.io/5237) led by SIG Cloud Provider.\n\n### Extended toleration operators for threshold-based placement\n\nKubernetes v1.35 introduces SLA-aware scheduling by enabling workloads to express reliability requirements. The feature adds numeric comparison operators to tolerations, allowing pods to match or avoid nodes based on SLA-oriented taints such as service guarantees or fault-domain quality.\n\nThe primary benefit is enhancing the scheduler with more precise placement. Critical workloads can demand higher-SLA nodes, while lower priority workloads can opt into lower SLA ones. This improves utilization and reduces cost without compromising reliability.\n\nThis work was done as part of [KEP #5471](https://kep.k8s.io/5471) led by SIG Scheduling.\n\n### Mutable container resources when Job is suspended\n\nRunning batch workloads often involves trial and error with resource limits. Currently, the Job specification is immutable, meaning that if a Job fails due to an Out of Memory (OOM) error or insufficient CPU, the user cannot simply adjust the resources; they must delete the Job and create a new one, losing the execution history and status.\n\nKubernetes v1.35 introduces the capability to update resource requests and limits for Jobs that are in a suspended state. Enabled via the `MutableJobPodResourcesForSuspendedJobs` feature gate, this enhancement allows users to pause a failing Job, modify its Pod template with appropriate resource values, and then resume execution with the corrected configuration.\n\nThe primary benefit is a smoother recovery workflow for misconfigured jobs. By allowing in-place corrections during suspension, users can resolve resource bottlenecks without disrupting the Job's lifecycle identity or losing track of its completion status, significantly improving the developer experience for batch processing.\n\nThis work was done as part of [KEP #5440](https://kep.k8s.io/5440) led by SIG Apps.\n\n## Other notable changes\n\n### Continued innovation in Dynamic Resource Allocation (DRA)\n\nThe [core functionality](https://kep.k8s.io/4381) was graduated to stable in v1.34, with the ability to turn it off. In v1.35 it is always enabled. Several alpha features have also been significantly improved and are ready for testing. We encourage users to provide feedback on these capabilities to help clear the path for their target promotion to beta in upcoming releases.\n\n#### Extended Resource Requests via DRA\n\nSeveral functional gaps compared to Extended Resource requests via Device Plugins were addressed, for example scoring and reuse of devices in init containers.\n\n#### Device Taints and Tolerations\n\nThe new \"None\" effect can be used to report a problem without immediately affecting scheduling or running pod. DeviceTaintRule now provides status information about an ongoing eviction. The \"None\" effect can be used for a \"dry run\" before actually evicting pods:\n\n- Create DeviceTaintRule with \"effect: None\".\n- Check the status to see how many pods would be evicted.\n- Replace \"effect: None\" with \"effect: NoExecute\".\n\n#### Partitionable Devices\n\nDevices belonging to the same partitionable devices may now be defined in different ResourceSlices. You can read more in the [official documentation](https://kubernetes.io/docs/concepts/scheduling-eviction/dynamic-resource-allocation/#partitionable-devices).\n\n#### Consumable Capacity, Device Binding Conditions\n\nSeveral bugs were fixed and/or more tests added. You can learn more about [Consumable Capacity](https://kubernetes.io/docs/concepts/scheduling-eviction/dynamic-resource-allocation/#consumable-capacity) and [Binding Conditions](https://kubernetes.io/docs/concepts/scheduling-eviction/dynamic-resource-allocation/#device-binding-conditions) in the official documentation.\n\n### Comparable resource version semantics\n\nKubernetes v1.35 changes the way that clients are allowed to interpret [resource versions](https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions).\n\nBefore v1.35, the only supported comparison that clients could make was to check for string equality: if two resource versions were equal, they were the same. Clients could also provide a resource version to the API server and ask the control plane to do internal comparisons, such as streaming all events since a particular resource version.\n\nIn v1.35, all in-tree resource versions meet a new stricter definition: the values are a special form of decimal number. And, because they can be compared, clients can do their own operations to compare two different resource versions. For example, this means that a client reconnecting after a crash can detect when it has lost updates, as distinct from the case where there has been an update but no lost changes in the meantime.\n\nThis change in semantics enables other important use cases such as [storage version migration](https://kubernetes.io/docs/tasks/manage-kubernetes-objects/storage-version-migration/), performance improvements to *informers* (a client helper concept), and controller reliability. All of those cases require knowing whether one resource version is newer than another.\n\nThis work was done as part of [KEP #5504](https://kep.k8s.io/5504) led by SIG API Machinery.\n\n## Graduations, deprecations, and removals in v1.35\n\n### Graduations to stable\n\nThis lists all the features that graduated to stable (also known as *general availability*). For a full list of updates including new features and graduations from alpha to beta, see the release notes.\n\nThis release includes a total of 15 enhancements promoted to stable:\n\n- [Add CPUManager policy option to restrict reservedSystemCPUs to system daemons and interrupt processing](https://kep.k8s.io/4540)\n- [Pod Generation](https://kep.k8s.io/5067)\n- [Invariant Testing](https://kep.k8s.io/5468)\n- [In-Place Update of Pod Resources](https://kep.k8s.io/1287)\n- [Fine-grained SupplementalGroups control](https://kep.k8s.io/3619)\n- [Add support for a drop-in kubelet configuration directory](https://kep.k8s.io/3983)\n- [Remove gogo protobuf dependency for Kubernetes API types](https://kep.k8s.io/5589)\n- [kubelet image GC after a maximum age](https://kep.k8s.io/4210)\n- [Kubelet limit of Parallel Image Pulls](https://kep.k8s.io/3673)\n- [Add a TopologyManager policy option for MaxAllowableNUMANodes](https://kep.k8s.io/4622)\n- [Include kubectl command metadata in http request headers](https://kep.k8s.io/859)\n- [PreferSameNode Traffic Distribution (formerly PreferLocal traffic policy / Node-level topology)](https://kep.k8s.io/3015)\n- [Job API managed-by mechanism](https://kep.k8s.io/4368)\n- [Transition from SPDY to WebSockets](https://kep.k8s.io/4006)\n\n### Deprecations, removals and community updates\n\nAs Kubernetes develops and matures, features may be deprecated, removed, or replaced with better ones to improve the project's overall health. See the Kubernetes [deprecation and removal policy](https://kubernetes.io/docs/reference/using-api/deprecation-policy/) for more details on this process. Kubernetes v1.35 includes a couple of deprecations.\n\n#### Ingress NGINX retirement\n\nFor years, the Ingress NGINX controller has been a popular choice for routing traffic into Kubernetes clusters. It was flexible, widely adopted, and served as the standard entry point for countless applications.\n\nHowever, maintaining the project has become unsustainable. With a severe shortage of maintainers and mounting technical debt, the community recently made the difficult decision to retire it. This isn't strictly part of the v1.35 release, but it's such an important change that we wanted to highlight it here.\n\nConsequently, the Kubernetes project announced that Ingress NGINX will receive only best-effort maintenance until **March 2026**. After this date, it will be archived with no further updates. The recommended path forward is to migrate to the [Gateway API](https://gateway-api.sigs.k8s.io/), which offers a more modern, secure, and extensible standard for traffic management.\n\nYou can find more in the [official blog post](https://kubernetes.io/blog/2025/11/11/ingress-nginx-retirement/).\n\n#### Removal of cgroup v1 support\n\nWhen it comes to managing resources on Linux nodes, Kubernetes has historically relied on cgroups (control groups). While the original cgroup v1 was functional, it was often inconsistent and limited. That is why Kubernetes introduced support for cgroup v2 back in v1.25, offering a much cleaner, unified hierarchy and better resource isolation.\n\nBecause cgroup v2 is now the modern standard, Kubernetes is ready to retire the legacy cgroup v1 support in v1.35. This is an important notice for cluster administrators: if you are still running nodes on older Linux distributions that don't support cgroup v2, your `kubelet` will fail to start. To avoid downtime, you will need to migrate those nodes to systems where cgroup v2 is enabled.\n\nTo learn more, read [about cgroup v2](https://kubernetes.io/docs/concepts/architecture/cgroups/);  \nyou can also track the switchover work via [KEP-5573: Remove cgroup v1 support](https://kep.k8s.io/5573).\n\n#### Deprecation of ipvs mode in kube-proxy\n\nYears ago, Kubernetes adopted the [`ipvs`](https://kubernetes.io/docs/reference/networking/virtual-ips/#proxy-mode-ipvs) mode in `kube-proxy` to provide faster load balancing than the standard [`iptables`](https://kubernetes.io/docs/reference/networking/virtual-ips/#proxy-mode-iptables). While it offered a performance boost, keeping it in sync with evolving networking requirements created too much technical debt and complexity.\n\nBecause of this maintenance burden, Kubernetes v1.35 deprecates `ipvs` mode. Although the mode remains available in this release, `kube-proxy` will now emit a warning on startup when configured to use it. The goal is to streamline the codebase and focus on modern standards. For Linux nodes, you should begin transitioning to [`nftables`](https://kubernetes.io/docs/reference/networking/virtual-ips/#proxy-mode-nftables), which is now the recommended replacement.\n\nYou can find more in [KEP-5495: Deprecate ipvs mode in kube-proxy](https://kep.k8s.io/5495).\n\n#### Final call for containerd v1.X\n\nWhile Kubernetes v1.35 still supports containerd 1.7 and other LTS releases, this is the final version with such support. The SIG Node community has designated v1.35 as the last release to support the containerd v1.X series.\n\nThis serves as an important reminder: before upgrading to the next Kubernetes version, you must switch to containerd 2.0 or later. To help identify which nodes need attention, you can monitor the `kubelet_cri_losing_support` metric within your cluster.\n\nYou can find more in the [official blog post](https://kubernetes.io/blog/2025/09/12/kubernetes-v1-34-cri-cgroup-driver-lookup-now-ga/#announcement-kubernetes-is-deprecating-containerd-v1-y-support) or in [KEP-4033: Discover cgroup driver from CRI](https://kep.k8s.io/4033).\n\n#### Improved Pod stability during kubelet restarts\n\nPreviously, restarting the `kubelet` service often caused a temporary disruption in Pod status. During a restart, the kubelet would reset container states, causing healthy Pods to be marked as `NotReady` and removed from load balancers, even if the application itself was still running correctly.\n\nTo address this reliability issue, this behavior has been corrected to ensure seamless node maintenance. The `kubelet` now properly restores the state of existing containers from the runtime upon startup. This ensures that your workloads remain `Ready` and traffic continues to flow uninterrupted during `kubelet` restarts or upgrades.\n\nYou can find more in [KEP-4781: Fix inconsistent container ready state after kubelet restart](https://kep.k8s.io/4871).\n\n## Release notes\n\nCheck out the full details of the Kubernetes v1.35 release in our [release notes](https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.35.md).\n\n## Availability\n\nKubernetes v1.35 is available for download on or on the [Kubernetes download page](https://kubernetes.io/releases/download/).\n\nTo get started with Kubernetes, check out these [interactive tutorials](https://kubernetes.io/docs/tutorials/) or run local Kubernetes clusters using [minikube](https://minikube.sigs.k8s.io/). You can also easily install v1.35 using [kubeadm](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/).\n\n## Release team\n\nKubernetes is only possible with the support, commitment, and hard work of its community. Each release team is made up of dedicated community volunteers who work together to build the many pieces that make up the Kubernetes releases you rely on. This requires the specialized skills of people from all corners of our community, from the code itself to its documentation and project management.\n\n[We honor the memory of Han Kang](https://github.com/cncf/memorials/blob/main/han-kang.md), a long-time contributor and respected engineer whose technical excellence and infectious enthusiasm left a lasting impact on the Kubernetes community. Han was a significant force within SIG Instrumentation and SIG API Machinery, earning a [2021 Kubernetes Contributor Award](https://www.kubernetes.dev/community/awards/2021/) for his critical work and sustained commitment to the project's core stability. Beyond his technical contributions, Han was deeply admired for his generosity as a mentor and his passion for building connections among people. He was known for \"opening doors\" for others, whether guiding new contributors through their first pull requests or supporting colleagues with patience and kindness. Han‚Äôs legacy lives on through the engineers he inspired, the robust systems he helped build, and the warm, collaborative spirit he fostered within the cloud native ecosystem.\n\nWe would like to thank the entire [Release Team](https://github.com/kubernetes/sig-release/blob/master/releases/release-1.35/release-team.md) for the hours spent hard at work to deliver the Kubernetes v1.35 release to our community. The Release Team's membership ranges from first-time shadows to returning team leads with experience forged over several release cycles. We are incredibly grateful to our Release Lead, [Drew Hagen](https://github.com/drewhagen), whose hands-on guidance and vibrant energy not only navigated us through complex challenges but also fueled the community spirit behind this successful release.\n\n## Project velocity\n\nThe CNCF K8s [DevStats](https://k8s.devstats.cncf.io/d/11/companies-contributing-in-repository-groups?orgId=1\u0026var-period=m\u0026var-repogroup_name=All) project aggregates a number of interesting data points related to the velocity of Kubernetes and various sub-projects. This includes everything from individual contributions to the number of companies that are contributing and is an illustration of the depth and breadth of effort that goes into evolving this ecosystem.\n\nDuring the v1.35 release cycle, which spanned 14 weeks from 15th September 2025 to 17th December 2025, Kubernetes received contributions from as many as 85 different companies and 419 individuals. In the wider cloud native ecosystem, the figure goes up to 281 companies, counting 1769 total contributors.\n\nNote that \"contribution\" counts when someone makes a commit, code review, comment, creates an issue or PR, reviews a PR (including blogs and documentation) or comments on issues and PRs.  \nIf you are interested in contributing, visit [Getting Started](https://www.kubernetes.dev/docs/guide/#getting-started) on our contributor website.\n\nSources for this data:\n\n- [Companies contributing to Kubernetes](https://k8s.devstats.cncf.io/d/11/companies-contributing-in-repository-groups?orgId=1\u0026from=1757890800000\u0026to=1765929599000\u0026var-period=d28\u0026var-repogroup_name=Kubernetes\u0026var-repo_name=kubernetes%2Fkubernetes)\n- [Overall ecosystem contributions](https://k8s.devstats.cncf.io/d/11/companies-contributing-in-repository-groups?orgId=1\u0026from=1757890800000\u0026to=1765929599000\u0026var-period=d28\u0026var-repogroup_name=All\u0026var-repo_name=kubernetes%2Fkubernetes)\n\n## Events update\n\nExplore upcoming Kubernetes and cloud native events, including KubeCon + CloudNativeCon, KCD, and other notable conferences worldwide. Stay informed and get involved with the Kubernetes community!\n\n**February 2026**\n\n- [**KCD - Kubernetes Community Days: New Delhi**](https://www.kcddelhi.com/): Feb 21, 2026 | New Delhi, India\n- [**KCD - Kubernetes Community Days: Guadalajara**](https://community.cncf.io/events/details/cncf-kcd-guadalajara-presents-kcd-guadalajara-open-source-contributor-summit/cohost-kcd-guadalajara): Feb 23, 2026 | Guadalajara, Mexico\n\n**March 2026**\n\n- [**KubeCon + CloudNativeCon Europe 2026**](https://events.linuxfoundation.org/kubecon-cloudnativecon-europe/): Mar 23-26, 2026 | Amsterdam, Netherlands\n\n**May 2026**\n\n- [**KCD - Kubernetes Community Days: Toronto**](https://community.cncf.io/events/details/cncf-kcd-toronto-presents-kcd-toronto-canada-2026/): May 13, 2026 | Toronto, Canada\n- [**KCD - Kubernetes Community Days: Helsinki**](https://cloudnativefinland.org/kcd-helsinki-2026/): May 20, 2026 | Helsinki, Finland\n\n**June 2026**\n\n- [**KubeCon + CloudNativeCon China 2026**](https://events.linuxfoundation.org/kubecon-cloudnativecon-china/): Jun 10-11, 2026 | Hong Kong\n- [**KubeCon + CloudNativeCon India 2026**](https://events.linuxfoundation.org/kubecon-cloudnativecon-india/): Jun 18-19, 2026 | Mumbai, India\n- [**KCD - Kubernetes Community Days: Kuala Lumpur**](https://community.cncf.io/kcd-kuala-lumpur-2026/): Jun 27, 2026 | Kuala Lumpur, Malaysia\n\n**July 2026**\n\n- [**KubeCon + CloudNativeCon Japan 2026**](https://events.linuxfoundation.org/kubecon-cloudnativecon-japan/): Jul 29-30, 2026 | Yokohama, Japan\n\nYou can find the latest event details [here](https://community.cncf.io/events/#/list).\n\n## Upcoming release webinar\n\nJoin members of the Kubernetes v1.35 Release Team on **Wednesday, January 14, 2026, at 5:00 PM (UTC)** to learn about the release highlights of this release. For more information and registration, visit the [event page](https://community.cncf.io/events/details/cncf-cncf-online-programs-presents-cloud-native-live-kubernetes-v135-release/) on the CNCF Online Programs site.\n\n## Get involved\n\nThe simplest way to get involved with Kubernetes is by joining one of the many [Special Interest Groups](https://github.com/kubernetes/community/blob/master/sig-list.md) (SIGs) that align with your interests. Have something you‚Äôd like to broadcast to the Kubernetes community? Share your voice at our weekly [community meeting](https://github.com/kubernetes/community/tree/master/communication), and through the channels below. Thank you for your continued feedback and support.\n\n- Follow us on Bluesky [@Kubernetesio](https://bsky.app/profile/kubernetes.io) for the latest updates\n- Join the community discussion on [Discuss](https://discuss.kubernetes.io/)\n- Join the community on [Slack](http://slack.k8s.io/)\n- Post questions (or answer questions) on [Stack Overflow](http://stackoverflow.com/questions/tagged/kubernetes)\n- Share your Kubernetes [story](https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform)\n- Read more about what‚Äôs happening with Kubernetes on the [blog](https://kubernetes.io/blog/)\n- Learn more about the [Kubernetes Release Team](https://github.com/kubernetes/sig-release/tree/master/release-team)","metadata":{"title":"Kubernetes v1.35: Timbernetes (The World Tree Release)","excerpt":"Kubernetes v1.35 introduces 60 enhancements including 17 stable features. Discover the latest improvements in security, scheduling, and resource management in this comprehensive release.","date":"2025-12-17","author":"Graziano Casto","platform":"Kubernetes"}},{"slug":"kubernetes-v135-sneak-peak","content":"\n[Published by Kubernetes co-authored with Aakanksha Bhende, Arujjwal Negi, Chad M. Crowell, Swathi Rao](https://kubernetes.io/blog/2025/11/26/kubernetes-v1-35-sneak-peek/)\n\nAs the release of Kubernetes v1.35 approaches, the Kubernetes project continues to evolve. Features may be deprecated, removed, or replaced to improve the project's overall health. This blog post outlines planned changes for the v1.35 release that the release team believes you should be aware of to ensure the continued smooth operation of your Kubernetes cluster(s), and to keep you up to date with the latest developments. The information below is based on the current status of the v1.35 release and is subject to change before the final release date.\n\n### Deprecations and removals for Kubernetes v1.35\n\n#### cgroup v1 support\n\nOn Linux nodes, container runtimes typically rely on cgroups (short for \"control groups\"). Support for using cgroup v2 has been stable in Kubernetes since v1.25, providing an alternative to the original v1 cgroup support. While cgroup v1 provided the initial resource control mechanism, it suffered from well-known inconsistencies and limitations. Adding support for cgroup v2 allowed use of a unified control group hierarchy, improved resource isolation, and served as the foundation for modern features, making legacy cgroup v1 support ready for removal. The removal of cgroup v1 support will only impact cluster administrators running nodes on older Linux distributions that do not support cgroup v2; on those nodes, the `kubelet` will fail to start. Administrators must migrate their nodes to systems with cgroup v2 enabled. More details on compatibility requirements will be available in a blog post soon after the v1.35 release.\n\nTo learn more, read [about cgroup v2](https://kubernetes.io/docs/concepts/architecture/cgroups/);  \nyou can also track the switchover work via [KEP-5573: Remove cgroup v1 support](https://kep.k8s.io/5573).\n\n#### Deprecation of ipvs mode in kube-proxy\n\nMany releases ago, the Kubernetes project implemented an [ipvs](https://kubernetes.io/docs/reference/networking/virtual-ips/#proxy-mode-ipvs) mode in `kube-proxy`. It was adopted as a way to provide high-performance service load balancing, with better performance than the existing `iptables` mode. However, maintaining feature parity between ipvs and other kube-proxy modes became difficult, due to technical complexity and diverging requirements. This created significant technical debt and made the ipvs backend impractical to support alongside newer networking capabilities.\n\nThe Kubernetes project intends to deprecate kube-proxy `ipvs` mode in the v1.35 release, to streamline the `kube-proxy` codebase. For Linux nodes, the recommended `kube-proxy` mode is already [nftables](https://kubernetes.io/docs/reference/networking/virtual-ips/#proxy-mode-nftables).\n\nYou can find more in [KEP-5495: Deprecate ipvs mode in kube-proxy](https://kep.k8s.io/5495)\n\n#### Kubernetes is deprecating containerd v1.y support\n\nWhile Kubernetes v1.35 still supports containerd 1.7 and other LTS releases of containerd, as a consequence of automated cgroup driver detection, the Kubernetes SIG Node community has formally agreed upon a final support timeline for containerd v1.X. Kubernetes v1.35 is the last release to offer this support (aligned with containerd 1.7 EOL).\n\nThis is a final warning that if you are using containerd 1.X, you must switch to 2.0 or later before upgrading Kubernetes to the next version. You are able to monitor the `kubelet_cri_losing_support` metric to determine if any nodes in your cluster are using a containerd version that will soon be unsupported.\n\nYou can find more in the [official blog post](https://kubernetes.io/blog/2025/09/12/kubernetes-v1-34-cri-cgroup-driver-lookup-now-ga/#announcement-kubernetes-is-deprecating-containerd-v1-y-support) or in [KEP-4033: Discover cgroup driver from CRI](https://kep.k8s.io/4033)\n\nThe following enhancements are some of those likely to be included in the v1.35 release. This is not a commitment, and the release content is subject to change.\n\n#### Node declared features\n\nWhen scheduling Pods, Kubernetes uses node labels, taints, and tolerations to match workload requirements with node capabilities. However, managing feature compatibility becomes challenging during cluster upgrades due to version skew between the control plane and nodes. This can lead to Pods being scheduled on nodes that lack required features, resulting in runtime failures.\n\nThe *node declared features* framework will introduce a standard mechanism for nodes to declare their supported Kubernetes features. With the new alpha feature enabled, a Node reports the features it can support, publishing this information to the control plane through a new `.status.declaredFeatures` field. Then, the `kube-scheduler`, admission controllers and third-party components can use these declarations. For example, you can enforce scheduling and API validation constraints, ensuring that Pods run only on compatible nodes.\n\nThis approach reduces manual node labeling, improves scheduling accuracy, and prevents incompatible pod placements proactively. It also integrates with the Cluster Autoscaler for informed scale-up decisions. Feature declarations are temporary and tied to Kubernetes feature gates, enabling safe rollout and cleanup.\n\nTargeting alpha in v1.35, *node declared features* aims to solve version skew scheduling issues by making node capabilities explicit, enhancing reliability and cluster stability in heterogeneous version environments.\n\nTo learn more about this before the official documentation is published, you can read [KEP-5328](https://kep.k8s.io/5328).\n\n#### In-place update of Pod resources\n\nKubernetes is graduating in-place updates for Pod resources to General Availability (GA). This feature allows users to adjust `cpu` and `memory` resources without restarting Pods or Containers. Previously, such modifications required recreating Pods, which could disrupt workloads, particularly for stateful or batch applications. Previous Kubernetes releases already allowed you to change infrastructure resources settings (requests and limits) for existing Pods. This allows for smoother [vertical scaling](https://kubernetes.io/docs/concepts/workloads/autoscaling/vertical-pod-autoscale/), improves efficiency, and can also simplify solution development.\n\nThe Container Runtime Interface (CRI) has also been improved, extending the `UpdateContainerResources` API for Windows and future runtimes while allowing `ContainerStatus` to report real-time resource configurations. Together, these changes make scaling in Kubernetes faster, more flexible, and disruption-free. The feature was introduced as alpha in v1.27, graduated to beta in v1.33, and is targeting graduation to stable in v1.35.\n\nYou can find more in [KEP-1287: In-place Update of Pod Resources](https://kep.k8s.io/1287)\n\n#### Pod certificates\n\nWhen running microservices, Pods often require a strong cryptographic identity to authenticate with each other using mutual TLS (mTLS). While Kubernetes provides Service Account tokens, these are designed for authenticating to the API server, not for general-purpose workload identity.\n\nBefore this enhancement, operators had to rely on complex, external projects like SPIFFE/SPIRE or cert-manager to provision and rotate certificates for their workloads. But what if you could issue a unique, short-lived certificate to your Pods natively and automatically? KEP-4317 is designed to enable such native workload identity. It opens up various possibilities for securing pod-to-pod communication by allowing the `kubelet` to request and mount certificates for a Pod via a projected volume.\n\nThis provides a built-in mechanism for workload identity, complete with automated certificate rotation, significantly simplifying the setup of service meshes and other zero-trust network policies. This feature was introduced as alpha in v1.34 and is targeting beta in v1.35.\n\nYou can find more in [KEP-4317: Pod Certificates](https://kep.k8s.io/4317)\n\n#### Numeric values for taints\n\nKubernetes is enhancing [taints and tolerations](https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/) by adding numeric comparison operators, such as `Gt` (Greater Than) and `Lt` (Less Than).\n\nPreviously, tolerations supported only exact (`Equal`) or existence (`Exists`) matches, which were not suitable for numeric properties such as reliability SLAs.\n\nWith this change, a Pod can use a toleration to \"opt-in\" to nodes that meet a specific numeric threshold. For example, a Pod can require a Node with an SLA taint value greater than 950 (`operator: Gt`, `value: \"950\"`).\n\nThis approach is more powerful than Node Affinity because it supports the NoExecute effect, allowing Pods to be automatically evicted if a node's numeric value drops below the tolerated threshold.\n\nYou can find more in [KEP-5471: Enable SLA-based Scheduling](https://kep.k8s.io/5471)\n\n#### User namespaces\n\nWhen running Pods, you can use `securityContext` to drop privileges, but containers inside the pod often still run as root (UID 0). This simplicity poses a significant challenge, as that container UID 0 maps directly to the host's root user.\n\nBefore this enhancement, a container breakout vulnerability could grant an attacker full root access to the node. But what if you could dynamically remap the container's root user to a safe, unprivileged user on the host? KEP-127 specifically allows such native support for Linux User Namespaces. It opens up various possibilities for pod security by isolating container and host user/group IDs. This allows a process to have root privileges (UID 0) within its namespace, while running as a non-privileged, high-numbered UID on the host.\n\nReleased as alpha in v1.25 and beta in v1.30, this feature continues to progress through beta maturity, paving the way for truly \"rootless\" containers that drastically reduce the attack surface for a whole class of security vulnerabilities.\n\nYou can find more in [KEP-127: User Namespaces](https://kep.k8s.io/127)\n\n#### Support for mounting OCI images as volumes\n\nWhen provisioning a Pod, you often need to bundle data, binaries, or configuration files for your containers. Before this enhancement, people often included that kind of data directly into the main container image, or required a custom init container to download and unpack files into an `emptyDir`. You can still take either of those approaches, of course.\n\nBut what if you could populate a volume directly from a data-only artifact in an OCI registry, just like pulling a container image? Kubernetes v1.31 added support for the `image` volume type, allowing Pods to pull and unpack OCI container image artifacts into a volume declaratively.\n\nThis allows for seamless distribution of data, binaries, or ML models using standard registry tooling, completely decoupling data from the container image and eliminating the need for complex init containers or startup scripts. This volume type has been in beta since v1.33 and will likely be enabled by default in v1.35.\n\nYou can try out the beta version of [`image` volumes](https://kubernetes.io/docs/concepts/storage/volumes/#image), or you can learn more about the plans from [KEP-4639: OCI Volume Source](https://kep.k8s.io/4639).\n\n### Want to know more?\n\nNew features and deprecations are also announced in the Kubernetes release notes. We will formally announce what's new in [Kubernetes v1.35](https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.35.md) as part of the CHANGELOG for that release.\n\nThe Kubernetes v1.35 release is planned for **December 17, 2025**. Stay tuned for updates!\n\nYou can also see the announcements of changes in the release notes for:\n\n- [Kubernetes v1.34](https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.34.md)\n- [Kubernetes v1.33](https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.33.md)\n- [Kubernetes v1.32](https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.32.md)\n- [Kubernetes v1.31](https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.31.md)\n- [Kubernetes v1.30](https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.30.md)\n\n### Get involved\n\nThe simplest way to get involved with Kubernetes is by joining one of the many [Special Interest Groups](https://github.com/kubernetes/community/blob/master/sig-list.md) (SIGs) that align with your interests. Have something you‚Äôd like to broadcast to the Kubernetes community? Share your voice at our weekly [community meeting](https://github.com/kubernetes/community/tree/master/communication), and through the channels below. Thank you for your continued feedback and support.\n\n- Follow us on Bluesky [@kubernetes.io](https://bsky.app/profile/kubernetes.io) for the latest updates\n- Join the community discussion on [Discuss](https://discuss.kubernetes.io/)\n- Join the community on [Slack](http://slack.k8s.io/)\n- Post questions (or answer questions) on [Server Fault](https://serverfault.com/questions/tagged/kubernetes) or [Stack Overflow](http://stackoverflow.com/questions/tagged/kubernetes)\n- Share your Kubernetes [story](https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform)\n- Read more about what‚Äôs happening with Kubernetes on the [blog](https://kubernetes.io/blog/)\n- Learn more about the [Kubernetes Release Team](https://github.com/kubernetes/sig-release/tree/master/release-team)","metadata":{"title":"Kubernetes v1.35 Sneak Peek","excerpt":"Explore the upcoming features in Kubernetes v1.35, including node declared features, in-place pod resource updates, and native pod certificates, along with important deprecations like cgroup v1 removal.","date":"2025-11-26","author":"Graziano Casto","platform":"Kubernetes"}},{"slug":"kubernetes-v134-announcement","content":"\n[Published by Kubernetes co-authored with Agustina Barbetta, Alejandro Josue Leon Bellido, Melony Qin, Dipesh Rawat](https://kubernetes.io/blog/2025/08/27/kubernetes-v1-34-release/)\n\nSimilar to previous releases, the release of Kubernetes v1.34 introduces new stable, beta, and alpha features. The consistent delivery of high-quality releases underscores the strength of our development cycle and the vibrant support from our community.\n\nThis release consists of 58 enhancements. Of those enhancements, 23 have graduated to Stable, 22 have entered Beta, and 13 have entered Alpha.\n\nThere are also some [deprecations and removals](https://kubernetes.io/blog/2025/08/27/kubernetes-v1-34-release/#deprecations-and-removals) in this release; make sure to read about those.\n\n## Release theme and logo\n\n![Kubernetes v1.34 logo: Three bears sail a wooden ship with a flag featuring a paw and a helm symbol on the sail, as wind blows across the ocean](https://kubernetes.io/blog/2025/08/27/kubernetes-v1-34-release/k8s-v1.34.png)\n\nKubernetes v1.34 logo: Three bears sail a wooden ship with a flag featuring a paw and a helm symbol on the sail, as wind blows across the ocean\n\nA release powered by the wind around us ‚Äî and the will within us.\n\nEvery release cycle, we inherit winds that we don't really control ‚Äî the state of our tooling, documentation, and the historical quirks of our project. Sometimes these winds fill our sails, sometimes they push us sideways or die down.\n\nWhat keeps Kubernetes moving isn't the perfect winds, but the will of our sailors who adjust the sails, man the helm, chart the courses and keep the ship steady. The release happens not because conditions are always ideal, but because of the people who build it, the people who release it, and the bears \u003csup\u003e^\u003c/sup\u003e, cats, dogs, wizards, and curious minds who keep Kubernetes sailing strong ‚Äî no matter which way the wind blows.\n\nThis release, **Of Wind \u0026 Will (O' WaW)**, honors the winds that have shaped us, and the will that propels us forward.\n\n\u003csub\u003e^ Oh, and you wonder why bears? Keep wondering!\u003c/sub\u003e\n\n## Spotlight on key updates\n\nKubernetes v1.34 is packed with new features and improvements. Here are a few select updates the Release Team would like to highlight!\n\n### Stable: The core of DRA is GA\n\n[Dynamic Resource Allocation](https://kubernetes.io/docs/concepts/scheduling-eviction/dynamic-resource-allocation/) (DRA) enables more powerful ways to select, allocate, share, and configure GPUs, TPUs, NICs and other devices.\n\nSince the v1.30 release, DRA has been based around claiming devices using *structured parameters* that are opaque to the core of Kubernetes. This enhancement took inspiration from dynamic provisioning for storage volumes. DRA with structured parameters relies on a set of supporting API kinds: ResourceClaim, DeviceClass, ResourceClaimTemplate, and ResourceSlice API types under `resource.k8s.io`, while extending the `.spec` for Pods with a new `resourceClaims` field.  \nThe `resource.k8s.io/v1` APIs have graduated to stable and are now available by default.\n\nThis work was done as part of [KEP #4381](https://kep.k8s.io/4381) led by WG Device Management.\n\n### Beta: Projected ServiceAccount tokens for kubelet image credential providers\n\nThe `kubelet` credential providers, used for pulling private container images, traditionally relied on long-lived Secrets stored on the node or in the cluster. This approach increased security risks and management overhead, as these credentials were not tied to the specific workload and did not rotate automatically.  \nTo solve this, the `kubelet` can now request short-lived, audience-bound ServiceAccount tokens for authenticating to container registries. This allows image pulls to be authorized based on the Pod's own identity rather than a node-level credential.  \nThe primary benefit is a significant security improvement. It eliminates the need for long-lived Secrets for image pulls, reducing the attack surface and simplifying credential management for both administrators and developers.\n\nThis work was done as part of [KEP #4412](https://kep.k8s.io/4412) led by SIG Auth and SIG Node.\n\n### Alpha: Support for KYAML, a Kubernetes dialect of YAML\n\nKYAML aims to be a safer and less ambiguous YAML subset, and was designed specifically for Kubernetes. Whatever version of Kubernetes you use, starting from Kubernetes v1.34 you are able to use KYAML as a new output format for kubectl.\n\nKYAML addresses specific challenges with both YAML and JSON. YAML's significant whitespace requires careful attention to indentation and nesting, while its optional string-quoting can lead to unexpected type coercion (for example: [\"The Norway Bug\"](https://hitchdev.com/strictyaml/why/implicit-typing-removed/)). Meanwhile, JSON lacks comment support and has strict requirements for trailing commas and quoted keys.\n\nYou can write KYAML and pass it as an input to any version of `kubectl`, because all KYAML files are also valid as YAML. With `kubectl` v1.34, you are also able to [request KYAML output](https://kubernetes.io/docs/reference/kubectl/#syntax-1) (as in kubectl get -o kyaml ‚Ä¶) by setting environment variable `KUBECTL_KYAML=true`. If you prefer, you can still request the output in JSON or YAML format.\n\nThis work was done as part of [KEP #5295](https://kep.k8s.io/5295) led by SIG CLI.\n\n## Features graduating to Stable\n\n*This is a selection of some of the improvements that are now stable following the v1.34 release.*\n\n### Delayed creation of Job‚Äôs replacement Pods\n\nBy default, Job controllers create replacement Pods immediately when a Pod starts terminating, causing both Pods to run simultaneously. This can cause resource contention in constrained clusters, where the replacement Pod may struggle to find available nodes until the original Pod fully terminates. The situation can also trigger unwanted cluster autoscaler scale-ups. Additionally, some machine learning frameworks like TensorFlow and [JAX](https://jax.readthedocs.io/en/latest/) require only one Pod per index to run at a time, making simultaneous Pod execution problematic. This feature introduces `.spec.podReplacementPolicy` in Jobs. You may choose to create replacement Pods only when the Pod is fully terminated (has `.status.phase: Failed`). To do this, set `.spec.podReplacementPolicy: Failed`.  \nIntroduced as alpha in v1.28, this feature has graduated to stable in v1.34.\n\nThis work was done as part of [KEP #3939](https://kep.k8s.io/3939) led by SIG Apps.\n\n### Recovery from volume expansion failure\n\nThis feature allows users to cancel volume expansions that are unsupported by the underlying storage provider, and retry volume expansion with smaller values that may succeed.  \nIntroduced as alpha in v1.23, this feature has graduated to stable in v1.34.\n\nThis work was done as part of [KEP #1790](https://kep.k8s.io/1790) led by SIG Storage.\n\n### VolumeAttributesClass for volume modification\n\n[VolumeAttributesClass](https://kubernetes.io/docs/concepts/storage/volume-attributes-classes/) has graduated to stable in v1.34. VolumeAttributesClass is a generic, Kubernetes-native API for modifying volume parameters like provisioned IO. It allows workloads to vertically scale their volumes on-line to balance cost and performance, if supported by their provider.  \nLike all new volume features in Kubernetes, this API is implemented via the [container storage interface (CSI)](https://kubernetes-csi.github.io/docs/). Your provisioner-specific CSI driver must support the new ModifyVolume API which is the CSI side of this feature.\n\nThis work was done as part of [KEP #3751](https://kep.k8s.io/3751) led by SIG Storage.\n\n### Structured authentication configuration\n\nKubernetes v1.29 introduced a configuration file format to manage API server client authentication, moving away from the previous reliance on a large set of command-line options. The [AuthenticationConfiguration](https://kubernetes.io/docs/reference/access-authn-authz/authentication/#using-authentication-configuration) kind allows administrators to support multiple JWT authenticators, CEL expression validation, and dynamic reloading. This change significantly improves the manageability and auditability of the cluster's authentication settings - and has graduated to stable in v1.34.\n\nThis work was done as part of [KEP #3331](https://kep.k8s.io/3331) led by SIG Auth.\n\n### Finer-grained authorization based on selectors\n\nKubernetes authorizers, including webhook authorizers and the built-in node authorizer, can now make authorization decisions based on field and label selectors in incoming requests. When you send **list**, **watch** or **deletecollection** requests with selectors, the authorization layer can now evaluate access with that additional context.\n\nFor example, you can write an authorization policy that only allows listing Pods bound to a specific `.spec.nodeName`. The client (perhaps the kubelet on a particular node) must specify the field selector that the policy requires, otherwise the request is forbidden. This change makes it feasible to set up least privilege rules, provided that the client knows how to conform to the restrictions you set. Kubernetes v1.34 now supports more granular control in environments like per-node isolation or custom multi-tenant setups.\n\nThis work was done as part of [KEP #4601](https://kep.k8s.io/4601) led by SIG Auth.\n\n### Restrict anonymous requests with fine-grained controls\n\nInstead of fully enabling or disabling anonymous access, you can now configure a strict list of endpoints where unauthenticated requests are allowed. This provides a safer alternative for clusters that rely on anonymous access to health or bootstrap endpoints like `/healthz`, `/readyz`, or `/livez`.\n\nWith this feature, accidental RBAC misconfigurations that grant broad access to anonymous users can be avoided without requiring changes to external probes or bootstrapping tools.\n\nThis work was done as part of [KEP #4633](https://kep.k8s.io/4633) led by SIG Auth.\n\nThe `kube-scheduler` can now make more accurate decisions about when to retry scheduling Pods that were previously unschedulable. Each scheduling plugin can now register callback functions that tell the scheduler whether an incoming cluster event is likely to make a rejected Pod schedulable again.\n\nThis reduces unnecessary retries and improves overall scheduling throughput - especially in clusters using dynamic resource allocation. The feature also lets certain plugins skip the usual backoff delay when it is safe to do so, making scheduling faster in specific cases.\n\nThis work was done as part of [KEP #4247](https://kep.k8s.io/4247) led by SIG Scheduling.\n\n### Ordered Namespace deletion\n\nSemi-random resource deletion order can create security gaps or unintended behavior, such as Pods persisting after their associated NetworkPolicies are deleted.  \nThis improvement introduces a more structured deletion process for Kubernetes [namespaces](https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/) to ensure secure and deterministic resource removal. By enforcing a structured deletion sequence that respects logical and security dependencies, this approach ensures Pods are removed before other resources.  \nThis feature was introduced in Kubernetes v1.33 and graduated to stable in v1.34. The graduation improves security and reliability by mitigating risks from non-deterministic deletions, including the vulnerability described in [CVE-2024-7598](https://github.com/advisories/GHSA-r56h-j38w-hrqq).\n\nThis work was done as part of [KEP #5080](https://kep.k8s.io/5080) led by SIG API Machinery.\n\n### Streaming list responses\n\nHandling large **list** responses in Kubernetes previously posed a significant scalability challenge. When clients requested extensive resource lists, such as thousands of Pods or Custom Resources, the API server was required to serialize the entire collection of objects into a single, large memory buffer before sending it. This process created substantial memory pressure and could lead to performance degradation, impacting the overall stability of the cluster.  \nTo address this limitation, a streaming encoding mechanism for collections (list responses) has been introduced. For the JSON and Kubernetes Protobuf response formats, that streaming mechanism is automatically active and the associated feature gate is stable. The primary benefit of this approach is the avoidance of large memory allocations on the API server, resulting in a much smaller and more predictable memory footprint. Consequently, the cluster becomes more resilient and performant, especially in large-scale environments where frequent requests for extensive resource lists are common.\n\nThis work was done as part of [KEP #5116](https://kep.k8s.io/5116) led by SIG API Machinery.\n\n### Resilient watch cache initialization\n\nWatch cache is a caching layer inside `kube-apiserver` that maintains an eventually consistent cache of cluster state stored in etcd. In the past, issues could occur when the watch cache was not yet initialized during `kube-apiserver` startup or when it required re-initialization.\n\nTo address these issues, the watch cache initialization process has been made more resilient to failures, improving control plane robustness and ensuring controllers and clients can reliably establish watches. This improvement was introduced as beta in v1.31 and is now stable.\n\nThis work was done as part of [KEP #4568](https://kep.k8s.io/4568) led by SIG API Machinery and SIG Scalability.\n\nPreviously, the strict validation of a Pod's DNS `search` path in Kubernetes often created integration challenges in complex or legacy network environments. This restrictiveness could block configurations that were necessary for an organization's infrastructure, forcing administrators to implement difficult workarounds.  \nTo address this, relaxed DNS validation was introduced as alpha in v1.32 and has now graduated to stable in v1.34. A common use case involves Pods that need to communicate with both internal Kubernetes services and external domains. By setting a single dot (`.`) as the first entry in the `searches` list of the Pod's `.spec.dnsConfig`, administrators can prevent the system's resolver from appending the cluster's internal search domains to external queries. This avoids generating unnecessary DNS requests to the internal DNS server for external hostnames, improving efficiency and preventing potential resolution errors.\n\nThis work was done as part of [KEP #4427](https://kep.k8s.io/4427) led by SIG Network.\n\n### Support for Direct Service Return (DSR) in Windows kube-proxy\n\nDSR provides performance optimizations by allowing return traffic routed through load balancers to bypass the load balancer and respond directly to the client, reducing load on the load balancer and improving overall latency. For information on DSR on Windows, read [Direct Server Return (DSR) in a nutshell](https://techcommunity.microsoft.com/blog/networkingblog/direct-server-return-dsr-in-a-nutshell/693710).  \nInitially introduced in v1.14, this feature has graduated to stable in v1.34.\n\nThis work was done as part of [KEP #5100](https://kep.k8s.io/5100) led by SIG Windows.\n\n### Sleep action for Container lifecycle hooks\n\nA Sleep action for containers‚Äô PreStop and PostStart lifecycle hooks was introduced to provide a straightforward way to manage graceful shutdowns and improve overall container lifecycle management.  \nThe Sleep action allows containers to pause for a specified duration after starting or before termination. Using a negative or zero sleep duration returns immediately, resulting in a no-op.  \nThe Sleep action was introduced in Kubernetes v1.29, with zero value support added in v1.32. Both features graduated to stable in v1.34.\n\nThis work was done as part of [KEP #3960](https://kep.k8s.io/3960) and [KEP #4818](https://kep.k8s.io/4818) led by SIG Node.\n\n### Linux node swap support\n\nHistorically, the lack of swap support in Kubernetes could lead to workload instability, as nodes under memory pressure often had to terminate processes abruptly. This particularly affected applications with large but infrequently accessed memory footprints and prevented more graceful resource management.\n\nTo address this, configurable per-node swap support was introduced in v1.22. It has progressed through alpha and beta stages and has graduated to stable in v1.34. The primary mode, `LimitedSwap`, allows Pods to use swap within their existing memory limits, providing a direct solution to the problem. By default, the `kubelet` is configured with `NoSwap` mode, which means Kubernetes workloads cannot use swap.\n\nThis feature improves workload stability and allows for more efficient resource utilization. It enables clusters to support a wider variety of applications, especially in resource-constrained environments, though administrators must consider the potential performance impact of swapping.\n\nThis work was done as part of [KEP #2400](https://kep.k8s.io/2400) led by SIG Node.\n\n### Allow special characters in environment variables\n\nThe environment variable validation rules in Kubernetes have been relaxed to allow nearly all printable ASCII characters in variable names, excluding `=`. This change supports scenarios where workloads require nonstandard characters in variable names - for example, frameworks like.NET Core that use `:` to represent nested configuration keys.\n\nThe relaxed validation applies to environment variables defined directly in Pod spec, as well as those injected using `envFrom` references to ConfigMaps and Secrets.\n\nThis work was done as part of [KEP #4369](https://kep.k8s.io/4369) led by SIG Node.\n\n### Taint management is separated from Node lifecycle\n\nHistorically, the `TaintManager` 's logic for applying NoSchedule and NoExecute taints to nodes based on their condition (NotReady, Unreachable, etc.) was tightly coupled with the node lifecycle controller. This tight coupling made the code harder to maintain and test, and it also limited the flexibility of the taint-based eviction mechanism. This KEP refactors the `TaintManager` into its own separate controller within the Kubernetes controller manager. It is an internal architectural improvement designed to increase code modularity and maintainability. This change allows the logic for taint-based evictions to be tested and evolved independently, but it has no direct user-facing impact on how taints are used.\n\nThis work was done as part of [KEP #3902](https://kep.k8s.io/3902) led by SIG Scheduling and SIG Node.\n\n## New features in Beta\n\n*This is a selection of some of the improvements that are now beta following the v1.34 release.*\n\n### Pod-level resource requests and limits\n\nDefining resource needs for Pods with multiple containers has been challenging, as requests and limits could only be set on a per-container basis. This forced developers to either over-provision resources for each container or meticulously divide the total desired resources, making configuration complex and often leading to inefficient resource allocation. To simplify this, the ability to specify resource requests and limits at the Pod level was introduced. This allows developers to define an overall resource budget for a Pod, which is then shared among its constituent containers. This feature was introduced as alpha in v1.32 and has graduated to beta in v1.34, with HPA now supporting pod-level resource specifications.\n\nThe primary benefit is a more intuitive and straightforward way to manage resources for multi-container Pods. It ensures that the total resources used by all containers do not exceed the Pod's defined limits, leading to better resource planning, more accurate scheduling, and more efficient utilization of cluster resources.\n\nThis work was done as part of [KEP #2837](https://kep.k8s.io/2837) led by SIG Scheduling and SIG Autoscaling.\n\n### .kuberc file for kubectl user preferences\n\nA `.kuberc` configuration file allows you to define preferences for `kubectl`, such as default options and command aliases. Unlike the kubeconfig file, the `.kuberc` configuration file does not contain cluster details, usernames or passwords.  \nThis feature was introduced as alpha in v1.33, gated behind the environment variable `KUBECTL_KUBERC`. It has graduated to beta in v1.34 and is enabled by default.\n\nThis work was done as part of [KEP #3104](https://kep.k8s.io/3104) led by SIG CLI.\n\n### External ServiceAccount token signing\n\nTraditionally, Kubernetes manages ServiceAccount tokens using static signing keys that are loaded from disk at `kube-apiserver` startup. This feature introduces an `ExternalJWTSigner` gRPC service for out-of-process signing, enabling Kubernetes distributions to integrate with external key management solutions (for example, HSMs, cloud KMSes) for ServiceAccount token signing instead of static disk-based keys.\n\nIntroduced as alpha in v1.32, this external JWT signing capability advances to beta and is enabled by default in v1.34.\n\nThis work was done as part of [KEP #740](https://kep.k8s.io/740) led by SIG Auth.\n\n### DRA features in beta\n\n#### Admin access for secure resource monitoring\n\nDRA supports controlled administrative access via the `adminAccess` field in ResourceClaims or ResourceClaimTemplates, allowing cluster operators to access devices already in use by others for monitoring or diagnostics. This privileged mode is limited to users authorized to create such objects in namespaces labeled `resource.k8s.io/admin-access: \"true\"`, ensuring regular workloads remain unaffected. Graduating to beta in v1.34, this feature provides secure introspection capabilities while preserving workload isolation through namespace-based authorization checks.\n\nThis work was done as part of [KEP #5018](https://kep.k8s.io/5018) led by WG Device Management and SIG Auth.\n\n#### Prioritized alternatives in ResourceClaims and ResourceClaimTemplates\n\nWhile a workload might run best on a single high-performance GPU, it might also be able to run on two mid-level GPUs.  \nWith the feature gate `DRAPrioritizedList` (now enabled by default), ResourceClaims and ResourceClaimTemplates get a new field named `firstAvailable`. This field is an ordered list that allows users to specify that a request may be satisfied in different ways, including allocating nothing at all if specific hardware is not available. The scheduler will attempt to satisfy the alternatives in the list in order, so the workload will be allocated the best set of devices available in the cluster.\n\nThis work was done as part of [KEP #4816](https://kep.k8s.io/4816) led by WG Device Management.\n\n#### The kubelet reports allocated DRA resources\n\nThe `kubelet` 's API has been updated to report on Pod resources allocated through DRA. This allows node monitoring agents to discover the allocated DRA resources for Pods on a node. Additionally, it enables node components to use the PodResourcesAPI and leverage this DRA information when developing new features and integrations.  \nStarting from Kubernetes v1.34, this feature is enabled by default.\n\nThis work was done as part of [KEP #3695](https://kep.k8s.io/3695) led by WG Device Management.\n\n### kube-scheduler non-blocking API calls\n\nThe `kube-scheduler` makes blocking API calls during scheduling cycles, creating performance bottlenecks. This feature introduces asynchronous API handling through a prioritized queue system with request deduplication, allowing the scheduler to continue processing Pods while API operations complete in the background. Key benefits include reduced scheduling latency, prevention of scheduler thread starvation during API delays, and immediate retry capability for unschedulable Pods. The implementation maintains backward compatibility and adds metrics for monitoring pending API operations.\n\nThis work was done as part of [KEP #5229](https://kep.k8s.io/5229) led by SIG Scheduling.\n\n### Mutating admission policies\n\n[MutatingAdmissionPolicies](https://kubernetes.io/docs/reference/access-authn-authz/mutating-admission-policy/) offer a declarative, in-process alternative to mutating admission webhooks. This feature leverages CEL's object instantiation and JSON Patch strategies, combined with Server Side Apply‚Äôs merge algorithms.  \nThis significantly simplifies admission control by allowing administrators to define mutation rules directly in the API server.  \nIntroduced as alpha in v1.32, mutating admission policies has graduated to beta in v1.34.\n\nThis work was done as part of [KEP #3962](https://kep.k8s.io/3962) led by SIG API Machinery.\n\n### Snapshottable API server cache\n\nThe `kube-apiserver` 's caching mechanism (watch cache) efficiently serves requests for the latest observed state. However, **list** requests for previous states (for example, via pagination or by specifying a `resourceVersion`) often bypass this cache and are served directly from etcd. This direct etcd access significantly increases performance costs and can lead to stability issues, particularly with large resources, due to memory pressure from transferring large data blobs.  \nWith the `ListFromCacheSnapshot` feature gate enabled by default, `kube-apiserver` will attempt to serve the response from snapshots if one is available with `resourceVersion` older than requested. The `kube-apiserver` starts with no snapshots, creates a new snapshot on every watch event, and keeps them until it detects etcd is compacted or if cache is full with events older than 75 seconds. If the provided `resourceVersion` is unavailable, the server will fallback to etcd.\n\nThis work was done as part of [KEP #4988](https://kep.k8s.io/4988) led by SIG API Machinery.\n\n### Tooling for declarative validation of Kubernetes-native types\n\nPrior to this release, validation rules for the APIs built into Kubernetes were written entirely by hand, which makes them difficult for maintainers to discover, understand, improve or test. There was no single way to find all the validation rules that might apply to an API.*Declarative validation* benefits Kubernetes maintainers by making API development, maintenance, and review easier while enabling programmatic inspection for better tooling and documentation. For people using Kubernetes libraries to write their own code (for example: a controller), the new approach streamlines adding new fields through IDL tags, rather than complex validation functions. This change helps speed up API creation by automating validation boilerplate, and provides more relevant error messages by performing validation on versioned types.  \nThis enhancement (which graduated to beta in v1.33 and continues as beta in v1.34) brings CEL-based validation rules to native Kubernetes types. It allows for more granular and declarative validation to be defined directly in the type definitions, improving API consistency and developer experience.\n\nThis work was done as part of [KEP #5073](https://kep.k8s.io/5073) led by SIG API Machinery.\n\n### Streaming informers for list requests\n\nThe streaming informers feature, which has been in beta since v1.32, gains further beta refinements in v1.34. This capability allows **list** requests to return data as a continuous stream of objects from the API server‚Äôs watch cache, rather than assembling paged results directly from etcd. By reusing the same mechanics used for **watch** operations, the API server can serve large datasets while keeping memory usage steady and avoiding allocation spikes that can affect stability.\n\nIn this release, the `kube-apiserver` and `kube-controller-manager` both take advantage of the new `WatchList` mechanism by default. For the `kube-apiserver`, this means list requests are streamed more efficiently, while the `kube-controller-manager` benefits from a more memory-efficient and predictable way to work with informers. Together, these improvements reduce memory pressure during large list operations, and improve reliability under sustained load, making list streaming more predictable and efficient.\n\nThis work was done as part of [KEP #3157](https://kep.k8s.io/3157) led by SIG API Machinery and SIG Scalability.\n\n### Graceful node shutdown handling for Windows nodes\n\nThe `kubelet` on Windows nodes can now detect system shutdown events and begin graceful termination of running Pods. This mirrors existing behavior on Linux and helps ensure workloads exit cleanly during planned shutdowns or restarts.  \nWhen the system begins shutting down, the `kubelet` reacts by using standard termination logic. It respects the configured lifecycle hooks and grace periods, giving Pods time to stop before the node powers off. The feature relies on Windows pre-shutdown notifications to coordinate this process. This enhancement improves workload reliability during maintenance, restarts, or system updates. It is now in beta and enabled by default.\n\nThis work was done as part of [KEP #4802](https://kep.k8s.io/4802) led by SIG Windows.\n\n### In-place Pod resize improvements\n\nGraduated to beta and enabled by default in v1.33, in-place Pod resizing receives further improvements in v1.34. These include support for decreasing memory usage and integration with Pod-level resources.\n\nThis feature remains in beta in v1.34. For detailed usage instructions and examples, refer to the documentation: [Resize CPU and Memory Resources assigned to Containers](https://kubernetes.io/docs/tasks/configure-pod-container/resize-container-resources/).\n\nThis work was done as part of [KEP #1287](https://kep.k8s.io/1287) led by SIG Node and SIG Autoscaling.\n\n## New features in Alpha\n\n*This is a selection of some of the improvements that are now alpha following the v1.34 release.*\n\n### Pod certificates for mTLS authentication\n\nAuthenticating workloads within a cluster, especially for communication with the API server, has primarily relied on ServiceAccount tokens. While effective, these tokens aren't always ideal for establishing a strong, verifiable identity for mutual TLS (mTLS) and can present challenges when integrating with external systems that expect certificate-based authentication.  \nKubernetes v1.34 introduces a built-in mechanism for Pods to obtain X.509 certificates via [PodCertificateRequests](https://kubernetes.io/docs/reference/access-authn-authz/certificate-signing-requests/#pod-certificate-requests). The `kubelet` can request and manage certificates for Pods, which can then be used to authenticate to the Kubernetes API server and other services using mTLS. The primary benefit is a more robust and flexible identity mechanism for Pods. It provides a native way to implement strong mTLS authentication without relying solely on bearer tokens, aligning Kubernetes with standard security practices and simplifying integrations with certificate-aware observability and security tooling.\n\nThis work was done as part of [KEP #4317](https://kep.k8s.io/4317) led by SIG Auth.\n\n### \"Restricted\" Pod security standard now forbids remote probes\n\nThe `host` field within probes and lifecycle handlers allows users to specify an entity other than the `podIP` for the `kubelet` to probe. However, this opens up a route for misuse and for attacks that bypass security controls, since the `host` field could be set to **any** value, including security sensitive external hosts, or localhost on the node. In Kubernetes v1.34, Pods only meet the [Restricted](https://kubernetes.io/docs/concepts/security/pod-security-standards/#restricted) Pod security standard if they either leave the `host` field unset, or if they don't even use this kind of probe. You can use *Pod security admission*, or a third party solution, to enforce that Pods meet this standard. Because these are security controls, check the documentation to understand the limitations and behavior of the enforcement mechanism you choose.\n\nThis work was done as part of [KEP #4940](https://kep.k8s.io/4940) led by SIG Auth.\n\n### Use.status.nominatedNodeName to express Pod placement\n\nWhen the `kube-scheduler` takes time to bind Pods to Nodes, cluster autoscalers may not understand that a Pod will be bound to a specific Node. Consequently, they may mistakenly consider the Node as underutilized and delete it.  \nTo address this issue, the `kube-scheduler` can use `.status.nominatedNodeName` not only to indicate ongoing preemption but also to express Pod placement intentions. By enabling the `NominatedNodeNameForExpectation` feature gate, the scheduler uses this field to indicate where a Pod will be bound. This exposes internal reservations to help external components make informed decisions.\n\nThis work was done as part of [KEP #5278](https://kep.k8s.io/5278) led by SIG Scheduling.\n\n### DRA features in alpha\n\n#### Resource health status for DRA\n\nIt can be difficult to know when a Pod is using a device that has failed or is temporarily unhealthy, which makes troubleshooting Pod crashes challenging or impossible.  \nResource Health Status for DRA improves observability by exposing the health status of devices allocated to a Pod in the Pod‚Äôs status. This makes it easier to identify the cause of Pod issues related to unhealthy devices and respond appropriately.  \nTo enable this functionality, the `ResourceHealthStatus` feature gate must be enabled, and the DRA driver must implement the `DRAResourceHealth` gRPC service.\n\nThis work was done as part of [KEP #4680](https://kep.k8s.io/4680) led by WG Device Management.\n\n#### Extended resource mapping\n\nExtended resource mapping provides a simpler alternative to DRA's expressive and flexible approach by offering a straightforward way to describe resource capacity and consumption. This feature enables cluster administrators to advertise DRA-managed resources as *extended resources*, allowing application developers and operators to continue using the familiar container‚Äôs `.spec.resources` syntax to consume them.  \nThis enables existing workloads to adopt DRA without modifications, simplifying the transition to DRA for both application developers and cluster administrators.\n\nThis work was done as part of [KEP #5004](https://kep.k8s.io/5004) led by WG Device Management.\n\n#### DRA consumable capacity\n\nKubernetes v1.33 added support for resource drivers to advertise slices of a device that are available, rather than exposing the entire device as an all-or-nothing resource. However, this approach couldn't handle scenarios where device drivers manage fine-grained, dynamic portions of a device resource based on user demand, or share those resources independently of ResourceClaims, which are restricted by their spec and namespace.  \nEnabling the `DRAConsumableCapacity` feature gate (introduced as alpha in v1.34) allows resource drivers to share the same device, or even a slice of a device, across multiple ResourceClaims or across multiple DeviceRequests. The feature also extends the scheduler to support allocating portions of device resources, as defined in the `capacity` field. This DRA feature improves device sharing across namespaces and claims, tailoring it to Pod needs. It enables drivers to enforce capacity limits, enhances scheduling, and supports new use cases like bandwidth-aware networking and multi-tenant sharing.\n\nThis work was done as part of [KEP #5075](https://kep.k8s.io/5075) led by WG Device Management.\n\n#### Device binding conditions\n\nThe Kubernetes scheduler gets more reliable by delaying binding a Pod to a Node until its required external resources, such as attachable devices or FPGAs, are confirmed to be ready.  \nThis delay mechanism is implemented in the [PreBind phase](https://kubernetes.io/docs/concepts/scheduling-eviction/scheduling-framework/#pre-bind) of the scheduling framework. During this phase, the scheduler checks whether all required device conditions are satisfied before proceeding with binding. This enables coordination with external device controllers, ensuring more robust, predictable scheduling.\n\nThis work was done as part of [KEP #5007](https://kep.k8s.io/5007) led by WG Device Management.\n\n### Container restart rules\n\nCurrently, all containers within a Pod will follow the same `.spec.restartPolicy` when exited or crashed. However, Pods that run multiple containers might have different restart requirements for each container. For example, for init containers used to perform initialization, you may not want to retry initialization if they fail. Similarly, in ML research environments with long-running training workloads, containers that fail with retriable exit codes should restart quickly in place, rather than triggering Pod recreation and losing progress.  \nKubernetes v1.34 introduces the `ContainerRestartRules` feature gate. When enabled, a `restartPolicy` can be specified for each container within a Pod. A `restartPolicyRules` list can also be defined to override `restartPolicy` based on the last exit code. This provides the fine-grained control needed to handle complex scenarios and better utilization of compute resources.\n\nThis work was done as part of [KEP #5307](https://kep.k8s.io/5307) led by SIG Node.\n\n### Load environment variables from files created in runtime\n\nApplication developers have long requested greater flexibility in declaring environment variables. Traditionally, environment variables are declared on the API server side via static values, ConfigMaps, or Secrets.\n\nBehind the `EnvFiles` feature gate, Kubernetes v1.34 introduces the ability to declare environment variables at runtime. One container (typically an init container) can generate the variable and store it in a file, and a subsequent container can start with the environment variable loaded from that file. This approach eliminates the need to \"wrap\" the target container's entry point, enabling more flexible in-Pod container orchestration.\n\nThis feature particularly benefits AI/ML training workloads, where each Pod in a training Job requires initialization with runtime-defined values.\n\nThis work was done as part of [KEP #5307](https://kep.k8s.io/3721) led by SIG Node.\n\n## Graduations, deprecations, and removals in v1.34\n\n### Graduations to stable\n\nThis lists all the features that graduated to stable (also known as *general availability*). For a full list of updates including new features and graduations from alpha to beta, see the release notes.\n\nThis release includes a total of 23 enhancements promoted to stable:\n\n- [Allow almost all printable ASCII characters in environment variables](https://kep.k8s.io/4369)\n- [Allow for recreation of pods once fully terminated in the job controller](https://kep.k8s.io/3939)\n- [Allow zero value for Sleep Action of PreStop Hook](https://kep.k8s.io/4818)\n- [API Server tracing](https://kep.k8s.io/647)\n- [AppArmor support](https://kep.k8s.io/24)\n- [Authorize with Field and Label Selectors](https://kep.k8s.io/4601)\n- [Consistent Reads from Cache](https://kep.k8s.io/2340)\n- [Decouple TaintManager from NodeLifecycleController](https://kep.k8s.io/3902)\n- [Discover cgroup driver from CRI](https://kep.k8s.io/4033)\n- [DRA: structured parameters](https://kep.k8s.io/4381)\n- [Introducing Sleep Action for PreStop Hook](https://kep.k8s.io/3960)\n- [Kubelet OpenTelemetry Tracing](https://kep.k8s.io/2831)\n- [Kubernetes VolumeAttributesClass ModifyVolume](https://kep.k8s.io/3751)\n- [Node memory swap support](https://kep.k8s.io/2400)\n- [Only allow anonymous auth for configured endpoints](https://kep.k8s.io/4633)\n- [Ordered namespace deletion](https://kep.k8s.io/5080)\n- [Per-plugin callback functions for accurate requeueing in kube-scheduler](https://kep.k8s.io/4247)\n- [Relaxed DNS search string validation](https://kep.k8s.io/4427)\n- [Resilient Watchcache Initialization](https://kep.k8s.io/4568)\n- [Streaming Encoding for LIST Responses](https://kep.k8s.io/5116)\n- [Structured Authentication Config](https://kep.k8s.io/3331)\n- [Support for Direct Service Return (DSR) and overlay networking in Windows kube-proxy](https://kep.k8s.io/5100)\n- [Support recovery from volume expansion failure](https://kep.k8s.io/1790)\n\n### Deprecations and removals\n\nAs Kubernetes develops and matures, features may be deprecated, removed, or replaced with better ones to improve the project's overall health. See the Kubernetes [deprecation and removal policy](https://kubernetes.io/docs/reference/using-api/deprecation-policy/) for more details on this process. Kubernetes v1.34 includes a couple of deprecations.\n\n#### Manual cgroup driver configuration is deprecated\n\nHistorically, configuring the correct cgroup driver has been a pain point for users running Kubernetes clusters. Kubernetes v1.28 added a way for the `kubelet` to query the CRI implementation and find which cgroup driver to use. That automated detection is now **strongly recommended** and support for it has graduated to stable in v1.34. If your CRI container runtime does not support the ability to report the cgroup driver it needs, you should upgrade or change your container runtime. The `cgroupDriver` configuration setting in the `kubelet` configuration file is now deprecated. The corresponding command-line option `--cgroup-driver` was previously deprecated, as Kubernetes recommends using the configuration file instead. Both the configuration setting and command-line option will be removed in a future release, that removal will not happen before the v1.36 minor release.\n\nThis work was done as part of [KEP #4033](https://kep.k8s.io/4033) led by SIG Node.\n\n#### Kubernetes to end containerd 1.x support in v1.36\n\nWhile Kubernetes v1.34 still supports containerd 1.7 and other LTS releases of containerd, as a consequence of automated cgroup driver detection, the Kubernetes SIG Node community has formally agreed upon a final support timeline for containerd v1.X. The last Kubernetes release to offer this support will be v1.35 (aligned with containerd 1.7 EOL). This is an early warning that if you are using containerd 1.X, consider switching to 2.0+ soon. You are able to monitor the `kubelet_cri_losing_support` metric to determine if any nodes in your cluster are using a containerd version that will soon be outdated.\n\nThis work was done as part of [KEP #4033](https://kep.k8s.io/4033) led by SIG Node.\n\n#### PreferClose traffic distribution is deprecated\n\nThe `spec.trafficDistribution` field within a Kubernetes [Service](https://kubernetes.io/docs/concepts/services-networking/service/) allows users to express preferences for how traffic should be routed to Service endpoints.\n\n[KEP-3015](https://kep.k8s.io/3015) deprecates `PreferClose` and introduces two additional values: `PreferSameZone` and `PreferSameNode`. `PreferSameZone` is an alias for the existing `PreferClose` to clarify its semantics. `PreferSameNode` allows connections to be delivered to a local endpoint when possible, falling back to a remote endpoint when not possible.\n\nThis feature was introduced in v1.33 behind the `PreferSameTrafficDistribution` feature gate. It has graduated to beta in v1.34 and is enabled by default.\n\nThis work was done as part of [KEP #3015](https://kep.k8s.io/3015) led by SIG Network.\n\n## Release notes\n\nCheck out the full details of the Kubernetes v1.34 release in our [release notes](https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.34.md).\n\n## Availability\n\nKubernetes v1.34 is available for download on or on the [Kubernetes download page](https://kubernetes.io/releases/download/).\n\nTo get started with Kubernetes, check out these [interactive tutorials](https://kubernetes.io/docs/tutorials/) or run local Kubernetes clusters using [minikube](https://minikube.sigs.k8s.io/). You can also easily install v1.34 using [kubeadm](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/).\n\n## Release Team\n\nKubernetes is only possible with the support, commitment, and hard work of its community. Each release team is made up of dedicated community volunteers who work together to build the many pieces that make up the Kubernetes releases you rely on. This requires the specialized skills of people from all corners of our community, from the code itself to its documentation and project management.\n\n[We honor the memory of Rodolfo \"Rodo\" Mart√≠nez Vega](https://github.com/cncf/memorials/blob/main/rodolfo-martinez.md), a dedicated contributor whose passion for technology and community building left a mark on the Kubernetes community. Rodo served as a member of the Kubernetes Release Team across multiple releases, including v1.22-v1.23 and v1.25-v1.30, demonstrating unwavering commitment to the project's success and stability.  \nBeyond his Release Team contributions, Rodo was deeply involved in fostering the Cloud Native LATAM community, helping to bridge language and cultural barriers in the space. His work on the Spanish version of Kubernetes documentation and the CNCF Glossary exemplified his dedication to making knowledge accessible to Spanish-speaking developers worldwide. Rodo's legacy lives on through the countless community members he mentored, the releases he helped deliver, and the vibrant LATAM Kubernetes community he helped cultivate.\n\nWe would like to thank the entire [Release Team](https://github.com/kubernetes/sig-release/blob/master/releases/release-1.34/release-team.md) for the hours spent hard at work to deliver the Kubernetes v1.34 release to our community. The Release Team's membership ranges from first-time shadows to returning team leads with experience forged over several release cycles. A very special thanks goes out to our release lead, Vyom Yadav, for guiding us through a successful release cycle, for his hands-on approach to solving challenges, and for bringing the energy and care that drives our community forward.\n\n## Project Velocity\n\nThe CNCF K8s [DevStats](https://k8s.devstats.cncf.io/d/11/companies-contributing-in-repository-groups?orgId=1\u0026var-period=m\u0026var-repogroup_name=All) project aggregates a number of interesting data points related to the velocity of Kubernetes and various sub-projects. This includes everything from individual contributions to the number of companies that are contributing and is an illustration of the depth and breadth of effort that goes into evolving this ecosystem.\n\nDuring the v1.34 release cycle, which spanned 15 weeks from 19th May 2025 to 27th August 2025, Kubernetes received contributions from as many as 106 different companies and 491 individuals. In the wider cloud native ecosystem, the figure goes up to 370 companies, counting 2235 total contributors.\n\nNote that \"contribution\" counts when someone makes a commit, code review, comment, creates an issue or PR, reviews a PR (including blogs and documentation) or comments on issues and PRs.  \nIf you are interested in contributing, visit [Getting Started](https://www.kubernetes.dev/docs/guide/#getting-started) on our contributor website.\n\nSource for this data:\n\n- [Companies contributing to Kubernetes](https://k8s.devstats.cncf.io/d/11/companies-contributing-in-repository-groups?orgId=1\u0026from=1747609200000\u0026to=1756335599000\u0026var-period=d28\u0026var-repogroup_name=Kubernetes\u0026var-repo_name=kubernetes%2Fkubernetes)\n- [Overall ecosystem contributions](https://k8s.devstats.cncf.io/d/11/companies-contributing-in-repository-groups?orgId=1\u0026from=1747609200000\u0026to=1756335599000\u0026var-period=d28\u0026var-repogroup_name=All\u0026var-repo_name=kubernetes%2Fkubernetes)\n\n## Event Update\n\nExplore upcoming Kubernetes and cloud native events, including KubeCon + CloudNativeCon, KCD, and other notable conferences worldwide. Stay informed and get involved with the Kubernetes community!\n\n**August 2025**\n\n- [**KCD - Kubernetes Community Days: Colombia**](https://community.cncf.io/events/details/cncf-kcd-colombia-presents-kcd-colombia-2025/): Aug 28, 2025 | Bogot√°, Colombia\n\n**September 2025**\n\n- [**CloudCon Sydney**](https://community.cncf.io/events/details/cncf-cloud-native-sydney-presents-cloudcon-sydney-sydney-international-convention-centre-910-september/): Sep 9‚Äì10, 2025 | Sydney, Australia.\n- [**KCD - Kubernetes Community Days: San Francisco Bay Area**](https://community.cncf.io/events/details/cncf-kcd-sf-bay-area-presents-kcd-san-francisco-bay-area/): Sep 9, 2025 | San Francisco, USA\n- [**KCD - Kubernetes Community Days: Washington DC**](https://community.cncf.io/events/details/cncf-kcd-washington-dc-presents-kcd-washington-dc-2025/): Sep 16, 2025 | Washington, D.C., USA\n- [**KCD - Kubernetes Community Days: Sofia**](https://community.cncf.io/events/details/cncf-kcd-sofia-presents-kubernetes-community-days-sofia/): Sep 18, 2025 | Sofia, Bulgaria\n- [**KCD - Kubernetes Community Days: El Salvador**](https://community.cncf.io/events/details/cncf-kcd-el-salvador-presents-kcd-el-salvador/): Sep 20, 2025 | San Salvador, El Salvador\n\n**October 2025**\n\n- [**KCD - Kubernetes Community Days: Warsaw**](https://community.cncf.io/events/details/cncf-kcd-warsaw-presents-kcd-warsaw-2025/): Oct 9, 2025 | Warsaw, Poland\n- [**KCD - Kubernetes Community Days: Edinburgh**](https://community.cncf.io/events/details/cncf-kcd-uk-presents-kubernetes-community-days-uk-edinburgh-2025/): Oct 21, 2025 | Edinburgh, United Kingdom\n- [**KCD - Kubernetes Community Days: Sri Lanka**](https://community.cncf.io/events/details/cncf-kcd-sri-lanka-presents-kcd-sri-lanka-2025/): Oct 26, 2025 | Colombo, Sri Lanka\n\n**November 2025**\n\n- [**KCD - Kubernetes Community Days: Porto**](https://community.cncf.io/events/details/cncf-kcd-porto-presents-kcd-porto-2025/): Nov 3, 2025 | Porto, Portugal\n- [**KubeCon + CloudNativeCon North America 2025**](https://events.linuxfoundation.org/kubecon-cloudnativecon-north-america/): Nov 10-13, 2025 | Atlanta, USA\n- [**KCD - Kubernetes Community Days: Hangzhou**](https://sessionize.com/kcd-hangzhou-and-oicd-2025/): Nov 15, 2025 | Hangzhou, China\n\n**December 2025**\n\n- [**KCD - Kubernetes Community Days: Suisse Romande**](https://community.cncf.io/events/details/cncf-kcd-suisse-romande-presents-kcd-suisse-romande/): Dec 4, 2025 | Geneva, Switzerland\n\nYou can find the latest event details [here](https://community.cncf.io/events/#/list).\n\n## Upcoming Release Webinar\n\nJoin members of the Kubernetes v1.34 Release Team on **Wednesday, September 24th 2025 at 4:00 PM (UTC)**, to learn about the release highlights of this release. For more information and registration, visit the [event page](https://community.cncf.io/events/details/cncf-cncf-online-programs-presents-cloud-native-live-kubernetes-v134-release/) on the CNCF Online Programs site.\n\n## Get Involved\n\nThe simplest way to get involved with Kubernetes is by joining one of the many [Special Interest Groups](https://github.com/kubernetes/community/blob/master/sig-list.md) (SIGs) that align with your interests. Have something you‚Äôd like to broadcast to the Kubernetes community? Share your voice at our weekly [community meeting](https://github.com/kubernetes/community/tree/master/communication), and through the channels below. Thank you for your continued feedback and support.\n\n- Follow us on Bluesky [@Kubernetesio](https://bsky.app/profile/kubernetes.io) for the latest updates\n- Join the community discussion on [Discuss](https://discuss.kubernetes.io/)\n- Join the community on [Slack](http://slack.k8s.io/)\n- Post questions (or answer questions) on [Stack Overflow](http://stackoverflow.com/questions/tagged/kubernetes)\n- Share your Kubernetes [story](https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform)\n- Read more about what‚Äôs happening with Kubernetes on the [blog](https://kubernetes.io/blog/)\n- Learn more about the [Kubernetes Release Team](https://github.com/kubernetes/sig-release/tree/master/release-team)","metadata":{"title":"Kubernetes v1.34: Of Wind \u0026 Will (O' WaW)","excerpt":"Kubernetes v1.34 introduces 58 enhancements including 23 stable features, from Dynamic Resource Allocation (DRA) GA to KYAML support. Discover the latest improvements in security, scheduling, and resource management in this comprehensive release.","date":"2025-08-27","author":"Graziano Casto","platform":"Kubernetes"}},{"slug":"ephemetal-cloud-with-crossplane-and-kubegreen","content":"\n[Published by DZONE](https://dzone.com/articles/infrastructure-crossplane-kube-green)\n\nWe were all sold a compelling vision of cloud computing: one filled with agility, endless scalability, and remarkable cost savings. Yet, for many of us in the trenches, the daily reality looks quite different. We find ourselves wrestling with an infrastructure model built on long-lived, static environments for development, testing, and staging. This old way of working has quietly become a massive drain on our resources, creating financial waste, operational headaches, and a growing list of security and environmental debts.\n\nThis isn't just one problem; it's a vicious cycle. The friction in our daily operations directly fuels the financial, security, and environmental burdens. To break free, we need more than just a new tool; we need to fundamentally rethink how we provision, manage, and consume infrastructure.\n\n## The High Cost of 'Always-On'\n\nThe most obvious symptom of this broken model is the money it wastes. It's not a small number; research suggests a staggering [30-45% of all cloud spending is wasted on idle or over-provisioned resources](https://42on.com/the-hidden-costs-of-cloud-waste-how-to-optimize-your-cloud-resources-and-reduce-environmental-impact/). This isn't surprising when you think about it. Our staging, UAT, and dev environments are built to handle peak loads, but they spend most of their lives (nights, weekends, holidays) doing absolutely nothing. This leads to an abysmal average CPU utilization of just 12-15%, a clear sign that something is deeply inefficient. This problem is made worse by \"cloud sprawl\", where new resources pop up uncontrolled, increasing costs, security holes, and complexity.\n\nThis financial drain is a direct result of operational friction. Many of us are familiar with the \"ticket-driven\" culture: a developer needs an environment, so they file a ticket and wait. And wait. This manual, slow process creates bottlenecks that grind the entire development lifecycle to a halt. In a world of complex microservices, setting these environments up by hand isn't just slow; it's a recipe for errors. This model encourages us to keep these costly, complex environments running indefinitely, because tearing them down and rebuilding them is just too painful.\n\nThis persistence creates a huge security risk. Over time, these static environments suffer from \"configuration drift.\" Manual tweaks, untracked updates, and inconsistent patching mean they no longer match production or even each other. They become, as one source aptly put it, a [\"hive of duplicated data, exploitable vulnerabilities, and misconfigurations\"](https://www.quali.com/glossary/ephemeral-environments/). Each one is a persistent attack surface, a liability that only grows with time.\n\n## The Unseen Environmental Toll\n\nThe money we waste on idle resources is just the tip of the iceberg. Under the surface lies a massive, often ignored, environmental cost. Every idle virtual machine or database isn't just a line item on a bill; it's a physical server in a data center, burning electricity and pumping out CO2 for no reason. Data centers already consume about [1% of the world's electricity](https://www.researchgate.net/publication/390569272_The_Environmental_Impact_of_Cloud_Computing_Sustainability_in_the_Cloud), and that number is only going up.\n\nTo put this in perspective, a 2019 study found that training a single large AI model can produce the same carbon emissions as [more than 300 cross-country flights](https://www.forbes.com/councils/forbestechcouncil/2025/02/20/the-unsung-mechanics-how-ai-and-cloud-integration-drive-the-engine-of-sustainability/). This is the energy cost of our digital world. Beyond emissions, there's e-waste from constant hardware upgrades and the immense amount of water used for cooling. While cloud providers are more efficient than on-premise data centers, the core problem remains: we, the customers, are provisioning resources that sit idle, driving unnecessary environmental impact.\n\nLet's make this real. Imagine a team of 10 engineers with a single, static staging environment. It runs 24/7, but they only use it for about 160 hours a month. For the other 560 hours, nearly 78% of the time, it's idle but still racking up costs. Using the CO2 calculator from the [kube-green project](https://kube-green.dev/), which assumes that a single pod produces about [11 kg of CO2eq per year](https://kube-green.dev/docs/FAQ/#how-much-co2-is-produced-by-a-pod), we can see the impact. If that staging environment runs 20 pods, it generates 220 kg of CO2eq annually. By simply shutting it down when it's idle, the team could prevent over 170 kg of CO2eq emissions. Now, scale that across an entire organization. The waste is enormous.\n\n## The Ephemeral Revolution\n\nThis is where a new idea comes in: the ephemeral environment. It‚Äôs a complete shift in thinking. Instead of permanent, long-lived infrastructure, we move to on-demand, temporary workspaces that directly combat waste and friction. An ephemeral environment is a short-lived, isolated, and complete copy of your application, spun up for a specific task and automatically destroyed when you're done.\n\nThis approach is built on a few key ideas. First, it's **on-demand and automated**. Environments are created by events in your workflow, like opening a pull request, not by filing a ticket. What once took days now takes minutes. Second, each environment is **completely isolated**. This means no more \"who broke the staging server?\" A developer can test a new feature, run a database migration, or experiment with a dependency without affecting anyone else. This enables true parallel development.\n\nCrucially, these are not just simple \"preview environments\". A true ephemeral environment achieves **production-parity**. It‚Äôs a full-fidelity clone of your production stack: backend services, databases, configurations, everything. This is how you kill the \"it works on my machine\" problem for good. Finally, these environments are both **shareable and disposable**. You get a unique URL to share with stakeholders for feedback on a live, running feature. When the pull request is merged, the entire environment is wiped away as if it never existed. This disposable nature is the key to its efficiency.\n\nAdopting this model has a transformative impact. It dramatically boosts developer velocity by removing bottlenecks. Some organizations have seen [delivery times improve by up to 35%](https://ephemeralenvironments.io/) and overall velocity increase by as much as 10x. It fosters better collaboration because everyone, from product managers to QA, can interact with a tangible, running feature, not just mockups. This \"shift-left\" approach to testing also means you catch bugs earlier, significantly improving code quality and reducing the risk of production failures. The old static staging environment is a relic of an old era: costly, slow, and risky. The ephemeral model is its opposite: on-demand, fast, safe, and efficient.\n\n## The Technology Pillars: Crossplane and kube-green\n\nThis vision of a fully ephemeral cloud requires the right technology. You need to be able to programmatically control not just your application containers, but the entire infrastructure ecosystem: databases, networks, storage, and more. This is where two key open-source projects come into play: Crossplane and kube-green.\n\nCrossplane is the heart of this architecture. It‚Äôs a universal control plane that extends the Kubernetes API to manage any cloud resource, not just containers. This represents a profound shift from traditional [Infrastructure as Code (IaC)](https://dzone.com/articles/what-is-infrastructure-as-code) tools like Terraform to a new model: [Infrastructure as Data (IaD)](https://medium.com/@swarupdonepudi/infrastructure-as-data-iad-coincidence-2ca6c70f896e).\n\nWith traditional IaC, you run a command like terraform apply to make a one-time change. With Crossplane, you declare the desired state of your infrastructure in a YAML file and submit it to the Kubernetes API. From then on, Crossplane's controllers work continuously to ensure reality matches your declaration. If anything drifts, Crossplane automatically corrects it. This \"self-healing\" capability is a game-changer.\n\nIt works through a clever, layered architecture. **Providers** teach Crossplane how to talk to a specific API, like AWS or GCP. **Managed Resources** are the building blocks, representing a single piece of infrastructure like an S3 bucket or an RDS database. But the real magic is in **Compositions**. A platform team can use a Composition to bundle all the necessary Managed Resources into a single, high-level abstraction, like a PostgreSQLInstance. This blueprint can embed all the company's best practices for security, networking, and compliance.\n\nDevelopers don't need to worry about that complexity. They simply request what they need by creating a **Claim**: a simple YAML asking for a PostgreSQLInstance for example. Crossplane sees the claim and provisions the entire, production-ready stack automatically. This is true developer self-service, and it's exactly what's needed for ephemeral environments. When a pull request is opened, a CI/CD pipeline can create a single Claim that tells Crossplane to spin up a dedicated database, message queue, and everything else the application needs. When the PR is closed, deleting that one Claim tears it all down.\n\nWhile Crossplane manages the existence of the environment, [kube-green](https://kube-green.dev/) handles the efficiency within that existence. An ephemeral environment might live for a few days, but developers aren't working on it 24/7. kube-green is a simple, brilliant utility that solves this \"last mile\" problem by automatically hibernating resources on a schedule.\n\nIt operates as a Kubernetes operator that you configure with a **SleepInfo** resource in each namespace. You can tell it to shut things down at 8 PM on weekdays and wake them up at 8 AM, for example. When the sleep time hits, kube-green scales down your Deployments and suspends your CronJobs. When the wake-up time arrives, it brings everything back to its original state. This simple action can lead to huge savings. Adopters have reported [30-40% reductions in cloud costs](https://kube-green.dev/docs/adopters/) for non-production clusters. And because it reduces energy consumption, it has a direct, positive impact on your carbon footprint.\n\n## The Synthesis: A New Blueprint for Efficiency\n\nWhen you combine Crossplane's strategic, full-stack orchestration with kube-green's tactical, time-based optimization, you get something truly powerful. This synergy creates a multi-layered system of efficiency that aligns your infrastructure cost and carbon footprint with the actual, value-generating work of your development teams.\n\nThe workflow, driven by GitOps tools like ArgoCD, is beautifully simple. A developer opens a pull request. This triggers a CI pipeline that generates the manifests for the new environment, including a Crossplane Claim for the infrastructure and a kube-green SleepInfo for the schedule. These manifests are committed to a Git repository. ArgoCD sees the new files and applies them to your management cluster. Crossplane springs into action, provisioning the entire infrastructure stack, perhaps in a lightweight vCluster for perfect isolation. Once the infrastructure is ready, ArgoCD deploys the application code. When the PR is merged or closed, a CI job removes the manifests from Git, and the entire environment vanishes.\n\nThis creates a two-tiered optimization model. **Macro-optimization** from Crossplane ensures you only pay for infrastructure for the few days a PR is active, not for a permanent staging environment. **Micro-optimization** from kube-green then eliminates waste within that short lifespan by hibernating resources during non-working hours.\n\nThis points to an exciting point: a \"deep sleep\" model. Using kube-green's ability to patch any Kubernetes resource, you could have it not only scale down pods but also patch the Crossplane Claim itself, for example, telling it to scale a database cluster down to a single, low-cost node overnight. This would extend time-based optimization to the entire stateful infrastructure stack, achieving the ultimate form of on-demand efficiency.\n\n## Getting Started on Your Journey\n\nWhile this vision of a fully automated, per-PR ephemeral world is the goal, you don't have to build it all at once. The journey can start with a single, high-impact step. To make these concepts tangible, I've put together a [hands-on tutorial](https://github.com/graz-dev/crossplane-ephemeral-environments) that focuses on a pragmatic starting point: creating on-demand infrastructure that's only active when you need it.\n\nThe tutorial walks you through using Crossplane to provision an AWS EKS cluster and then using kube-green to put it on a sleep/wake schedule. It's a direct, practical way to tackle the biggest source of waste: idle resources.\n\nIf this vision for a more efficient cloud resonates with you, the best way to understand it is to build it. I invite you to check out the tutorial, [Ephemeral Cloud Resources with Crossplane and kube-green](https://github.com/graz-dev/crossplane-ephemeral-environments). Dive in, experiment with the code, and see the benefits for yourself. If you find it valuable, please give the repository a star on GitHub and share this article with your network.\n\n## A Strategic Outlook for the Future\n\nThe shift to an ephemeral cloud model is more than a technical upgrade; it's a strategic evolution. It allows us to move from passively managing static resources to actively orchestrating dynamic, value-generating capabilities. The future of this model is even more exciting, driven by AI-powered optimization and the formalization of GreenOps.\n\nNew AI tools like StormForge and CAST AI are emerging that can analyze workloads to automatically tune resource configurations and predict demand. Imagine an AI that could predict a PR's review cycle to allocate resources more intelligently or manage energy use based on the real-time carbon intensity of the power grid.\n\nThis model is also a foundational practice for GreenOps, the discipline of bringing environmental accountability to cloud operations. As sustainability becomes a key business metric, the ability to measure and minimize the carbon footprint of software development will be a major differentiator.\n\nFor technology leaders, the path forward is clear. Treat your internal infrastructure as a product and invest in a platform team to build out this capability. Foster a culture of accountability by making cost and carbon data transparent to your engineering teams. And start small. Pick a single service and use it as a pilot project to prove the value and build expertise.\n\nThe combination of Crossplane and kube-green offers a powerful and pragmatic toolkit to begin this journey. By adopting this new blueprint, you can accelerate innovation while dramatically reducing your financial and environmental costs.","metadata":{"title":"The Ephemeral Cloud: A New Blueprint for Infrastructure Efficiency With Crossplane and kube-green","excerpt":"Use Crossplane and kube-green to replace wasteful, always-on cloud environments with on-demand, ephemeral ones, dramatically cutting costs and environmental impact.","date":"2025-08-25","author":"Graziano Casto","platform":"DZONE"}}]},"schema":{"@context":"https://schema.org","@type":"WebSite","name":"Graziano Casto WebSite","alternateName":"Graziano's WebSite","url":"https://castograziano.com"}},"__N_SSG":true},"page":"/","query":{},"buildId":"aI1FMStRJNvpGWbdYQdRM","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>