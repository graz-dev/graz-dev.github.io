<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><link rel="preload" href="/_next/static/media/graziano-casto.17cc67b9.jpg" as="image" fetchpriority="high"/><title>Graziano Casto - DevRel Engineer</title><script type="application/ld+json">{"@context":"https://schema.org","@type":"WebSite","name":"Graziano Casto WebSite","alternateName":"Graziano's WebSite","url":"https://castograziano.com"}</script><meta name="description" content="Welcome to the personal website of Graziano Casto."/><link rel="canonical" href="https://www.castograziano.comhttps://castograziano.com"/><meta property="og:title" content="Graziano Casto - DevRel Engineer"/><meta property="og:description" content="Welcome to the personal website of Graziano Casto."/><meta property="og:url" content="https://www.castograziano.comhttps://castograziano.com"/><meta property="og:image" content="https://castograziano.com/casto-graziano.jpg"/><meta property="og:type" content="article"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:title" content="Graziano Casto - DevRel Engineer"/><meta name="twitter:description" content="Welcome to the personal website of Graziano Casto."/><meta name="twitter:image" content="https://castograziano.com/casto-graziano.jpg"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="icon" href="/images/favicon.ico"/><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png"/><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png"/><meta name="next-head-count" content="20"/><link rel="preload" href="/_next/static/css/f47e3e4f61fa5b21.css" as="style"/><link rel="stylesheet" href="/_next/static/css/f47e3e4f61fa5b21.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-78c92fac7aa8fdd8.js"></script><script src="/_next/static/chunks/webpack-0b5d8249fb15f5f3.js" defer=""></script><script src="/_next/static/chunks/framework-2c16ac744b6cdea6.js" defer=""></script><script src="/_next/static/chunks/main-69b16c27ce463005.js" defer=""></script><script src="/_next/static/chunks/pages/_app-666f3c6bf21653ac.js" defer=""></script><script src="/_next/static/chunks/762-f4dac766814a8955.js" defer=""></script><script src="/_next/static/chunks/pages/index-8bb665e247d2d067.js" defer=""></script><script src="/_next/static/SYKRgUHRNYLLx0g7j2Qzh/_buildManifest.js" defer=""></script><script src="/_next/static/SYKRgUHRNYLLx0g7j2Qzh/_ssgManifest.js" defer=""></script></head><body><div id="__next"><script>!function(){try{var d=document.documentElement,c=d.classList;c.remove('light','dark');var e=localStorage.getItem('theme');if('system'===e||(!e&&true)){var t='(prefers-color-scheme: dark)',m=window.matchMedia(t);if(m.media!==t||m.matches){d.style.colorScheme = 'dark';c.add('dark')}else{d.style.colorScheme = 'light';c.add('light')}}else if(e){c.add(e|| '')}if(e==='light'||e==='dark')d.style.colorScheme=e}catch(e){}}()</script><div class="flex w-full"><div class="fixed inset-0 flex justify-center sm:px-8"><div class="flex w-full max-w-7xl lg:px-8"><div class="w-full bg-white ring-1 ring-zinc-100 dark:bg-zinc-900 dark:ring-zinc-300/20"></div></div></div><div class="relative flex w-full flex-col"><header class="pointer-events-none relative z-50 flex flex-none flex-col" style="height:var(--header-height);margin-bottom:var(--header-mb)"><div class="order-last mt-[calc(theme(spacing.16)-theme(spacing.3))]"></div><div class="sm:px-8 top-0 order-last -mb-3 pt-3" style="position:var(--header-position)"><div class="mx-auto w-full max-w-7xl lg:px-8"><div class="relative px-4 sm:px-8 lg:px-12"><div class="mx-auto max-w-2xl lg:max-w-5xl"><div class="top-[var(--avatar-top,theme(spacing.3))] w-full" style="position:var(--header-inner-position)"><div class="relative"><div class="absolute left-0 top-3 origin-left transition-opacity h-10 w-10 rounded-full bg-white/90 p-0.5 shadow-lg shadow-zinc-800/5 ring-1 ring-zinc-900/5 backdrop-blur dark:bg-zinc-800/90 dark:ring-white/10" style="opacity:var(--avatar-border-opacity, 0);transform:var(--avatar-border-transform)"></div><a aria-label="Home" class="block h-16 w-16 origin-left pointer-events-auto" style="transform:var(--avatar-image-transform)" href="/"><img alt="" fetchpriority="high" width="1872" height="1914" decoding="async" data-nimg="1" class="rounded-full bg-zinc-100 object-cover dark:bg-zinc-800 h-16 w-16" style="color:transparent" src="/_next/static/media/graziano-casto.17cc67b9.jpg"/></a></div></div></div></div></div></div><div class="top-0 z-10 h-16 pt-6" style="position:var(--header-position)"><div class="sm:px-8 top-[var(--header-top,theme(spacing.6))] w-full" style="position:var(--header-inner-position)"><div class="mx-auto w-full max-w-7xl lg:px-8"><div class="relative px-4 sm:px-8 lg:px-12"><div class="mx-auto max-w-2xl lg:max-w-5xl"><div class="relative flex gap-4"><div class="flex flex-1"></div><div class="flex flex-1 justify-end md:justify-center"><div class="pointer-events-auto md:hidden" data-headlessui-state=""><button class="group flex items-center rounded-full bg-white/90 px-4 py-2 text-sm font-medium text-zinc-800 shadow-lg shadow-zinc-800/5 ring-1 ring-zinc-900/5 backdrop-blur dark:bg-zinc-800/90 dark:text-zinc-200 dark:ring-white/10 dark:hover:ring-white/20" type="button" aria-expanded="false" data-headlessui-state="">Menu<svg viewBox="0 0 8 6" aria-hidden="true" class="ml-3 h-auto w-2 stroke-zinc-500 group-hover:stroke-zinc-700 dark:group-hover:stroke-zinc-400"><path d="M1.75 1.75 4 4.25l2.25-2.5" fill="none" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></button></div><div hidden="" style="position:fixed;top:1px;left:1px;width:1px;height:0;padding:0;margin:-1px;overflow:hidden;clip:rect(0, 0, 0, 0);white-space:nowrap;border-width:0;display:none"></div><nav class="pointer-events-auto hidden md:block"><ul class="flex rounded-full bg-white/90 px-3 text-sm font-medium text-zinc-800 shadow-lg shadow-zinc-800/5 ring-1 ring-zinc-900/5 backdrop-blur dark:bg-zinc-800/90 dark:text-zinc-200 dark:ring-white/10"><li><a class="relative block px-3 py-2 transition hover:text-teal-500 dark:hover:text-teal-400" href="/about-me/">About</a></li><li><a class="relative block px-3 py-2 transition hover:text-teal-500 dark:hover:text-teal-400" href="/articles/">Articles</a></li><li><a class="relative block px-3 py-2 transition hover:text-teal-500 dark:hover:text-teal-400" href="/speaking/">Speaking</a></li><li><a class="relative block px-3 py-2 transition hover:text-teal-500 dark:hover:text-teal-400" href="/projects/">Projects</a></li></ul></nav></div><div class="flex justify-end md:flex-1"><div class="pointer-events-auto"><button type="button" aria-label="Toggle theme" class="group rounded-full bg-white/90 px-3 py-2 shadow-lg shadow-zinc-800/5 ring-1 ring-zinc-900/5 backdrop-blur transition dark:bg-zinc-800/90 dark:ring-white/10 dark:hover:ring-white/20"><svg viewBox="0 0 24 24" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" aria-hidden="true" class="h-6 w-6 fill-zinc-100 stroke-zinc-500 transition group-hover:fill-zinc-200 group-hover:stroke-zinc-700 dark:hidden [@media(prefers-color-scheme:dark)]:fill-teal-50 [@media(prefers-color-scheme:dark)]:stroke-teal-500 [@media(prefers-color-scheme:dark)]:group-hover:fill-teal-50 [@media(prefers-color-scheme:dark)]:group-hover:stroke-teal-600"><path d="M8 12.25A4.25 4.25 0 0 1 12.25 8v0a4.25 4.25 0 0 1 4.25 4.25v0a4.25 4.25 0 0 1-4.25 4.25v0A4.25 4.25 0 0 1 8 12.25v0Z"></path><path d="M12.25 3v1.5M21.5 12.25H20M18.791 18.791l-1.06-1.06M18.791 5.709l-1.06 1.06M12.25 20v1.5M4.5 12.25H3M6.77 6.77 5.709 5.709M6.77 17.73l-1.061 1.061" fill="none"></path></svg><svg viewBox="0 0 24 24" aria-hidden="true" class="hidden h-6 w-6 fill-zinc-700 stroke-zinc-500 transition dark:block [@media(prefers-color-scheme:dark)]:group-hover:stroke-zinc-400 [@media_not_(prefers-color-scheme:dark)]:fill-teal-400/10 [@media_not_(prefers-color-scheme:dark)]:stroke-teal-500"><path d="M17.25 16.22a6.937 6.937 0 0 1-9.47-9.47 7.451 7.451 0 1 0 9.47 9.47ZM12.75 7C17 7 17 2.75 17 2.75S17 7 21.25 7C17 7 17 11.25 17 11.25S17 7 12.75 7Z" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></button></div></div></div></div></div></div></div></div></header><div class="flex-none" style="height:var(--content-offset)"></div><main class="flex-auto"><div class="sm:px-8 mt-9"><div class="mx-auto w-full max-w-7xl lg:px-8"><div class="relative px-4 sm:px-8 lg:px-12"><div class="mx-auto max-w-2xl lg:max-w-5xl"><div class="max-w-2xl"><h1 class="text-4xl font-bold tracking-tight text-zinc-800 sm:text-5xl dark:text-zinc-100">Graziano Casto üëãüèª</h1><p class="mt-6 text-base text-zinc-600 dark:text-zinc-400"> Developer Relations | Kubernetes v1.35 Release Comms Lead | CNCF TAG Developer Experience Tech Lead | Cloud Native Days Italy Organizer | OSS &amp; Cloud Native Advocate </p><div class="mt-6 flex gap-6"><a class="group -m-1 p-1" aria-label="Follow on GitHub" target="_blank" rel="noopener noreferrer" href="https://github.com/graz-dev"><svg viewBox="0 0 24 24" aria-hidden="true" class="h-6 w-6 fill-zinc-500 transition group-hover:fill-zinc-600 dark:fill-zinc-400 dark:group-hover:fill-zinc-300"><path fill-rule="evenodd" clip-rule="evenodd" d="M12 2C6.475 2 2 6.588 2 12.253c0 4.537 2.862 8.369 6.838 9.727.5.09.687-.218.687-.487 0-.243-.013-1.05-.013-1.91C7 20.059 6.35 18.957 6.15 18.38c-.113-.295-.6-1.205-1.025-1.448-.35-.192-.85-.667-.013-.68.788-.012 1.35.744 1.538 1.051.9 1.551 2.338 1.116 2.912.846.088-.666.35-1.115.638-1.371-2.225-.256-4.55-1.14-4.55-5.062 0-1.115.387-2.038 1.025-2.756-.1-.256-.45-1.307.1-2.717 0 0 .837-.269 2.75 1.051.8-.23 1.65-.346 2.5-.346.85 0 1.7.115 2.5.346 1.912-1.333 2.75-1.05 2.75-1.05.55 1.409.2 2.46.1 2.716.637.718 1.025 1.628 1.025 2.756 0 3.934-2.337 4.806-4.562 5.062.362.32.675.936.675 1.897 0 1.371-.013 2.473-.013 2.82 0 .268.188.589.688.486a10.039 10.039 0 0 0 4.932-3.74A10.447 10.447 0 0 0 22 12.253C22 6.588 17.525 2 12 2Z"></path></svg></a><a class="group -m-1 p-1" aria-label="Follow on LinkedIn" target="_blank" rel="noopener noreferrer" href="https://www.linkedin.com/in/castograziano/"><svg viewBox="0 0 24 24" aria-hidden="true" class="h-6 w-6 fill-zinc-500 transition group-hover:fill-zinc-600 dark:fill-zinc-400 dark:group-hover:fill-zinc-300"><path d="M18.335 18.339H15.67v-4.177c0-.996-.02-2.278-1.39-2.278-1.389 0-1.601 1.084-1.601 2.205v4.25h-2.666V9.75h2.56v1.17h.035c.358-.674 1.228-1.387 2.528-1.387 2.7 0 3.2 1.778 3.2 4.091v4.715zM7.003 8.575a1.546 1.546 0 01-1.548-1.549 1.548 1.548 0 111.547 1.549zm1.336 9.764H5.666V9.75H8.34v8.589zM19.67 3H4.329C3.593 3 3 3.58 3 4.297v15.406C3 20.42 3.594 21 4.328 21h15.338C20.4 21 21 20.42 21 19.703V4.297C21 3.58 20.4 3 19.666 3h.003z"></path></svg></a></div></div></div></div></div></div><div class="sm:px-8 mt-24 md:mt-28"><div class="mx-auto w-full max-w-7xl lg:px-8"><div class="relative px-4 sm:px-8 lg:px-12"><div class="mx-auto max-w-2xl lg:max-w-5xl"><div class="mx-auto grid max-w-4xl grid-cols-1 gap-y-20 lg:max-w-none lg:grid-cols-2 lg:gap-x-12"><div class="rounded-2xl border border-zinc-100 p-6 dark:border-zinc-700/40"><h2 class="text-sm font-semibold text-zinc-900 dark:text-zinc-100">Latest Articles</h2><div class="mt-6 space-y-8"><a class="block" href="/articles/kcd-nyc-pe-observability-roundtable/"><article class="group relative flex flex-col items-start"><div class="group relative flex flex-col items-start"><h2 class="text-base font-semibold tracking-tight text-zinc-800 dark:text-zinc-100">From Chaos to Clarity: Navigating Observability in the Platform Engineering Era (and a Dash of AI)</h2><time class="relative z-10 order-first mb-3 flex items-center text-sm text-zinc-400 dark:text-zinc-500 pl-3.5" dateTime="2025-08-20"><span class="absolute inset-y-0 left-0 flex items-center" aria-hidden="true"><span class="h-4 w-0.5 rounded-full bg-zinc-200 dark:bg-zinc-500"></span></span>August 20, 2025</time><p class="relative z-10 mt-2 text-sm text-zinc-600 dark:text-zinc-400">A comprehensive recap of the KCD New York roundtable discussion on Platform Engineering and Observability, exploring key challenges, solutions, and the role of AI in modern observability practices.</p><div aria-hidden="true" class="relative z-10 mt-2 flex items-center text-sm font-medium text-teal-500">Read article<svg viewBox="0 0 16 16" fill="none" aria-hidden="true" class="ml-1 h-4 w-4 stroke-current"><path d="M6.75 5.75 9.25 8l-2.5 2.25" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></div></div></article></a><a class="block" href="/articles/wasm-next-universal-runtime/"><article class="group relative flex flex-col items-start"><div class="group relative flex flex-col items-start"><h2 class="text-base font-semibold tracking-tight text-zinc-800 dark:text-zinc-100">WebAssembly: From Browser Plugin to the Next Universal Runtime</h2><time class="relative z-10 order-first mb-3 flex items-center text-sm text-zinc-400 dark:text-zinc-500 pl-3.5" dateTime="2025-08-04"><span class="absolute inset-y-0 left-0 flex items-center" aria-hidden="true"><span class="h-4 w-0.5 rounded-full bg-zinc-200 dark:bg-zinc-500"></span></span>August 4, 2025</time><p class="relative z-10 mt-2 text-sm text-zinc-600 dark:text-zinc-400">Explore WebAssembly&#x27;s evolution from a browser performance booster to a universal runtime reshaping cloud, edge, and distributed computing with near-native performance across platforms.</p><div aria-hidden="true" class="relative z-10 mt-2 flex items-center text-sm font-medium text-teal-500">Read article<svg viewBox="0 0 16 16" fill="none" aria-hidden="true" class="ml-1 h-4 w-4 stroke-current"><path d="M6.75 5.75 9.25 8l-2.5 2.25" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></div></div></article></a><a class="block" href="/articles/kubernetes-v134-sneak-peak/"><article class="group relative flex flex-col items-start"><div class="group relative flex flex-col items-start"><h2 class="text-base font-semibold tracking-tight text-zinc-800 dark:text-zinc-100">Kubernetes v1.34 Sneak Peek</h2><time class="relative z-10 order-first mb-3 flex items-center text-sm text-zinc-400 dark:text-zinc-500 pl-3.5" dateTime="2025-07-28"><span class="absolute inset-y-0 left-0 flex items-center" aria-hidden="true"><span class="h-4 w-0.5 rounded-full bg-zinc-200 dark:bg-zinc-500"></span></span>July 28, 2025</time><p class="relative z-10 mt-2 text-sm text-zinc-600 dark:text-zinc-400">Discover the exciting enhancements coming in Kubernetes v1.34, including stable Dynamic Resource Allocation, improved scheduling capabilities, and enhanced security features for production workloads.</p><div aria-hidden="true" class="relative z-10 mt-2 flex items-center text-sm font-medium text-teal-500">Read article<svg viewBox="0 0 16 16" fill="none" aria-hidden="true" class="ml-1 h-4 w-4 stroke-current"><path d="M6.75 5.75 9.25 8l-2.5 2.25" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></div></div></article></a><a class="block" href="/articles/ai-native-platforms/"><article class="group relative flex flex-col items-start"><div class="group relative flex flex-col items-start"><h2 class="text-base font-semibold tracking-tight text-zinc-800 dark:text-zinc-100">AI-Native Platforms: The Unstoppable Alliance of GenAI and Platform Engineering</h2><time class="relative z-10 order-first mb-3 flex items-center text-sm text-zinc-400 dark:text-zinc-500 pl-3.5" dateTime="2025-06-11"><span class="absolute inset-y-0 left-0 flex items-center" aria-hidden="true"><span class="h-4 w-0.5 rounded-full bg-zinc-200 dark:bg-zinc-500"></span></span>June 11, 2025</time><p class="relative z-10 mt-2 text-sm text-zinc-600 dark:text-zinc-400">Discover how Generative AI is transforming platform engineering from static toolsets into intelligent, dynamic, and self-optimizing ecosystems that unlock 100% of platform potential.</p><div aria-hidden="true" class="relative z-10 mt-2 flex items-center text-sm font-medium text-teal-500">Read article<svg viewBox="0 0 16 16" fill="none" aria-hidden="true" class="ml-1 h-4 w-4 stroke-current"><path d="M6.75 5.75 9.25 8l-2.5 2.25" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></div></div></article></a></div></div><div class="space-y-10"><div class="rounded-2xl border border-zinc-100 p-6 dark:border-zinc-700/40"><h2 class="text-sm font-semibold text-zinc-900 dark:text-zinc-100">Upcoming Talks</h2><ol class="mt-6 space-y-4"><li class="flex gap-4"><div class="relative mt-1 flex h-10 w-10 flex-none items-center justify-center rounded-full shadow-md shadow-zinc-800/5 ring-1 ring-zinc-900/5 dark:border dark:border-zinc-700/50 dark:bg-zinc-800 dark:ring-0"><svg class="h-6 w-6 text-zinc-400 dark:text-zinc-500" viewBox="0 0 24 24" fill="none" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"><path d="M12 2a3 3 0 0 0-3 3v7a3 3 0 0 0 6 0V5a3 3 0 0 0-3-3Z" class="fill-zinc-100 stroke-zinc-400 dark:fill-zinc-100/10 dark:stroke-zinc-500"></path><path d="M19 10v2a7 7 0 0 1-14 0v-2" class="stroke-zinc-400 dark:stroke-zinc-500"></path><path d="M12 18v4M8 22h8" class="stroke-zinc-400 dark:stroke-zinc-500"></path></svg></div><dl class="flex flex-auto flex-wrap gap-x-2"><dt class="sr-only">Event</dt><dd class="w-full flex-none text-sm font-medium text-zinc-900 dark:text-zinc-100"><a target="_blank" rel="noopener noreferrer" href="https://community.cncf.io/events/details/cncf-kcd-porto-presents-kcd-porto-2025/">KCD Porto</a></dd><dt class="sr-only">Talk Title</dt><dd class="text-xs text-zinc-500 dark:text-zinc-400">Serving LLMs on Kubernetes: A Golden Path for Platform Engineers</dd><dt class="sr-only">Location and Date</dt><dd class="text-xs text-zinc-500 dark:text-zinc-400">üáµüáπ Porto<!-- --> - <time dateTime="2025-11-03">November 3, 2025</time></dd></dl></li><li class="flex gap-4"><div class="relative mt-1 flex h-10 w-10 flex-none items-center justify-center rounded-full shadow-md shadow-zinc-800/5 ring-1 ring-zinc-900/5 dark:border dark:border-zinc-700/50 dark:bg-zinc-800 dark:ring-0"><svg class="h-6 w-6 text-zinc-400 dark:text-zinc-500" viewBox="0 0 24 24" fill="none" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"><path d="M12 2a3 3 0 0 0-3 3v7a3 3 0 0 0 6 0V5a3 3 0 0 0-3-3Z" class="fill-zinc-100 stroke-zinc-400 dark:fill-zinc-100/10 dark:stroke-zinc-500"></path><path d="M19 10v2a7 7 0 0 1-14 0v-2" class="stroke-zinc-400 dark:stroke-zinc-500"></path><path d="M12 18v4M8 22h8" class="stroke-zinc-400 dark:stroke-zinc-500"></path></svg></div><dl class="flex flex-auto flex-wrap gap-x-2"><dt class="sr-only">Event</dt><dd class="w-full flex-none text-sm font-medium text-zinc-900 dark:text-zinc-100"><a target="_blank" rel="noopener noreferrer" href="https://www.containerday.it/talk/the-kubernetes-compass-come-navigare-spazio-e-tempo-per-essere-piu-sostenibili/?_gl=1*9vcxdj*_up*MQ..*_ga*MTU3OTMxMzgxNS4xNzUzMTE5Mzk4*_ga_GH3N0VLD75*czE3NTMxMTkzOTUkbzEkZzAkdDE3NTMxMTkzOTUkajYwJGwwJGgw">Container Day</a></dd><dt class="sr-only">Talk Title</dt><dd class="text-xs text-zinc-500 dark:text-zinc-400">The Kubernetes Compass: Navigating Space and Time for Energy Efficiency</dd><dt class="sr-only">Location and Date</dt><dd class="text-xs text-zinc-500 dark:text-zinc-400">üáÆüáπ Bologna<!-- --> - <time dateTime="2025-11-05">November 5, 2025</time></dd></dl></li><li class="flex gap-4"><div class="relative mt-1 flex h-10 w-10 flex-none items-center justify-center rounded-full shadow-md shadow-zinc-800/5 ring-1 ring-zinc-900/5 dark:border dark:border-zinc-700/50 dark:bg-zinc-800 dark:ring-0"><svg class="h-6 w-6 text-zinc-400 dark:text-zinc-500" viewBox="0 0 24 24" fill="none" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"><path d="M12 2a3 3 0 0 0-3 3v7a3 3 0 0 0 6 0V5a3 3 0 0 0-3-3Z" class="fill-zinc-100 stroke-zinc-400 dark:fill-zinc-100/10 dark:stroke-zinc-500"></path><path d="M19 10v2a7 7 0 0 1-14 0v-2" class="stroke-zinc-400 dark:stroke-zinc-500"></path><path d="M12 18v4M8 22h8" class="stroke-zinc-400 dark:stroke-zinc-500"></path></svg></div><dl class="flex flex-auto flex-wrap gap-x-2"><dt class="sr-only">Event</dt><dd class="w-full flex-none text-sm font-medium text-zinc-900 dark:text-zinc-100"><a target="_blank" rel="noopener noreferrer" href="https://events.linuxfoundation.org/kubecon-cloudnativecon-north-america/co-located-events/cncf-hosted-co-located-events-overview/">Kubecon NA - BackstageCon</a></dd><dt class="sr-only">Talk Title</dt><dd class="text-xs text-zinc-500 dark:text-zinc-400">PANEL | The Future of Developer Portals</dd><dt class="sr-only">Location and Date</dt><dd class="text-xs text-zinc-500 dark:text-zinc-400">üá∫üá∏ Atlanta, GA<!-- --> - <time dateTime="2025-11-10">November 10, 2025</time></dd></dl></li></ol></div></div></div></div></div></div></div></main><footer class="mt-32 flex-none"><div class="sm:px-8"><div class="mx-auto w-full max-w-7xl lg:px-8"><div class="border-t border-zinc-100 pb-16 pt-10 dark:border-zinc-700/40"><div class="relative px-4 sm:px-8 lg:px-12"><div class="mx-auto max-w-2xl lg:max-w-5xl"><div class="flex flex-col items-center justify-between gap-6 sm:flex-row"><div class="flex flex-wrap justify-center gap-x-6 gap-y-1 text-sm font-medium text-zinc-800 dark:text-zinc-200"><a class="transition hover:text-teal-500 dark:hover:text-teal-400" href="/about-me/">About</a><a class="transition hover:text-teal-500 dark:hover:text-teal-400" href="/projects/">Projects</a><a class="transition hover:text-teal-500 dark:hover:text-teal-400" href="/speaking/">Speaking</a></div><p class="text-sm text-zinc-500 dark:text-zinc-400">¬© <!-- -->2025<!-- --> Graziano Casto. All rights reserved.</p></div></div></div></div></div></div></footer></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"data":{"pageTitle":"Graziano Casto - DevRel Engineer","pageDescription":"Welcome to the personal website of Graziano Casto.","pageLink":"https://castograziano.com","pageImage":"https://castograziano.com/casto-graziano.jpg","articles":[{"slug":"kcd-nyc-pe-observability-roundtable","content":"\n[Published by Cloud Native PE Community](https://cloudnativeplatforms.com/blog/kcd-nyc-platform-engineering-and-observability-roundtable/)\n\n\u003e There was a great energy at KCD New York this year, and for Graziano Casto, a personal highlight was leading a roundtable on observability. It was a fascinating discussion that really got him thinking about the challenges we‚Äôre all facing in the platform engineering space. Here is Graziano‚Äôs recap of the key problems and promising ideas that came up.\n\n### Introduction\n\nIt was an absolute pleasure recently to moderate a roundtable at [KCD New York](https://community.cncf.io/events/details/cncf-kcd-new-york-presents-kcd-new-york-2025/), diving deep into the fascinating (and let‚Äôs be honest, sometimes frustrating) world where **Platform Engineering meets Observability**. As my first time moderating a roundtable, I was genuinely thrilled by the energy and candid participation from everyone in the room. A huge thank you to all the participants: [Michel Murabito](https://www.linkedin.com/in/mich-murabito/), [Colin Lacy](https://www.linkedin.com/in/colinjlacy/), [Tiara Sykes](https://www.linkedin.com/in/tiara-sykes/), [Andrew Espira](https://www.linkedin.com/in/andrew-espira/), [Mariia Rudenko](https://www.linkedin.com/in/mariia-r-748931163/), [Aderianna Williams](https://www.linkedin.com/in/at-williams/), [Marino Wijay](https://www.linkedin.com/in/mwijay/) and [Maria Ashby](https://www.linkedin.com/in/maria-ashby/) whose insights made this discussion truly invaluable. We had an incredibly insightful exchange, and I walked away with some serious food for thought.\n\n![](../../assets/newsletter/12-kcd-new-york/photo-4.jpeg)\n\nWe kicked off by acknowledging a universal truth in today‚Äôs cloud-native landscape: managing a full observability stack often feels like trying to hit a moving target. The more we aim to observe, the more inherent complexity seems to creep in. We continuously pile on tools, data, and dashboards‚Äìbe it metrics, traces, logs, or profiling ‚Äì and suddenly, we‚Äôre swimming in a sea of cognitive load, entropy, and quite often, plain old confusion. So, instead of me listing the common headaches, I threw it open to the room: ‚ÄúWhen you think about managing a full observability stack, across logs, metrics, traces, and so on, what are your biggest pain points? If you had to name the biggest challenge your team is facing with observability right now, what would it be?‚Äù\n\nThe responses flowed freely, revealing a shared understanding that while observability promises clarity, its real-world implementation often introduces its own unique set of challenges. And, quite organically, our conversation drifted into the exciting (and slightly unsettling) realm of Generative AI, specifically discussing how Large Language Models (LLMs) can be synergistically integrated within platforms to serve as enablers in resolving some of these very challenges.\n\n### The Observability Headaches: Where Do We Feel the Pinch?\n\nOne of the loudest points of contention was the persistent struggle to correlate telemetry data with the actual services generating them, and the broader challenge of **telemetry data correlation**. It‚Äôs like having all the pieces of a complex puzzle but no clear idea how they fit together. You might spot a spike in CPU utilization ‚Äì a metric ‚Äì but then you‚Äôre left guessing which microservice is the culprit. Then begins the detective work: diving into logs to pinpoint an error, and finally tracing requests to understand the flow. The fundamental problem is that these critical data points often reside in disparate systems, use inconsistent identifiers, and demand a significant amount of manual effort and intuition to connect the dots. This fragmentation makes it incredibly difficult to quickly identify the root cause of a problem when seconds count.\n\nAdding to this complexity is the sheer volume of alerts and the difficulty in correlating them with the actual underlying problem. We‚Äôve all experienced it: a dozen alerts fire simultaneously, each pointing to a symptom, yet none clearly indicating the core issue. This leads to what‚Äôs known as **alert fatigue**, resulting in missed critical incidents, wasted time triaging false positives, and ultimately, a palpable loss of trust in the alerting system itself. The challenge isn‚Äôt merely about receiving notifications; it‚Äôs about receiving meaningful alerts that directly pinpoint the underlying problem, not just its outward manifestations.\n\nFurthermore, a significant unspoken burden that often comes with observability is the **cost** of both creating and maintaining the entire observability stack. From licensing fees for proprietary tools to the infrastructure costs of storing massive volumes of telemetry data and the operational overhead of managing these complex systems, the financial outlay can be substantial. This constant investment of resources, both human and monetary, can become a major pain point, often weighing heavily on budget decisions and resource allocation.\n\nThen there‚Äôs the pervasive issue of **making insights accessible** and visualizing them in a way that provides the right insight to the right person. Raw telemetry data, in its unadulterated form, is overwhelming. Different roles within an organization ‚Äì SREs, developers, product managers ‚Äì need distinct views and varying levels of detail. A developer might require granular trace data, while a product manager needs high-level business metrics. The constant battle involves creating and maintaining these tailored dashboards and ensuring everyone knows where to find what they need. This often leads to information silos and missed opportunities for proactive improvement.\n\nA recurring theme throughout our discussion was the persistent problem of **siloed teams** and the resulting **lack of standardization**. When different teams adopt disparate observability tools, inconsistent naming conventions, or even varied logging formats, it inevitably creates a fragmented and chaotic landscape. This makes it incredibly challenging to gain a holistic view of the system, collaborate effectively during incidents, and leverage best practices across the entire organization. It‚Äôs a classic case of ‚Äúeveryone doing their own thing‚Äù, leading to pervasive inefficiencies and increased complexity.\n\nFinally, a crucial point that resonated deeply was the importance of **developer education**. Observability isn‚Äôt merely about deploying tools; it‚Äôs about cultivating a specific mindset. Developers need to grasp why observability is vital, how to effectively instrument their code, how to interpret telemetry data, and critically, how to leverage observability tools to troubleshoot their applications. This knowledge gap can lead to poorly instrumented services, ignored alerts, and a general underutilization of the powerful observability stacks organizations invest heavily in.\n\n### Internal Developer Platforms: The Unified Solution\n\nSo, with these common headaches laid out, how do we begin to alleviate them? This is precisely where the concept of an **Internal Developer Platform (IDP)** steps in as a truly powerful solution, providing a cohesive answer to many of these challenges.\n\nAn IDP, at its core, inherently solves the problem of standardization. By providing clear standards and abstractions through ‚Äúgolden paths‚Äù for building and deploying applications, it ensures consistency across the organization. However, it‚Äôs crucial that these **golden paths** don‚Äôt become ‚Äúgolden cages‚Äù. A well-designed IDP empowers developers with the necessary autonomy to cover edge cases, allowing them to step outside the perimeter of the provided golden paths when needed for specific requirements. This balance is vital for both consistency and innovation.\n\nMoving beyond standardization, IDPs also play a crucial role in addressing the challenges faced by siloed teams that might be working on different components of the same system and often lack a shared performance baseline. During our discussion, we introduced the concept of leveraging generative models within these platforms. Specifically, the role of Generative AI, particularly **Large Language Models (LLMs)**, in the observability space emerged as a truly futuristic and exciting prospect. The idea is that LLMs can help close the gap between users and telemetry data. Imagine being able to ask natural language questions like, ‚ÄúWhy is our checkout service slow right now?‚Äù and have an LLM sift through mountains of metrics, logs, and traces to provide a concise, actionable answer. Or, ‚ÄúWhat were the top 3 errors in our authentication service last night?‚Äù and get a summary, perhaps even with links to relevant log lines. These models can also be instrumental in enabling teams to define and compare their telemetry data against customized thresholds, ensuring that the entire system is monitored according to a collectively defined baseline, fostering a shared understanding of system health.\n\nFrom here, we delved into how these models further enhance the transparency and clarity of insights. LLMs, integrated within the IDP, can analyze vast amounts of telemetry data and provide various stakeholders with personalized insights and alerts. This capability opens the door to entirely new interfaces beyond the traditional dashboards, making complex operational data more accessible and actionable for different roles. Unfortunately, we didn‚Äôt have the opportunity to delve deeper into the intricate topic of telemetry data correlation during the roundtable, but I have written an article that explores this topic further, which you can find [here](https://www.linkedin.com/pulse/9-serving-observability-first-dish-graziano-casto-05rhf).\n\n### The Open Question: Balancing Trust and Cost with Benefits in the LLM Era\n\nHowever, as with any powerful new technology, the discussion around LLMs quickly led to a critical open question for the community:\n\n**How do we effectively balance the significant benefits that LLMs bring ‚Äì such as improved automation and deeper insights ‚Äì against the inherent costs? These costs include not only the economic investment required for these models but also the crucial aspect of trust, both in the accuracy of the results and in entrusting our sensitive data to an LLM, particularly when utilized as a service.**\n\nThis is a conversation that needs to continue. As we push the boundaries of what‚Äôs possible with AI in operations, we must collectively figure out how to build systems that are not only efficient and intelligent but also fundamentally secure, trustworthy, and cost-effective.\n\n### Wrapping Up\n\nMy first moderating experience at KCD New York was an absolute blast, and the insights from the roundtable on Platform Engineering and Observability were truly invaluable. It‚Äôs clear that while observability brings its own set of complexities, Internal Developer Platforms offer a robust framework for overcoming these challenges by promoting standardization, providing contextualized insights, and empowering developers. And looking ahead, the potential of LLMs to revolutionize how we interact with our telemetry data is incredibly exciting, even if it comes with some important questions we need to answer as a community.\n\nWhat are your thoughts on these challenges and solutions? And how do you see the role of LLMs evolving in the observability space, especially concerning the trust and cost trade-offs? Let‚Äôs keep the conversation going!\n","metadata":{"title":"From Chaos to Clarity: Navigating Observability in the Platform Engineering Era (and a Dash of AI)","excerpt":"A comprehensive recap of the KCD New York roundtable discussion on Platform Engineering and Observability, exploring key challenges, solutions, and the role of AI in modern observability practices.","date":"2025-08-20","author":"Graziano Casto"}},{"slug":"wasm-next-universal-runtime","content":"\n[Published by DZONE co-authored with Alex Casalboni (Developer Advocate @ Edgee)](https://dzone.com/articles/webassembly-from-browser-plugin-to-the-next-univer)\n\n\nFor decades, the digital world has converged on a single, universal computing platform: the web browser. This remarkable piece of software, present on nearly every device, promised a \"write once, run anywhere\" paradigm, but with a crucial limitation, it only spoke one language natively: JavaScript. While incredibly versatile, JavaScript's nature as a dynamically typed, interpreted language created a performance ceiling. For computationally intensive tasks, developers often hit a wall, unable to achieve the raw speed of native applications. This limitation also meant that the vast, mature ecosystems of code written in languages like C++, C, and Rust were largely inaccessible on the web without cumbersome and often inefficient cross-compilation to JavaScript.\n\nInto this landscape emerged [**WebAssembly**](https://dzone.com/articles/what-is-webassembly) **(Wasm)**. Often referred to as a fourth standard language for the web alongside HTML, CSS, and JavaScript, Wasm was not designed to replace JavaScript but to be its powerful companion. It is a binary instruction format, a low-level, assembly-like language that **serves as a portable compilation target**. This simple yet profound idea meant that developers could take existing code written in high-performance languages, compile it into a compact Wasm binary, and run it directly within the browser at near-native speeds. This breakthrough unlocked a new class of applications that were previously impractical for the web, from sophisticated in-browser tools to full-fledged 3D gaming engines.\n\nThe design of WebAssembly was forged in the demanding and often hostile environment of the public internet, leading to a set of foundational principles that would define its destiny. It had to be **fast**, with a compact binary format that could be decoded and executed far more efficiently than parsing text-based JavaScript. It had to be **secure**, running inside a tightly controlled, memory-safe sandbox that isolated it from the host system and other browser tabs. And it had to be **portable**, a universal format independent of any specific operating system or hardware architecture.\n\nThese very principles, essential for its success in the browser, were also the seeds of a much grander vision. This article charts the remarkable journey of WebAssembly, following its evolution from a browser-based performance booster into a foundational technology that is reshaping our approach to cloud, edge, and distributed computing, promising a future built on a truly universal runtime.\n\n### Beyond the Browser With the WebAssembly System Interface (WASI)\n\nWebAssembly's potential was too significant to remain confined within the browser. Developers and architects quickly recognized that a portable, fast, and secure runtime could be immensely valuable for server-side applications. However, a critical piece of the puzzle was missing.\n\nWasm modules running in the browser can interact with its environment through a rich set of Web APIs, allowing it to fetch data, manipulate the screen, or play audio. Server-side applications have a **completely different set of needs**: they must read and write files, access environment variables, open network sockets, and interact with the system clock. Without a standardized way to perform these basic operations, server-side Wasm would be a collection of incompatible, proprietary solutions, shattering its promise of portability.\n\nThe solution is the **WebAssembly System Interface (WASI)**, an evolving set of APIs. It's crucial to understand that WASI is not a single, monolithic standard but is currently in a significant transition, from the stable but limited **WASI Preview 1** (which lacks standardized networking) to the fundamentally redesigned **WASI Preview 2**. This newer version is built upon the still-in-proposal Component Model and introduces modular APIs for features like HTTP and sockets. ¬†Looking ahead, the next iteration, [WASI Preview 3](https://wasi.dev/roadmap#upcoming-wasi-03-releases), is anticipated for release in **August 2025**, promising further advancements such as native async and streaming support.\n\nThis layer of abstraction is the key to preserving Wasm's \"write once, run anywhere\" superpower. The WASI standard allows developers to write code in their preferred programming language (including [Rust](https://dzone.com/articles/rust-and-webassembly-for-web-apps), C/C++, C#, Go, JavaScript, TypeScript, and Python), compile it into a single Wasm binary, and run it on any operating system or CPU architecture using a compliant runtime.. In the browser, the JavaScript engine acts as the host runtime; outside the browser, this role is filled by standalone runtimes such as Wasmtime, Wasmer, or WasmEdge, which implement the WASI standard to provide secure access to system resources.\n\nMore than just enabling server-side execution, WASI introduced a fundamentally different and more secure way for programs to interact with the system. Traditional applications, following a model established by POSIX, typically inherit the permissions of the user who runs them. If a user can access a file, any program they run can also access that file, which creates a broad and implicit grant of authority.\n\nWASI, in contrast, implements a **capability-based security model**. By default, a Wasm module running via WASI can do nothing. It has no access to the filesystem, no ability to make network connections, and no visibility into system clocks or environment variables. To perform any of these actions, the host runtime must explicitly grant the module a 'capability'. For example, to allow a module to read files, the host must grant it a capability for a specific directory. The module receives a handle to that directory and can operate only within its confines. Any attempt to access a path outside of it will fail at the runtime level with a 'permission denied' error, even if the user running the process has permissions for that file. This enforces the **Principle of Least Privilege** at a granular level, a stark contrast to the traditional POSIX model where a process inherits all the ambient permissions of the user.\n\nThis \" *deny-by-default* \" posture represents a paradigm shift in application security. The decision to build WASI around a capability-based model was not merely a technical convenience; it was a deliberate architectural choice that transformed Wasm from a simple performance tool into a foundational building block for trustworthy computing. The browser sandbox provided an implicit security boundary designed to protect users from malicious websites. Simply mirroring traditional OS permissions on the server would have compromised this security-first ethos. Instead, by externalizing permission management from the application to the host runtime, WASI makes security an explicit, auditable contract.\n\nThis has profound implications, making Wasm uniquely suited for scenarios where the code being executed cannot be fully trusted. This includes multi-tenant serverless platforms running customer-submitted functions, extensible applications with third-party plugin systems, and edge devices executing logic from various sources. WASI did not just allow Wasm to run on the server; it defined how it would run: securely, with granular permissions, and by default, with no authority at all.\n\n### A Different Kind of Isolation: Wasm vs. Containers\n\nFor many developers today, the container has become the default unit of application deployment, a standardized box for packaging and running software. The rise of WebAssembly has introduced a new model, prompting a comparison that is less about which technology is superior and more about understanding two fundamentally different philosophies for achieving portability and isolation.\n\nThe container philosophy centers on porting the entire **environment**. A container image, such as one built with [Docker](https://dzone.com/articles/docker-use-cases-15-most-common-ways-to-use-docker), packages an application along with a complete slice of its user-space operating system: a filesystem, system libraries, configuration files, and all other dependencies. It achieves isolation from the host and other containers by leveraging OS-level virtualization features, primarily Linux namespaces and control groups (cgroups), which create the illusion of a private machine. The container's promise is that this self-contained environment will run consistently everywhere a container engine is installed.\n\nThe WebAssembly philosophy, in contrast, is about porting only the **application logic**. A Wasm module is a single, self-contained binary file containing just the compiled application code. It brings no operating system, no filesystem, and no system bundled libraries. Instead, it relies on the host runtime to provide a standardized environment and to mediate access to system resources through the WASI interface. Wasm's promise is that the application logic, compiled once, will run consistently everywhere a compliant Wasm runtime is present.\n\nThis philosophical divergence leads to significant practical trade-offs in size, speed, and security. Because a container must package a slice of an operating system, its image size is measured in (hundreds of) megabytes, even for simple applications. A Wasm module, containing only the application code, is orders of magnitude smaller, typically measured in kilobytes or a few megabytes. This dramatic difference impacts everything from storage costs and network transfer times to the density of workloads that can run on a single machine.\n\nThe most critical distinction, particularly for modern cloud architectures, is startup speed. A container must initialize its packaged environment: a process that involves setting up namespaces, mounting the filesystem, and booting the application. This \"cold start\" can take hundreds of milliseconds, or even several seconds. A Wasm module, on the other hand, is instantiated by an already-running runtime, a process that can take less than a millisecond (for compiled languages like Rust, C or Go). This near-instantaneous startup effectively eliminates the cold start problem, making Wasm an ideal technology for event-driven, scale-to-zero architectures like serverless functions, where responsiveness is paramount.\n\nThe security models also differ profoundly. Containers provide isolation at the OS kernel level. This means all containers on a host share the same kernel, which represents a large and complex attack surface. Security vulnerabilities often center on kernel exploits or misconfigurations that allow a process to \"escape\" its container and gain access to the host system. WebAssembly introduces an additional, finer-grained layer of isolation: the application-level sandbox. The attack surface is not the entire OS kernel, but the much smaller and more rigorously defined boundary of the Wasm runtime and the WASI interface. Combined with its capability-based security model, this makes Wasm \"secure by default\" and a far safer choice for running untrusted or third-party code.\n\n| Feature                 | **WebAssembly (WASM)**                                              | **Containers**                                                   |\n| ----------------------- | ------------------------------------------------------------------- | ---------------------------------------------------------------- |\n| **Unit of Portability** | Application Logic (a `.wasm` binary)                                | Application Environment (an OCI image with an OS filesystem)     |\n| **Isolation Model**     | Application-level Sandbox (deny-by-default)                         | OS-level Virtualization (namespaces, cgroups)                    |\n| **Security Boundary**   | Wasm Runtime \u0026 WASI Interface (small, well-defined)                 | Host OS Kernel (large, complex attack surface)                   |\n| **Startup Time**        | Sub-millisecond (\"zero cold start\")                                 | Hundreds of milliseconds to seconds (\"cold start\" problem)       |\n| **Size / Footprint**    | Kilobytes to Megabytes                                              | Megabytes to Gigabytes                                           |\n| **Platform Dependency** | Runtime-dependent (any OS/arch with a Wasm runtime)                 | OS and Architecture-dependent (e.g. `linux/amd64`)               |\n| **Ideal Use Case**      | Serverless functions, microservices, edge computing, plugin systems | Lift-and-shift legacy apps, complex stateful services, databases |\n\nUltimately, these two technologies are not adversaries but complements. It is common to run Wasm workloads inside containers as a first step toward integrating them into existing infrastructure. Each technology is optimized for different scenarios. Containers excel at lifting and shifting existing, complex applications that depend on a full POSIX-compliant environment, such as databases or legacy monolithic services. WebAssembly shines in the world of greenfield, cloud-native development, offering a lighter, faster, and more secure foundation for building the next generation of microservices and serverless functions.\n\n### New Foundations for Platform Engineering: The Cloud and the Edge\n\nFor WebAssembly to fulfill its potential as a server-side technology, it must integrate seamlessly into the dominant paradigm for cloud infrastructure management: Kubernetes. This integration is not just possible; it is already well underway, enabled by the extensible architecture of the cloud-native ecosystem. At its core, Kubernetes orchestrates workloads by communicating with a high-level container runtime, such as containerd, on each of its worker nodes. This high-level runtime is responsible for managing images and container lifecycles, but it delegates the actual task of running a process to a low-level runtime. For traditional Linux containers, this runtime is typically *runc*.\n\nThe key to running Wasm on Kubernetes lies in replacing this final link in the chain. Projects like [runwasi](https://github.com/containerd/runwasi) provide a \"shim\", a small piece of software that acts as a bridge, allowing containerd to communicate with a WebAssembly runtime (like Wasmtime or WasmEdge) just as it would with *runc*. This makes the Wasm runtime appear to Kubernetes as just another way to run workloads. The final piece of the integration is a Kubernetes object called a *RuntimeClass*, which acts as a label. By applying this label to a workload definition, developers can instruct the Kubernetes scheduler to deploy that specific workload to nodes configured with the Wasm shim, enabling Wasm modules and traditional containers to run side-by-side within the same cluster. Projects like [SpinKube](https://www.spinkube.dev/) are emerging to automate this entire setup process, making it easier for organizations to adopt Wasm without rebuilding their infrastructure from scratch.\n\nThis deep integration enables new and more efficient approaches to platform engineering: the discipline of building and managing the internal platforms that development teams use to ship software. In this pattern, the platform team provides standardized components that encapsulate common, cross-cutting concerns like logging, metrics, network access, and security policies. Application developers, in turn, focus solely on writing a \"user\" component that contains pure business logic. At deployment time, these two pieces are composed into a single, tiny, and secure Wasm binary. This creates a powerful separation of concerns. Developers are freed from boilerplate code and infrastructure details, while the platform team can enforce standards, patch vulnerabilities, and evolve the platform's capabilities centrally and transparently, without requiring application teams to rebuild or redeploy their code.\n\nWhile these patterns are transforming the cloud, it is at the network's edge where WebAssembly's advantages become not just beneficial, but essential. Edge computing involves moving computation away from centralized data centers and closer to where data is generated and consumed: on IoT devices, in factory machinery, at retail locations, or within telecommunication networks. These environments are often severely resource-constrained, with limited CPU, memory, and power, making heavyweight containers impractical or impossible to run.\n\nWebAssembly is a near-perfect fit for this world. Its incredibly small binary size and minimal resource footprint allow it to run on devices where containers cannot. Its near-instantaneous startup times are critical for the event-driven, real-time processing required in many edge scenarios. And its true platform independence, the ability for a single compiled binary to run on any CPU architecture, be it x86, ARM, or RISC-V, is a necessity in the heterogeneous hardware landscape of the edge. This has unlocked a new wave of applications, from running machine learning inference models to executing dynamic logic within Content Delivery Networks (CDNs) with ultra-low latency.\n\nThe ability of WebAssembly to operate seamlessly across these diverse environments reveals its most profound impact. Historically, software development has been siloed; building for the browser, the cloud, and embedded devices required different tools, different languages, and different deployment models. Containers helped unify deployment in the cloud, but they are foreign to the browser and too cumbersome for much of the edge. WebAssembly is the first technology to provide a single, consistent application runtime that spans this entire compute continuum. The true strength of WebAssembly lies in how its ecosystem bridges the historically separate worlds of the browser, cloud, and edge. While the final.wasm module is often tailored for its specific environment, Wasm as a standard provides a common compilation target. This allows developers to deploy applications across a vast spectrum: from a rich user interface in a web browser, to large-scale processing orchestrated by Kubernetes, and even to tiny, resource-constrained IoT devices. This reality enables a future where developers write their core business logic once and can deploy it to the most appropriate location: close to the user for low latency, in the cloud for heavy computation, or in the browser for interactivity without needing to rewrite or repackage it. This capability breaks down the architectural barriers that have long defined distributed systems, paving the way for a truly fluid and unified model of computation.\n\n### The Future is Composable: The WebAssembly Component Model\n\nDespite its portability and security, a final, fundamental challenge has historically limited WebAssembly's potential: true interoperability. While a single Wasm module is a self-contained unit, getting multiple modules to communicate with each other effectively has been remarkably difficult. The core Wasm specification only allows for the passing of simple numeric types, integers and floats, between modules. Exchanging more complex data structures like strings, lists, or objects requires developers to manually manage pointers and memory layouts, a process that is deeply tied to the conventions of the source language and compiler. This \"impedance mismatch\" means that a Wasm module compiled from Rust cannot easily call a function in a module compiled from Go, as they represent data in fundamentally incompatible ways. This has been the primary barrier to creating a vibrant, language-agnostic ecosystem of reusable Wasm libraries, forcing developers into fragile, language-specific linking models where modules must share a single linear memory space.\n\nThe [WebAssembly Component Model](https://component-model.bytecodealliance.org/) is the ambitious proposal designed to solve this final challenge. **It is critical, however, to understand its current status: the Component Model is an active proposal under development, not a finalized W3C standard**. While tooling and runtimes are rapidly implementing it, the specification is still subject to change. It is an evolution of the core standard that elevates Wasm from a format for individual, isolated modules into a system for building complex applications from smaller, interoperable, and language-agnostic parts. The most effective analogy for the Component Model is that it turns Wasm modules into standardized \"LEGO bricks\". Each component is a self-contained, reusable piece of software with well-defined connection points, allowing them to be snapped together to build something larger.\n\nTwo key concepts make this possible: **WIT** and ‚Äú **worlds** ‚Äù. The WebAssembly Interface Type (WIT) is an Interface Definition Language (IDL) used to describe the \"shape\" of the connectors on these metaphorical LEGO bricks. A WIT file defines the high-level functions and rich data types such as strings, lists, variants, and records that a component either **exports** (provides to others) or **imports** (requires from its environment).\n\nCrucially, the standard **WASI interfaces** themselves (e.g. for filesystems or sockets) are also defined using WIT. This means developers can use the exact same language to extend the default system capabilities with their own **domain-specific interfaces**, creating a unified and powerful way to describe any interaction.\n\nA \"world\" is a WIT definition that describes the complete set of interfaces a component interacts with, effectively declaring all of its capabilities and dependencies. Tooling built around the Component Model, such as [wit-bindgen](https://github.com/bytecodealliance/wit-bindgen), then automatically generates the necessary \"binding code\" for each language. This code handles the complex task of translating data between a language's native representation (e.g., a Rust String or a Python list) and a standardized, language-agnostic memory layout known as the [Canonical ABI](https://component-model.bytecodealliance.org/advanced/canonical-abi.html). The result is seamless interoperability: a component written in C++ can call a function exported by a component written in TinyGo, passing complex data back and forth as if they were native libraries in the same language, without either needing any knowledge of the other's internal implementation.\n\nThis enables a fundamentally different approach to software composition compared to the container world. Container-based architectures are typically composed at design time. Developers build discrete services, package them into containers, and then define how they interact, usually over a network via APIs, using orchestration configurations like Kubernetes manifests or Docker Compose files. This is a model for composing distributed systems. The WebAssembly Component Model enables granular composition at runtime. Components communicate through fast, standardized in-memory interfaces rather than network protocols, allowing them to be linked together within the same process. This creates a model for building applications from secure, sandboxed, and interchangeable parts.\n\nA prime example is [wasmCloud](https://wasmcloud.com/docs/concepts/linking-components/linking-at-runtime/). In this platform, components (called actors) declare dependencies on abstract interfaces, like a key-value store. At runtime, they are dynamically linked to providers that offer concrete implementations (e.g. a Redis provider).\n\nThe key advantage is that these links can be changed on the fly. You can swap the Redis provider for a different one without restarting or recompiling the application, perfectly realizing the goal of building flexible systems from truly interchangeable parts.\n\nThis shift from source-level libraries to compiled, sandboxed components as the fundamental unit of software reuse represents a paradigm shift. It is the technical realization of architectural concepts like Packaged Business Capabilities (PBCs), where distinct business functions are encapsulated as autonomous, deployable software components. A Wasm component provides a near-perfect implementation of a PBC: it is a compiled, portable, and secure artifact that encapsulates specific logic. The Component Model, therefore, is not just a technical upgrade for linking code. It is the foundation for a future where software is no longer just written, but composed. Developers will be able to assemble applications from a universal ecosystem of secure, pre-built components that provide best-of-breed solutions for specific tasks, fundamentally altering the nature of the software supply chain and accelerating innovation across all languages and platforms.\n\n### Conclusion: From a Faster Web to a Universal Runtime\n\nWebAssembly's journey has been one of remarkable and accelerating evolution. Born from the practical need to overcome performance bottlenecks in the web browser, its core principles of speed, portability, and security proved to be far more powerful than its creators may have initially envisioned. What began as a way to run C++ code alongside JavaScript has grown into a technology that is fundamentally reshaping our conception of software.\n\nThe introduction of the WebAssembly System Interface (WASI) was the pivotal moment, transforming Wasm from a browser-centric tool into a viable, universal runtime for server-side computing. Its capability-based security model offered a fresh, \"secure-by-default\" alternative to traditional application architectures. This new foundation allowed Wasm to emerge as a compelling counterpart to containers, offering an unparalleled combination of lightweight footprint, near-instantaneous startup, and a hardened security sandbox that is ideally suited for the demands of serverless functions and the resource-constrained world of edge computing. Today, Wasm is not just a technology for the browser, the cloud, or the edge; it is the first to provide a single, consistent runtime that spans this entire continuum, breaking down long-standing silos in software development.\n\nNow, with the advent of the Component Model, WebAssembly is poised for its next great leap. By solving the final, critical challenge of language-agnostic interoperability, it lays the groundwork for a future where applications are not monoliths to be built, but solutions to be composed from a global ecosystem of secure, reusable, and portable software components. WebAssembly is more than just a faster way to run code; it is a foundational shift toward a more modular, more secure, and truly universal paradigm for the next era of computing.","metadata":{"title":"WebAssembly: From Browser Plugin to the Next Universal Runtime","excerpt":"Explore WebAssembly's evolution from a browser performance booster to a universal runtime reshaping cloud, edge, and distributed computing with near-native performance across platforms.","date":"2025-08-04","author":"Graziano Casto"}},{"slug":"kubernetes-v134-sneak-peak","content":"\n[Published by Kubernetes co-authored with Agustina Barbetta, Alejandro Josue Leon Bellido, Melony Qin, Dipesh Rawat](https://kubernetes.io/blog/2025/07/28/kubernetes-v1-34-sneak-peek/)\n\nKubernetes v1.34 is coming at the end of August 2025. This release will not include any removal or deprecation, but it is packed with an impressive number of enhancements. Here are some of the features we are most excited about in this cycle!\n\nPlease note that this information reflects the current state of v1.34 development and may change before release.\n\n### Featured enhancements of Kubernetes v1.34 \n\nThe following list highlights some of the notable enhancements likely to be included in the v1.34 release, but is not an exhaustive list of all planned changes. This is not a commitment and the release content is subject to change.\n\n#### The core of DRA targets stable\n\n[Dynamic Resource Allocation](https://kubernetes.io/docs/concepts/scheduling-eviction/dynamic-resource-allocation/) (DRA) provides a flexible way to categorize, request, and use devices like GPUs or custom hardware in your Kubernetes cluster.\n\nSince the v1.30 release, DRA has been based around claiming devices using *structured parameters* that are opaque to the core of Kubernetes. The relevant enhancement proposal, [KEP-4381](https://kep.k8s.io/4381), took inspiration from dynamic provisioning for storage volumes. DRA with structured parameters relies on a set of supporting API kinds: ResourceClaim, DeviceClass, ResourceClaimTemplate, and ResourceSlice API types under `resource.k8s.io`, while extending the `.spec` for Pods with a new `resourceClaims` field. The core of DRA is targeting graduation to stable in Kubernetes v1.34.\n\nWith DRA, device drivers and cluster admins define device classes that are available for use. Workloads can claim devices from a device class within device requests. Kubernetes allocates matching devices to specific claims and places the corresponding Pods on nodes that can access the allocated devices. This framework provides flexible device filtering using CEL, centralized device categorization, and simplified Pod requests, among other benefits.\n\nOnce this feature has graduated, the `resource.k8s.io/v1` APIs will be available by default.\n\n#### ServiceAccount tokens for image pull authentication\n\nThe [ServiceAccount](https://kubernetes.io/docs/concepts/security/service-accounts/) token integration for `kubelet` credential providers is likely to reach beta and be enabled by default in Kubernetes v1.34. This allows the `kubelet` to use these tokens when pulling container images from registries that require authentication.\n\nThat support already exists as alpha, and is tracked as part of [KEP-4412](https://kep.k8s.io/4412).\n\nThe existing alpha integration allows the `kubelet` to use short-lived, automatically rotated ServiceAccount tokens (that follow OIDC-compliant semantics) to authenticate to a container image registry. Each token is scoped to one associated Pod; the overall mechanism replaces the need for long-lived image pull Secrets.\n\nAdopting this new approach reduces security risks, supports workload-level identity, and helps cut operational overhead. It brings image pull authentication closer to modern, identity-aware good practice.\n\n#### Pod replacement policy for Deployments\n\nAfter a change to a [Deployment](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/), terminating pods may stay up for a considerable amount of time and may consume additional resources. As part of [KEP-3973](https://kep.k8s.io/3973), the `.spec.podReplacementPolicy` field will be introduced (as alpha) for Deployments.\n\nIf your cluster has the feature enabled, you'll be able to select one of two policies:\n\n`TerminationStarted`\n\nCreates new pods as soon as old ones start terminating, resulting in faster rollouts at the cost of potentially higher resource consumption.\n\n`TerminationComplete`\n\nWaits until old pods fully terminate before creating new ones, resulting in slower rollouts but ensuring controlled resource consumption.\n\nThis feature makes Deployment behavior more predictable by letting you choose when new pods should be created during updates or scaling. It's beneficial when working in clusters with tight resource constraints or with workloads with long termination periods.\n\nIt's expected to be available as an alpha feature and can be enabled using the `DeploymentPodReplacementPolicy` and `DeploymentReplicaSetTerminatingReplicas` feature gates in the API server and kube-controller-manager.\n\n#### Production-ready tracing for kubelet and API Server\n\nTo address the longstanding challenge of debugging node-level issues by correlating disconnected logs,[KEP-2831](https://kep.k8s.io/2831) provides deep, contextual insights into the `kubelet`.\n\nThis feature instruments critical `kubelet` operations, particularly its gRPC calls to the Container Runtime Interface (CRI), using the vendor-agnostic OpenTelemetry standard. It allows operators to visualize the entire lifecycle of events (for example: a Pod startup) to pinpoint sources of latency and errors. Its most powerful aspect is the propagation of trace context; the `kubelet` passes a trace ID with its requests to the container runtime, enabling runtimes to link their own spans.\n\nThis effort is complemented by a parallel enhancement, [KEP-647](https://kep.k8s.io/647), which brings the same tracing capabilities to the Kubernetes API server. Together, these enhancements provide a more unified, end-to-end view of events, simplifying the process of pinpointing latency and errors from the control plane down to the node. These features have matured through the official Kubernetes release process.[KEP-2831](https://kep.k8s.io/2831) was introduced as an alpha feature in v1.25, while [KEP-647](https://kep.k8s.io/647) debuted as alpha in v1.22. Both enhancements were promoted to beta together in the v1.27 release. Looking forward, Kubelet Tracing ([KEP-2831](https://kep.k8s.io/2831)) and API Server Tracing ([KEP-647](https://kep.k8s.io/647)) are now targeting graduation to stable in the upcoming v1.34 release.\n\n#### PreferSameZone and PreferSameNode traffic distribution for Services\n\nThe `spec.trafficDistribution` field within a Kubernetes [Service](https://kubernetes.io/docs/concepts/services-networking/service/) allows users to express preferences for how traffic should be routed to Service endpoints.\n\n[KEP-3015](https://kep.k8s.io/3015) deprecates `PreferClose` and introduces two additional values: `PreferSameZone` and `PreferSameNode`.`PreferSameZone` is equivalent to the current `PreferClose`.`PreferSameNode` prioritizes sending traffic to endpoints on the same node as the client.\n\nThis feature was introduced in v1.33 behind the `PreferSameTrafficDistribution` feature gate. It is targeting graduation to beta in v1.34 with its feature gate enabled by default.\n\n#### Support for KYAML: a Kubernetes dialect of YAML\n\nKYAML aims to be a safer and less ambiguous YAML subset, and was designed specifically for Kubernetes. Whatever version of Kubernetes you use, you'll be able use KYAML for writing manifests and/or Helm charts. You can write KYAML and pass it as an input to **any** version of `kubectl`, because all KYAML files are also valid as YAML. With kubectl v1.34, we expect you'll also be able to request KYAML output from `kubectl` (as in `kubectl get -o kyaml ‚Ä¶`). If you prefer, you can still request the output in JSON or YAML format.\n\nKYAML addresses specific challenges with both YAML and JSON. YAML's significant whitespace requires careful attention to indentation and nesting, while its optional string-quoting can lead to unexpected type coercion (for example: [\"The Norway Bug\"](https://hitchdev.com/strictyaml/why/implicit-typing-removed/)). Meanwhile, JSON lacks comment support and has strict requirements for trailing commas and quoted keys.\n\n[KEP-5295](https://kep.k8s.io/5295) introduces KYAML, which tries to address the most significant problems by:\n\n- Always double-quoting value strings\n- Leaving keys unquoted unless they are potentially ambiguous\n- Always using `{}` for mappings (associative arrays)\n- Always using `[]` for lists\n\nThis might sound a lot like JSON, because it is! But unlike JSON, KYAML supports comments, allows trailing commas, and doesn't require quoted keys.\n\nWe're hoping to see KYAML introduced as a new output format for `kubectl` v1.34. As with all these features, none of these changes are 100% confirmed; watch this space!\n\nAs a format, KYAML is and will remain a **strict subset of YAML**, ensuring that any compliant YAML parser can parse KYAML documents. Kubernetes does not require you to provide input specifically formatted as KYAML, and we have no plans to change that.\n\n#### Fine-grained autoscaling control with HPA configurable tolerance\n\n[KEP-4951](https://kep.k8s.io/4951) introduces a new feature that allows users to configure autoscaling tolerance on a per-HPA basis, overriding the default cluster-wide 10% tolerance setting that often proves too coarse-grained for diverse workloads. The enhancement adds an optional `tolerance` field to the HPA's `spec.behavior.scaleUp` and `spec.behavior.scaleDown` sections, enabling different tolerance values for scale-up and scale-down operations, which is particularly valuable since scale-up responsiveness is typically more critical than scale-down speed for handling traffic surges.\n\nReleased as alpha in Kubernetes v1.33 behind the `HPAConfigurableTolerance` feature gate, this feature is expected to graduate to beta in v1.34. This improvement helps to address scaling challenges with large deployments, where for scaling in, a 10% tolerance might mean leaving hundreds of unnecessary Pods running. Using the new, more flexible approach would enable workload-specific optimization for both responsive and conservative scaling behaviors.\n\n### Want to know more?\n\nNew features and deprecations are also announced in the Kubernetes release notes. We will formally announce what's new in [Kubernetes v1.34](https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.34.md) as part of the CHANGELOG for that release.\n\nThe Kubernetes v1.34 release is planned for **Wednesday 27th August 2025**. Stay tuned for updates!\n\n### Get involved\n\nThe simplest way to get involved with Kubernetes is to join one of the many [Special Interest Groups](https://github.com/kubernetes/community/blob/master/sig-list.md) (SIGs) that align with your interests. Have something you'd like to broadcast to the Kubernetes community? Share your voice at our weekly [community meeting](https://github.com/kubernetes/community/tree/master/communication), and through the channels below. Thank you for your continued feedback and support.\n\n- Follow us on Bluesky [@kubernetes.io](https://bsky.app/profile/kubernetes.io) for the latest updates\n- Join the community discussion on [Discuss](https://discuss.kubernetes.io/)\n- Join the community on [Slack](http://slack.k8s.io/)\n- Post questions (or answer questions) on [Server Fault](https://serverfault.com/questions/tagged/kubernetes) or [Stack Overflow](http://stackoverflow.com/questions/tagged/kubernetes)\n- Share your Kubernetes [story](https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform)\n- Read more about what's happening with Kubernetes on the [blog](https://kubernetes.io/blog/)\n- Learn more about the [Kubernetes Release Team](https://github.com/kubernetes/sig-release/tree/master/release-team)","metadata":{"title":"Kubernetes v1.34 Sneak Peek","excerpt":"Discover the exciting enhancements coming in Kubernetes v1.34, including stable Dynamic Resource Allocation, improved scheduling capabilities, and enhanced security features for production workloads.","date":"2025-07-28","author":"Graziano Casto"}},{"slug":"ai-native-platforms","content":"\n[Published by DZONE](https://dzone.com/articles/ai-native-platforms-genai-platform-engineering)\n\nLet's be honest. Building developer platforms, especially for **AI-native** teams, is a complex art, a constant challenge. It's about finding a delicate balance: granting maximum autonomy to development teams without spiraling into chaos, and providing incredibly powerful, cutting-edge tools without adding superfluous complexity to their already dense workload. Our objective as Platform Engineers has always been to pave the way, remove obstacles, and accelerate innovation. But what if the next, inevitable phase of platform evolution wasn't just about what we build and provide, but what Generative AI can help us co-build, co-design, and co-manage?\n\nWe're not talking about a mere incremental improvement, a minor optimization, or a marginal new feature. We're facing a genuine **paradigm shift**, a conceptual earthquake where artificial intelligence is no longer merely the final product of our efforts, the result of our development toils, but becomes the silent partner, the tireless ally that is already reimagining, rewriting, and redefining our entire development experience. This is the real gamble, the challenge that awaits us: transforming our platforms from simple toolsets, however sophisticated, into **intelligent, dynamic, and self-optimizing ecosystems**. A place where productivity isn't just high, but exceptionally high, and innovation flows frictionlessly.\n\n### What if we unlock 100% of our Platform‚Äôs potential?\n\nYour primary goal, like that of any good Platform Engineer, is already to make developers' lives simpler, faster, and, let's admit it, significantly more enjoyable. Now, imagine endowing your platform with genuine intelligence, with the ability to understand, anticipate, and even generate. GenAI, in this context, isn't just an additional feature that layers onto existing ones; it's the **catalyst** that is already fundamentally redefining the **Developer Experience (DevEx)**, exponentially accelerating the entire **software development lifecycle**, and, even more fascinating, creating **new, intuitive, and natural interfaces** for interacting with the platform's intrinsic capabilities.\n\nLet's momentarily consider the most common and frustrating pain points that still afflict the average developer: the exhaustive and often fruitless hunt through infinite and fragmented documentation, the obligation to memorize dozens, if not hundreds, of specific and often cryptic CLI commands, or the tedious and repetitive generation of boilerplate code. With the intelligent integration of GenAI, your platform magically evolves into a true **intelligent co-pilot**. Imagine a developer who can simply express a request in natural language, as if speaking to an expert colleague: \"Provision a new staging environment for my authentication microservice, complete with a PostgreSQL database, a dedicated Kafka topic, and integration with our monitoring system.\" The GenAI-powered platform not only understands the deep meaning and context of the request, not only translates the intention into a series of technical actions, but **executes the operation autonomously**, providing immediate feedback and magically configuring everything needed. This isn't mere automation, which we already know; it's a **conversational interaction**, deep and contextual, that almost completely zeroes out the developer's cognitive load, freeing their mind and creative energies to focus on innovation, not on the complex and often tedious infrastructural \"plumbing\".\n\nBut the impact extends far beyond simple commands. GenAI can act as an **omnipresent expert**, an always-available and incredibly informed figure, providing real-time, contextual assistance. Imagine being stuck on a dependency error, a hard-to-diagnose configuration problem, or a security vulnerability. Instead of spending hours searching forums or asking colleagues, you can ask the platform directly. And it, magically, suggests practical solutions, directs you to relevant internal best practices (perhaps your own guides, finally usable in an intelligent way!), or even proposes complete code patches to solve the problem. It can proactively identify potential security vulnerabilities in the code you've just generated or modified, suggest intelligent refactorings to improve performance, or even scaffold entire new modules or microservices based on high-level descriptions. This drastically accelerates the entire software development lifecycle, making best practices inherent to the process and transforming bottlenecks into opportunities for automation. Your platform is no longer a mere collection of passive tools, but an **intelligent and proactive partner** at every single stage of the developer's workflow, from conception to implementation, from testing to deployment.\n\nCrucially, for this to work, the GenAI model must be **fed with the right platform context**. By ingesting all platform documentation, internal APIs, service catalogs, and architectural patterns, the AI becomes an unparalleled tool for **discoverability of platform items**. Developers can now query in natural language to find the right component, service, or golden path for their needs. Furthermore, this contextual understanding allows the AI to **interrogate and access all data and assets** within the platform itself, as well as from the applications being developed on it, providing insights and recommendations in real-time. This elevates the concept of a **composable architecture**, already enabled by your platform, to an entirely new level. With an AI co-pilot that not only knows all available platform items but also understands how to use them optimally and how others have used them effectively, the development of new composable applications or rapid Proofs of Concept (PoCs) becomes faster than ever before.\n\nThe new interfaces enabled by GenAI go beyond mere suggestion. Think of natural language **chatbot interfaces** for giving commands, where the platform responds like a virtual assistant. Crucially, thanks to advancements like **Model Context Protocol (MCP)** or similar **tool-use capabilities**, the GenAI-powered platform can move beyond just \"suggesting\" and actively \"doing\". It can execute complex workflows, interact with external APIs, and trigger actions within your infrastructure. This fosters a true **cognitive architecture** where the model isn't just generating text but is an active participant in your operations, capable of generating architectural diagrams, provisioning resources, or even deploying components based on a simple natural language description. The vision is that of a \"platform agent\" or an \"AI persona\" that learns and adapts to the specific needs of the team and the individual developer, constantly optimizing their path and facilitating the adoption of best practices.\n\n### Platforms: the launchpad for AI-powered applications\n\nThis synergy is two-way, a deep symbiotic relationship. If, on one hand, GenAI infuses new intelligence and vitality into platforms, on the other, your **Internal Developer Platforms** are, and will increasingly become, the **essential launchpad** for the unstoppable explosion of **AI-powered applications**. The complex and often winding journey of an artificial intelligence model‚Äîfrom the very first phase of experimentation and prototyping, through intensive training, to serving in production and scalable inference‚Äîis riddled with often daunting infrastructural complexities. Dedicated GPU clusters, specialized Machine Learning frameworks, complex data pipelines, and scalable, secure, and performant serving endpoints are by no means trivial for every single team to manage independently.\n\nAnd this is where your platform uniquely shines. It has the power to **abstract away all the thorny and technical details** of AI infrastructure, providing **self-service and on-demand provisioning** of the exact compute resources (CPU, various types of GPUs), storage (object storage, data lakes), and networking required for every single phase of the model's lifecycle. Imagine a developer who has just finished training a new model and needs to deploy an inference service. Instead of interacting with the Ops team for days or weeks, they simply request it through an intuitive self-service portal on the platform, and within minutes, the platform automatically provisions the necessary hardware (perhaps a dedicated GPU instance), deploys the model to a scalable endpoint (e.g., a serverless service or a container on a dedicated cluster), and, transparently, even generates a secure API key for access and consumption. This process **eliminates days or weeks of manual configuration**, of tickets and waiting times, transforming a complex and often frustrating MLOps challenge into a fluid, instant, and completely self-service operation. The platform manages not only serving but the entire lifecycle: from data preparation, to training clusters, to evaluation and A/B testing phases, all the way to post-deployment monitoring.\n\nFurthermore, platforms provide crucial **golden paths** for AI application development at the application layer. There's no longer a need for every team to reinvent the wheel for common AI patterns. Your platform can offer **pre-built templates and codified best practices** for integrating Large Language Models (LLMs), implementing patterns like Retrieval-Augmented Generation (RAG) with connectors to your internal data sources, or setting up complete pipelines for model monitoring and evaluation. Think of robust libraries and opinionated frameworks for prompt engineering, for managing model and dataset versions, for specific AI model observability (e.g., tools for bias detection, model interpretation, or drift management). The platform becomes a hub for collaboration on AI assets, facilitating the sharing and reuse of models, datasets, and components, **including the development of AI agents**. By embedding best practices and pre-integrating the most common and necessary AI services, every single developer, even one without a deep Machine Learning background, is empowered to infuse their applications with intelligent, cutting-edge capabilities. This not only **democratizes AI development** across the organization but unlocks unprecedented innovation that was previously limited to a few specialized teams.\n\n### The future is symbiotic: your next move\n\nThe era of **AI-native** development isn't an option; it's an imminent reality, and it urgently demands **AI-native platforms**. The marriage of GenAI and Platform Engineering isn't just an evolutionary step; it's a **revolutionary leap** destined to redefine the very foundations of our craft. GenAI makes platforms intrinsically smarter, more intuitive, more responsive, and consequently, incredibly more powerful. Platforms, in turn, provide the robust, self-service infrastructure and the well-paved roads necessary to massively accelerate the adoption and deployment of AI across the enterprise, transforming potential into reality.\n\nAre you ready to stop building for AI and start building with AI? Now is the time to act. Identify the most painful bottlenecks in your current DevEx and think about how GenAI could transform them. Prioritize the creation of self-service capabilities for AI infrastructure, making model deployment as simple as that of a traditional microservice. Cultivate a culture of \"platform as a product\", where AI is not just a consumer, but a fundamental feature of the platform itself.\n\nThe future of software development isn't just about AI-powered applications; it's about an **AI-powered development experience** that completely redefines the concepts of productivity, creativity, and the very act of value creation. Embrace this unstoppable alliance, and unlock the next fascinating frontier of innovation. The time of static platforms is over. The era of intelligent platforms has just begun.\n\n","metadata":{"title":"AI-Native Platforms: The Unstoppable Alliance of GenAI and Platform Engineering","excerpt":"Discover how Generative AI is transforming platform engineering from static toolsets into intelligent, dynamic, and self-optimizing ecosystems that unlock 100% of platform potential.","date":"2025-06-11","author":"Graziano Casto"}}]},"schema":{"@context":"https://schema.org","@type":"WebSite","name":"Graziano Casto WebSite","alternateName":"Graziano's WebSite","url":"https://castograziano.com"}},"__N_SSG":true},"page":"/","query":{},"buildId":"SYKRgUHRNYLLx0g7j2Qzh","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>